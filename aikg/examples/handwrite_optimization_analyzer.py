# Copyright 2025 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
æ‰‹å†™ä¼˜åŒ–å»ºè®®åˆ©ç”¨åˆ†æå·¥å…·

åŠŸèƒ½ï¼šåˆ†æAIç”Ÿæˆä»£ç æ˜¯å¦æ­£ç¡®åˆ©ç”¨äº†ä¼˜åŒ–å»ºè®®ï¼Œå¯¹æ¯”æ€§èƒ½

ä½¿ç”¨æ–¹æ³•ï¼š
python examples/handwrite_optimization_analyzer.py
"""

import os
import sys
import asyncio
from ai_kernel_generator.config.config_validator import load_config
from ai_kernel_generator.core.task import Task
from ai_kernel_generator.core.async_pool.device_pool import DevicePool
from ai_kernel_generator.core.verifier.kernel_verifier import KernelVerifier


def get_task_desc():
    """è·å–å®é™…ä»»åŠ¡çš„torchä»£ç """
    task_desc = '''
import torch
import torch.nn as nn


class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, input_tensor):
        # Mean along dimension 1
        return torch.mean(input_tensor, 1)


def get_inputs():
    # Batch size: 2000
    # Hidden dimension: 4096
    input_tensor = torch.randn(2000, 4096, dtype=torch.float32)
    return [input_tensor]


def get_init_inputs():
    # No parameters required
    return []
'''
    return task_desc


def get_handwrite_optimization_example():
    """è·å–æ‰‹å†™ä¼˜åŒ–ç¤ºä¾‹"""
    example_name = "sum_fused_elemwise"

    torch_code = '''
import torch
import torch_npu

class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
    
    def forward(self, x, bias):
        t1 = x * 2.0
        t2 = t1 + bias
        t3 = torch.sigmoid(t2)
        t4 = torch.sum(t3, dim=-1, keepdim=True)
        
        return t4

def get_init_inputs():
    return []

def get_inputs():
    x = torch.randn(1000, 8192)
    bias = torch.randn(8192)
    return [x, bias]
'''

    triton_code = '''
import torch
import triton
import triton.language as tl

@triton.autotune(configs=[
        triton.Config({'BLOCK_SIZE_M': 50, 'SUB_BLOCK_SIZE_M': 25, 'BLOCK_SIZE_N': 256}),
        triton.Config({'BLOCK_SIZE_M': 25, 'SUB_BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 2048}),
        triton.Config({'BLOCK_SIZE_M': 25, 'SUB_BLOCK_SIZE_M': 25, 'BLOCK_SIZE_N': 256}),
        triton.Config({'BLOCK_SIZE_M': 20, 'SUB_BLOCK_SIZE_M': 20, 'BLOCK_SIZE_N': 256}),
    ],
    key=['M', 'N']
)
@triton.jit
def sum_fused_elemwise_kernel(
    x_ptr, bias_ptr, output_ptr,
    M, N,
    stride_x_m, stride_x_n,
    stride_out_m,
    BLOCK_SIZE_M: tl.constexpr, 
    SUB_BLOCK_SIZE_M: tl.constexpr, 
    BLOCK_SIZE_N: tl.constexpr 
):
    pid = tl.program_id(0)

    for m_start in range(0, BLOCK_SIZE_M, SUB_BLOCK_SIZE_M):
        m_offsets = pid * BLOCK_SIZE_M + m_start + tl.arange(0, SUB_BLOCK_SIZE_M)
        mmask = m_offsets < M

        acc = tl.zeros([SUB_BLOCK_SIZE_M, BLOCK_SIZE_N], dtype=tl.float32)
        for n_start in range(0, N, BLOCK_SIZE_N):
            n_offsets = n_start + tl.arange(0, BLOCK_SIZE_N)
            nmask = n_offsets < N

            mask = (mmask[:, None]) & (nmask[None, :])
            offsets = m_offsets[:,None] * stride_x_m + n_offsets[None,:] * stride_x_n

            x = tl.load(x_ptr + offsets, mask=mask, other=0.0)
            bias = tl.load(bias_ptr + n_offsets, mask=nmask, other=0.0)
            
            t1 = x * 2.0
            t2 = t1 + bias
            t3 = tl.sigmoid(t2)
            
            acc += tl.where(mask, t3, 0.0)

        total_sum = tl.sum(acc, axis=1)

        output_ptrs = output_ptr + m_offsets * stride_out_m
        tl.store(output_ptrs, total_sum, mask=mmask)


def sum_fused_elemwise_triton_torch(x, bias):
    assert x.dim() == 2
    assert bias.dim() == 1
    assert x.shape[1] == bias.shape[0]
    
    M, N = x.shape
    
    output = torch.empty((M, 1), device=x.device, dtype=x.dtype)
    
    grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE_M']), )
    
    sum_fused_elemwise_kernel[grid](
        x, bias, output,
        M, N,
        x.stride(0), x.stride(1),
        output.stride(0)
    )
    
    return output
'''

    improvement_doc = '''
# ä»»åŠ¡ç‰¹å¾
**æ“ä½œç±»å‹**ï¼šreduction+elementwiseèåˆï¼Œreduceè½´ä¸ºæœ€åä¸€æ ¹è½´ï¼›2D Tensorè¾“å…¥ï¼Œ1D Tensorè¾“å‡º
**æ•°æ®å°ºå¯¸**ï¼š(1000, 8192), (8192,) -> ç®—å­è§„æ ¼è¾ƒå¤§
**æ•°æ®ç±»å‹**ï¼šè¾“å…¥è¾“å‡ºå‡ä¸ºfloat32ç±»å‹
**ä»»åŠ¡ç‰¹ç‚¹**ï¼šé€å…ƒç´ ä¸å½’çº¦è®¡ç®—èåˆï¼Œå…ˆè¿›è¡Œå‘é‡åŒ–é€å…ƒç´ æ“ä½œï¼Œå†æ²¿åˆ—æ–¹å‘æ±‚å’Œå½’çº¦ï¼›é€šè¿‡è¡Œåˆ—åŒå‘åˆ†å—ä¼˜åŒ–å†…å­˜è®¿é—®ï¼Œåˆ©ç”¨forå¾ªç¯å¤„ç†è¡Œåˆ—åˆ†å—ï¼›é‡‡ç”¨Auto-Tuningæœºåˆ¶åŠ¨æ€é€‰æ‹©åˆ†å—é…ç½®ã€‚

# å…³é”®ä¼˜åŒ–ç­–ç•¥

## ä¼˜åŒ–1: è°ƒæ•´è¡Œåˆ‡åˆ†ç­–ç•¥
```python
pid = tl.program_id(0)
for m_start in range(0, BLOCK_SIZE_M, SUB_BLOCK_SIZE_M):
    m_offsets = pid * BLOCK_SIZE_M + m_start + tl.arange(0, SUB_BLOCK_SIZE_M)
```
- æ¯ä¸ªkernelè®¡ç®—å¤šè¡Œï¼ˆBLOCK_SIZE_Mï¼‰ï¼Œå‡å°‘æ€»çº¿ç¨‹å—æ•°é‡
- kernelå†…äºŒæ¬¡åˆ‡åˆ†ï¼ˆSUB_BLOCK_SIZE_Mï¼‰ï¼Œé¿å…è¶…è¿‡ç¡¬ä»¶ç¼“å­˜

## ä¼˜åŒ–2: AutoTuneé…ç½®ä¼˜åŒ–
- gridç­‰äºæ ¸æ•°ï¼ŒSUBåˆ‡åˆ†ä¸å«å°¾å—æ—¶æ€§èƒ½æœ€ä¼˜
- é¿å…å°¾å—è®¡ç®—ï¼Œä¼˜å…ˆé‡‡ç”¨ä¸å«å°¾å—çš„åˆ‡åˆ†æ–¹æ¡ˆ

## ä¼˜åŒ–3: å»¶è¿Ÿå½’çº¦æ“ä½œ
```python
acc = tl.zeros([SUB_BLOCK_SIZE_M, BLOCK_SIZE_N], dtype=tl.float32)
for n_start in range(0, N, BLOCK_SIZE_N):
    acc += tl.where(mask, t3, 0.0)
total_sum = tl.sum(acc, axis=1)
```
- å¼•å…¥ç´¯åŠ å™¨çŸ©é˜µï¼Œå°†å¾ªç¯å†…å¤šæ¬¡å°å½’çº¦åˆå¹¶ä¸ºå¾ªç¯åä¸€æ¬¡æ‰¹é‡å½’çº¦
'''
    
    return example_name, torch_code, triton_code, improvement_doc


async def run_analysis(
    op_name="custom_op",
    task_desc=None,
    example_name=None,
    torch_code=None,
    triton_reference_code=None,
    improvement_doc=None,
    device_id=0,
    run_times=50,
    warmup_times=5
):
    """è¿è¡Œä¼˜åŒ–å»ºè®®åˆ©ç”¨åˆ†æ
    
    Args:
        op_name: ç®—å­åç§°
        task_desc: å®é™…ä»»åŠ¡çš„torchä»£ç ï¼ˆç”¨äºTaskæ‰§è¡Œï¼‰
        example_name: ä¼˜åŒ–ç¤ºä¾‹åç§°
        torch_code: ä¼˜åŒ–ç¤ºä¾‹çš„torchä»£ç ï¼ˆç”¨äºhandwrite_suggestionsï¼‰
        triton_reference_code: æ‰‹å†™çš„tritonå‚è€ƒä»£ç 
        improvement_doc: ä¼˜åŒ–å»ºè®®æ–‡æ¡£
        device_id: è®¾å¤‡ID
        run_times: æ€§èƒ½æµ‹è¯•è¿è¡Œæ¬¡æ•°
        warmup_times: æ€§èƒ½æµ‹è¯•é¢„çƒ­æ¬¡æ•°
    """
    print("="*80)
    print("æ‰‹å†™ä¼˜åŒ–å»ºè®®åˆ©ç”¨åˆ†æå·¥å…·")
    print("="*80)
    print(f"\nğŸ“‹ ç®—å­: {op_name}")
    print(f"ğŸ¯ ç›®æ ‡: å¯¹æ¯”æœ‰æ— ä¼˜åŒ–å»ºè®®çš„æ€§èƒ½å·®å¼‚\n")
    
    # 1. åŠ è½½é…ç½®
    config = load_config(config_path="./python/ai_kernel_generator/config/vllm_triton_ascend_evolve_config.yaml")
    device_pool = DevicePool(device_list=[device_id])
    
    # 2. ä¸å¸¦ä¼˜åŒ–å»ºè®®çš„æµ‹è¯•
    print("ğŸš€ æ­¥éª¤1: ä¸å¸¦ä¼˜åŒ–å»ºè®®çš„ä»£ç ç”Ÿæˆ")
    print("-" * 80)
    
    task_no_hint = Task(
        op_name=op_name,
        task_desc=task_desc,
        task_id="no_hint_001",
        backend="ascend",
        arch="ascend910b4",
        dsl="triton_ascend",
        config=config,
        device_pool=device_pool,
        framework="torch",
        task_type="profile",
        workflow="default_workflow",
        handwrite_suggestions=[]
    )
    
    _, success_no_hint, task_info_no_hint = await task_no_hint.run()
    
    if not success_no_hint:
        print("âŒ ä¸å¸¦ä¼˜åŒ–å»ºè®®çš„Taskæ‰§è¡Œå¤±è´¥")
        return None
    
    print("âœ… ä¸å¸¦ä¼˜åŒ–å»ºè®®çš„Taskæ‰§è¡Œå®Œæˆ\n")
    
    no_hint_code = task_info_no_hint.get('coder_code', '')
    no_hint_gen_time = task_info_no_hint.get('profile_res', {}).get('gen_time', float('inf'))
    no_hint_base_time = task_info_no_hint.get('profile_res', {}).get('base_time', 0.0)
    no_hint_speedup = task_info_no_hint.get('profile_res', {}).get('speedup', 0.0)
    
    # 3. å¸¦ä¼˜åŒ–å»ºè®®çš„æµ‹è¯•
    print("ğŸš€ æ­¥éª¤2: å¸¦ä¼˜åŒ–å»ºè®®çš„ä»£ç ç”Ÿæˆ")
    print("-" * 80)
    
    handwrite_suggestions = [{
        'name': example_name,
        'torch_code': torch_code,
        'triton_code': triton_reference_code,
        'improvement': improvement_doc
    }]
    
    task_with_hint = Task(
        op_name=op_name,
        task_desc=task_desc,
        task_id="with_hint_001",
        backend="ascend",
        arch="ascend910b4",
        dsl="triton_ascend",
        config=config,
        device_pool=device_pool,
        framework="torch",
        task_type="profile",
        workflow="default_workflow",
        handwrite_suggestions=handwrite_suggestions
    )
    
    _, success_with_hint, task_info_with_hint = await task_with_hint.run()
    
    if not success_with_hint:
        print("âŒ å¸¦ä¼˜åŒ–å»ºè®®çš„Taskæ‰§è¡Œå¤±è´¥")
        return None
    
    print("âœ… å¸¦ä¼˜åŒ–å»ºè®®çš„Taskæ‰§è¡Œå®Œæˆ\n")
    
    with_hint_code = task_info_with_hint.get('coder_code', '')
    with_hint_gen_time = task_info_with_hint.get('profile_res', {}).get('gen_time', float('inf'))
    with_hint_base_time = task_info_with_hint.get('profile_res', {}).get('base_time', 0.0)
    with_hint_speedup = task_info_with_hint.get('profile_res', {}).get('speedup', 0.0)
    
    # 4. æ‰“å°å¯¹æ¯”ç»“æœ
    print("="*80)
    print("æ€§èƒ½å¯¹æ¯”ç»“æœ")
    print("="*80)
    print(f"\n{'æŒ‡æ ‡':<20} {'TorchåŸºå‡†':<15} {'æ— ä¼˜åŒ–å»ºè®®':<15} {'æœ‰ä¼˜åŒ–å»ºè®®':<15}")
    print("-" * 80)
    print(f"{'æ—¶é—´(us)':<20} {no_hint_base_time:>14.2f} {no_hint_gen_time:>14.2f} {with_hint_gen_time:>14.2f}")
    print(f"{'åŠ é€Ÿæ¯”':<20} {'1.00x':>14} {no_hint_speedup:>14.2f}x {with_hint_speedup:>14.2f}x")
    
    if no_hint_gen_time > 0:
        improvement_ratio = (no_hint_gen_time - with_hint_gen_time) / no_hint_gen_time * 100
        print(f"{'æ€§èƒ½æå‡':<20} {'-':>14} {'åŸºå‡†':>14} {improvement_ratio:>13.1f}%")
    
    print("="*80)
    
    print("\nä¸å¸¦ä¼˜åŒ–å»ºè®®çš„ä»£ç :")
    print("-" * 80)
    print(no_hint_code[:500] + "..." if len(no_hint_code) > 500 else no_hint_code)
    
    print("\nå¸¦ä¼˜åŒ–å»ºè®®çš„ä»£ç :")
    print("-" * 80)
    print(with_hint_code[:500] + "..." if len(with_hint_code) > 500 else with_hint_code)
    print("="*80)
    
    return {
        'success': True,
        'no_hint': {
            'base_time': no_hint_base_time,
            'gen_time': no_hint_gen_time,
            'speedup': no_hint_speedup,
            'code': no_hint_code
        },
        'with_hint': {
            'base_time': with_hint_base_time,
            'gen_time': with_hint_gen_time,
            'speedup': with_hint_speedup,
            'code': with_hint_code
        },
        'improvement_ratio': improvement_ratio if no_hint_gen_time > 0 else 0.0
    }


async def main():
    """ä¸»å‡½æ•°"""
    device_id = int(os.getenv("DEVICE_ID", "0"))
    
    # è·å–å®é™…ä»»åŠ¡ä»£ç 
    task_desc = get_task_desc()
    
    # è·å–ä¼˜åŒ–ç¤ºä¾‹
    example_name, torch_code, triton_code, improvement_doc = get_handwrite_optimization_example()
    
    result = await run_analysis(
        op_name="custom_op",
        task_desc=task_desc,
        example_name=example_name,
        torch_code=torch_code,
        triton_reference_code=triton_code,
        improvement_doc=improvement_doc,
        device_id=device_id,
        run_times=50,
        warmup_times=5
    )
    
    if result and result['success']:
        print("âœ… åˆ†æå®Œæˆ!\n")
    else:
        print("âŒ åˆ†æå¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
