# Copyright 2025 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio
import os
import argparse
# å¯¼å…¥evolveå‡½æ•°å’Œå¿…è¦çš„æ¨¡å—
from ai_kernel_generator.core.evolve import evolve
from ai_kernel_generator.core.async_pool.task_pool import TaskPool
from ai_kernel_generator.core.worker.manager import register_worker
from ai_kernel_generator.config.config_validator import load_config
from ai_kernel_generator.utils.environment_check import check_env_for_task
from ai_kernel_generator.utils.evolve.runner_manager import RunnerConfig, print_evolve_config, print_evolution_result
from ai_kernel_generator import get_project_root
from pathlib import Path


def get_op_name():
    return 'aikg_relu'


def get_task_desc():
    return '''
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs a ReLU activation.
    """
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies ReLU activation to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of any shape.

        Returns:
            torch.Tensor: Output tensor with ReLU applied, same shape as input.
        """
        return torch.relu(x)

batch_size = 16
dim = 16384

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed
'''


async def run_torch_evolve_triton(worker_mode="local", worker_url=None):
    """
    è¿è¡ŒTritonè¿›åŒ–ç¤ºä¾‹
    
    Args:
        worker_mode: "local" æˆ– "remote"ï¼ŒæŒ‡å®šä½¿ç”¨æœ¬åœ°è¿˜æ˜¯è¿œç¨‹ Worker
        worker_url: å½“ worker_mode="remote" æ—¶ï¼ŒæŒ‡å®šè¿œç¨‹ Worker Service çš„ URL
    """
    # åˆ›å»ºé…ç½®å¯¹è±¡å¹¶è®¾ç½®ç¡¬ç¼–ç å‚æ•°
    config = RunnerConfig()

    # åŸºç¡€é…ç½®
    config.dsl = "triton_cuda"  # ä½¿ç”¨triton_cudaæ›¿ä»£é€šç”¨çš„triton
    config.framework = "torch"
    config.backend = "cuda"
    config.arch = "a100"

    # è¿›åŒ–å‚æ•°
    config.max_rounds = 2
    config.parallel_num = 2

    # å²›å±¿æ¨¡å‹å‚æ•°
    config.num_islands = 2
    config.migration_interval = 2
    config.elite_size = 5
    config.parent_selection_prob = 0.5

    # è®¾å¤‡é…ç½®
    config.device_list = [0]

    # é…ç½®æ–‡ä»¶è·¯å¾„
    config.config_path = str(Path(get_project_root()) / "config" / "vllm_triton_cuda_evolve_config.yaml")

    # é€‰æ‹©è¦è¿è¡Œçš„ä»»åŠ¡
    config.op_name = get_op_name()
    config.task_desc = get_task_desc()

    # æ‰“å°é…ç½®ä¿¡æ¯
    print_evolve_config(config.op_name, config)
    
    # æ‰“å° Worker æ¨¡å¼
    print(f"\n{'='*60}")
    print(f"Worker æ¨¡å¼: {worker_mode.upper()}")
    if worker_mode == "remote":
        worker_url = worker_url or os.getenv("AIKG_WORKER_URL")
        if worker_url:
            print(f"Remote Worker URL: {worker_url}")
        else:
            print(f"Remote Worker URL: å°†ä»ç¯å¢ƒå˜é‡ AIKG_WORKER_URL è¯»å–")
    print(f"{'='*60}\n")

    # åˆå§‹åŒ–èµ„æºæ± 
    task_pool = TaskPool(max_concurrency=config.parallel_num)
    
    # æ ¹æ® worker_mode è®¾ç½® worker
    if worker_mode == "remote":
        target_worker_url = worker_url or os.getenv("AIKG_WORKER_URL")
        print(f"ğŸ”— æ³¨å†Œ RemoteWorker (url={target_worker_url or 'AIKG_WORKER_URL'})")
        await register_worker(
            backend=config.backend,
            arch=config.arch,
            worker_url=target_worker_url
        )
    else:
        print(f"ğŸ”— æ³¨å†Œ LocalWorker: devices={config.device_list}")
        await register_worker(
            backend=config.backend,
            arch=config.arch,
            device_ids=config.device_list
        )
    print()

    # åŠ è½½é…ç½®å¹¶æ£€æŸ¥ç¯å¢ƒ
    loaded_config = load_config(config_path=config.config_path)
    
    # Remote æ¨¡å¼è·³è¿‡ç¡¬ä»¶æ£€æŸ¥
    is_remote = (worker_mode == "remote")
    check_env_for_task(
        config.framework, 
        config.backend, 
        config.dsl, 
        loaded_config,
        is_remote=is_remote
    )

    # è°ƒç”¨evolveå‡½æ•°
    print("å¼€å§‹è¿›åŒ–è¿‡ç¨‹...")
    evolution_result = await evolve(
        op_name=config.op_name,
        task_desc=config.task_desc,
        dsl=config.dsl,
        framework=config.framework,
        backend=config.backend,
        arch=config.arch,
        config=loaded_config,
        task_pool=task_pool,
        max_rounds=config.max_rounds,
        parallel_num=config.parallel_num,
        num_islands=config.num_islands,
        migration_interval=config.migration_interval,
        elite_size=config.elite_size,
        parent_selection_prob=config.parent_selection_prob
    )

    # æ‰“å°ç»“æœ
    print_evolution_result(evolution_result, config)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="è¿è¡Œ Triton è¿›åŒ–ç¤ºä¾‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨æœ¬åœ° Workerï¼ˆé»˜è®¤ï¼‰
  python run_torch_evolve_triton.py
  
  # ä½¿ç”¨è¿œç¨‹ Workerï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ï¼‰
  export AIKG_WORKER_URL=http://localhost:9001
  python run_torch_evolve_triton.py --worker remote
  
  # ä½¿ç”¨è¿œç¨‹ Workerï¼ˆæŒ‡å®š URLï¼‰
  python run_torch_evolve_triton.py --worker remote --worker-url http://192.168.1.100:9001
        """
    )
    parser.add_argument(
        "--worker",
        choices=["local", "remote"],
        default="local",
        help="Worker æ¨¡å¼: local (æœ¬åœ°) æˆ– remote (è¿œç¨‹)ï¼Œé»˜è®¤: local"
    )
    parser.add_argument(
        "--worker-url",
        type=str,
        default=None,
        help="è¿œç¨‹ Worker Service çš„ URLï¼ˆä»… remote æ¨¡å¼éœ€è¦ï¼‰ã€‚ä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡ AIKG_WORKER_URL è®¾ç½®"
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("Triton è¿›åŒ–ç¤ºä¾‹")
    print("=" * 60)
    
    if args.worker == "remote":
        worker_url = args.worker_url or os.getenv("AIKG_WORKER_URL")
        if worker_url:
            print(f"\nâš ï¸  Remote Worker æ¨¡å¼")
            print(f"   ç¡®ä¿è¿œç¨‹ Worker Service æ­£åœ¨è¿è¡Œ: {worker_url}")
            print(f"   å¦‚æœä½¿ç”¨ SSH éš§é“ï¼Œç¡®ä¿éš§é“å·²å»ºç«‹")
        else:
            print(f"\nâš ï¸  Remote Worker æ¨¡å¼")
            print(f"   è¯·è®¾ç½®ç¯å¢ƒå˜é‡ AIKG_WORKER_URL æˆ–ä½¿ç”¨ --worker-url å‚æ•°")
        print()
    
    asyncio.run(run_torch_evolve_triton(worker_mode=args.worker, worker_url=args.worker_url))
