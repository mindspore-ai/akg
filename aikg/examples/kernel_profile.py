# Copyright 2025 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Kernelæ€§èƒ½å¯¹æ¯”å·¥å…·
ç”¨äºå¯¹æ¯”triton-ascendå’ŒåŸç”Ÿtorch_npuçš„æ€§èƒ½

ä½¿ç”¨æ–¹æ³•ï¼š
1. ä¿®æ”¹ get_custom_op_torch_code() å‡½æ•°ï¼Œå¡«å…¥ä½ çš„torchå®ç°ä»£ç 
2. ä¿®æ”¹ get_custom_op_triton_code() å‡½æ•°ï¼Œå¡«å…¥ä½ çš„tritonå®ç°ä»£ç 
3. è¿è¡Œ python examples/kernel_profile.py å³å¯å¾—åˆ°æ€§èƒ½å¯¹æ¯”ç»“æœ
"""

import os
from ai_kernel_generator.core.verifier.kernel_verifier import KernelVerifier
from ai_kernel_generator.config.config_validator import load_config


def get_custom_op_torch_code():
    """è·å–è‡ªå®šä¹‰ç®—å­çš„torchå®ç°ä»£ç ï¼ˆç¤ºä¾‹ï¼šReLUï¼‰"""
    return '''
import torch
import torch.nn as nn


class Model(nn.Module):

    def __init__(self):
        super(Model, self).__init__()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.relu(x)


batch_size = 16
dim = 16384


def get_inputs():
    x = torch.randn(batch_size, dim)
    return [x]


def get_init_inputs():
    return []  # No special initialization inputs needed
'''


def get_custom_op_triton_code():
    """è·å–è‡ªå®šä¹‰ç®—å­çš„tritonå®ç°ä»£ç ï¼ˆç¤ºä¾‹ï¼šReLUï¼‰"""
    return '''
import torch
import triton
import triton.language as tl


@triton.jit
def custom_op_kernel(
    x_ptr,  # è¾“å…¥æŒ‡é’ˆ
    output_ptr,  # è¾“å‡ºæŒ‡é’ˆ
    n_elements,  # æ€»å…ƒç´ æ•°
    BLOCK_SIZE: tl.constexpr,  # æ¯ä¸ªblockå¤„ç†çš„å…ƒç´ æ•°
):
    # è·å–ç¨‹åºID
    pid = tl.program_id(axis=0)
    # è®¡ç®—è¿™ä¸ªblockçš„èµ·å§‹ä½ç½®
    block_start = pid * BLOCK_SIZE
    # åˆ›å»ºåç§»é‡
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    # åˆ›å»ºæ©ç ï¼Œç¡®ä¿ä¸è¶Šç•Œ
    mask = offsets < n_elements

    # åŠ è½½è¾“å…¥æ•°æ®
    x = tl.load(x_ptr + offsets, mask=mask)

    # æ‰§è¡Œè®¡ç®—: è¿™é‡Œæ˜¯ReLUç¤ºä¾‹ max(0, x)
    output = tl.maximum(x, 0.0)

    # å­˜å‚¨ç»“æœ
    tl.store(output_ptr + offsets, output, mask=mask)


def custom_op_triton_torch(x):
    x = x.contiguous()
    n_elements = x.numel()
    output = torch.empty_like(x, device=x.device)

    BLOCK_SIZE = 1024
    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)

    # å¯åŠ¨kernel
    custom_op_kernel[grid](
        x, output, n_elements,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    return output
'''


def run_kernel_profile(
    op_name="custom_op",
    op_task_str=None,
    kernel_code=None,
    framework="torch",
    dsl="triton",
    backend="ascend",
    arch="ascend910b4",
    device_id=0,
    run_times=50,
    warmup_times=5
):
    """
    è¿è¡Œkernelæ€§èƒ½å¯¹æ¯”æµ‹è¯•
    
    Args:
        op_name: ç®—å­åç§°
        op_task_str: æ¡†æ¶å®ç°ä»£ç ï¼ˆå¦‚torchå®ç°ï¼‰
        kernel_code: DSLå®ç°ä»£ç ï¼ˆå¦‚tritonå®ç°ï¼‰
        framework: æ¡†æ¶åç§°ï¼Œé»˜è®¤"torch"
        dsl: DSLåç§°ï¼Œé»˜è®¤"triton"
        backend: åç«¯åç§°ï¼Œé»˜è®¤"ascend"
        arch: æ¶æ„åç§°ï¼Œé»˜è®¤"ascend910b4"
        device_id: è®¾å¤‡IDï¼Œé»˜è®¤0
        run_times: è¿è¡Œæ¬¡æ•°ï¼Œé»˜è®¤50
        warmup_times: é¢„çƒ­æ¬¡æ•°ï¼Œé»˜è®¤5
        
    Returns:
        tuple: (gen_time, base_time, speedup) ç”Ÿæˆä»£ç æ€§èƒ½ã€åŸå§‹æ€§èƒ½ã€åŠ é€Ÿæ¯”
    """
    # åŠ è½½é…ç½®
    config = load_config(dsl)
    
    # åˆ›å»ºéªŒè¯å™¨
    impl_func_name = f"{op_name}_{dsl}_{framework}"
    verifier = KernelVerifier(
        op_name=op_name,
        framework_code=op_task_str,
        task_id="kernel_profile_001",
        framework=framework,
        dsl=dsl,
        backend=backend,
        arch=arch,
        impl_func_name=impl_func_name,
        config=config
    )
    
    task_info = {"coder_code": kernel_code}
    
    # å…ˆè¿›è¡ŒéªŒè¯ï¼Œç¡®ä¿éªŒè¯é€šè¿‡
    print(f"æ­£åœ¨éªŒè¯ {op_name} kernelçš„æ­£ç¡®æ€§...")
    result, error_log = verifier.run(task_info, device_id=device_id)
    if not result:
        print(f"âŒ éªŒè¯å¤±è´¥: {error_log}")
        return None, None, None
    
    print("âœ… æ­£ç¡®æ€§éªŒè¯é€šè¿‡ï¼")
    print("\n" + "="*60)
    print(f"å¼€å§‹æ€§èƒ½æµ‹è¯• (é¢„çƒ­ {warmup_times} æ¬¡ï¼Œæµ‹è¯• {run_times} æ¬¡)")
    print("="*60 + "\n")
    
    # è¿›è¡Œæ€§èƒ½åˆ†æ
    profile_settings = {
        "run_times": run_times,
        "warmup_times": warmup_times
    }
    gen_time, base_time, speedup = verifier.run_profile(
        current_step=0, device_id=device_id, profile_settings=profile_settings
    )
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("æ€§èƒ½æµ‹è¯•ç»“æœ")
    print("="*60)
    print(f"ğŸ“Š åŸç”Ÿ {framework} æ€§èƒ½:     {base_time:.2f} us")
    print(f"ğŸš€ {dsl} kernel æ€§èƒ½:      {gen_time:.2f} us")
    print(f"âš¡ åŠ é€Ÿæ¯”:                 {speedup:.2f}x")
    print("="*60 + "\n")
    
    return gen_time, base_time, speedup


def main():
    """ä¸»å‡½æ•° - è‡ªå®šä¹‰ç®—å­æ€§èƒ½æµ‹è¯•ï¼ˆç¤ºä¾‹ï¼šReLUï¼‰"""
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰
    device_id = int(os.getenv("DEVICE_ID", "0"))
    
    # è·å–op_task_strå’Œkernel_code
    # æç¤ºï¼šç›´æ¥ä¿®æ”¹ get_custom_op_torch_code() å’Œ get_custom_op_triton_code() 
    # ä¸¤ä¸ªå‡½æ•°ä¸­çš„ä»£ç å³å¯æµ‹è¯•ä¸åŒçš„ç®—å­
    op_task_str = get_custom_op_torch_code()
    kernel_code = get_custom_op_triton_code()
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    run_kernel_profile(
        op_name="custom_op",
        op_task_str=op_task_str,
        kernel_code=kernel_code,
        framework="torch",
        dsl="triton",
        backend="ascend",
        arch="ascend910b4",
        device_id=device_id,
        run_times=50,
        warmup_times=5
    )


if __name__ == "__main__":
    main()

