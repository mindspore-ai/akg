import os
import json
import numpy as np
from typing import Union, Literal, List, Tuple, Any
{% if framework == "torch" %}
import torch
from {{ op_name }}_torch import Model as FrameworkModel, get_inputs, get_init_inputs
TensorType = torch.Tensor
{% elif framework == "numpy" %}
from {{ op_name }}_numpy import Model as FrameworkModel, get_inputs, get_init_inputs
TensorType = np.ndarray
{% elif framework == "mindspore" %}
import mindspore as ms
from mindspore.common import np_dtype
from {{ op_name }}_mindspore import Model as FrameworkModel, get_inputs, get_init_inputs
TensorType = ms.Tensor
MS_TO_NP_DTYPE_MAP = {
    ms.float32: np.float32,
    ms.float16: np.float16,
    ms.bfloat16: np_dtype.bfloat16,
    ms.int8: np.int8,
    ms.int16: np.int16,
    ms.int32: np.int32,
    ms.int64: np.int64,
    ms.uint8: np.uint8,
    ms.uint16: np.uint16,
    ms.uint32: np.uint32,
    ms.uint64: np.uint64,
    ms.bool_: np.bool_,
}
{% endif %}

{% if "triton" in dsl %}
from {{ op_name }}_triton import {{ impl_func_name }}
import triton
import triton.testing
{% elif dsl == "swft" %}
from {{ op_name }}_swft import {{ impl_func_name }}
{% endif %}

def run_base_implementations():
    """重复测试"""
    # 获取运行模式
    backend = "{{ backend }}"  # 计算设备后端
    arch = "{{ arch }}"  # 硬件架构
    dsl = "{{ dsl }}"  # 实现方式
    
    {% if framework == "torch" %}
    torch.manual_seed(0)
    # PyTorch设备设置
    if backend == "cuda":
        os.environ['CUDA_VISIBLE_DEVICES'] = str({{ device_id }})
        device = torch.device("cuda")
        # 设置CUDA架构
        if arch == "a100":
            torch.cuda.set_device(0)  # 使用第一个a100设备
        elif arch == "v100":
            torch.cuda.set_device(0)  # 使用第一个v100设备
    elif backend == "ascend":
        if "ascend910" in arch:
            {% if dsl == "triton-russia" %}
            os.environ['TRITON_ASCEND_DEVICE'] = str({{ device_id }})
            device = torch.device("cpu")
            {% else %}
            os.environ['DEVICE_ID'] = str({{ device_id }})
            device = torch.device("npu")
            torch.npu.manual_seed(0)
            torch.npu.set_device({{ device_id }})
            {% endif %}
        elif "ascend310" in arch:
            os.environ['DEVICE_ID'] = str({{ device_id }})
            device = torch.device("cpu")
        else:
            raise ValueError(f"不支持的ascend架构: {arch}")
    elif backend == "cpu":
        device = torch.device("cpu")
    else:
        raise ValueError(f"不支持的后端: {backend}")
    {% elif framework == "numpy" %}
    # numpy 不需要设备设置
    {% elif framework == "mindspore" %}
    # MindSpore设备设置
    os.environ['DEVICE_ID'] = str({{ device_id }})
    ms.set_seed(0)
    if backend == "ascend":
        device = "Ascend"
        # 设置Ascend架构
        if arch not in ["ascend910b4", "ascend310p3"]:
            raise ValueError(f"不支持的ascend架构: {arch}")
    elif backend == "cpu":
        device = "CPU"
    else:
        raise ValueError(f"MindSpore不支持的后端: {backend}")
    {% endif %}

    # 获取初始化参数和输入数据
    init_params = get_init_inputs()
    framework_model = FrameworkModel(*init_params)
    
    {% if framework == "torch" %}
    framework_model = framework_model.to(device)
    # 处理输入数据
    inputs = get_inputs()
    def process_input(x):
        if isinstance(x, torch.Tensor):
            return x.to(device)
        elif isinstance(x, np.ndarray):
            return torch.from_numpy(x).to(device)
        elif isinstance(x, (list, tuple)):
            return type(x)(process_input(item) for item in x)
        elif isinstance(x, (int, float, bool, type(None))):
            return x
        else:
            try:
                return x.to(device)
            except (AttributeError, TypeError):
                return x
    inputs = [process_input(x) for x in inputs]
    {% elif framework == "numpy" %}
    inputs = get_inputs()
    {% elif framework == "mindspore" %}
    inputs = get_inputs()
    {% endif %}
    
    {% if "triton" in dsl %}
    # 使用triton.testing.do_bench进行基准测试
    def base_benchmark_fn():
        result = framework_model(*inputs)
        return result
    
    # 使用do_bench测量执行时间
    execution_time = triton.testing.do_bench(
        base_benchmark_fn,
        warmup={{ warmup_times }},
        rep={{ run_times }},
        return_mode="min"
    )
    
    # 保存时间结果到文件
    result_data = {
        "execution_time_ms": execution_time,  # triton do_bench返回毫秒
        "execution_time_us": execution_time * 1000,  # 转换为微秒以保持兼容性
        "warmup_times": {{ warmup_times }},
        "run_times": {{ run_times }},
        "method": "triton_do_bench"
    }
    
    with open("base_profile_result.json", "w") as f:
        json.dump(result_data, f, indent=2)
    
    print(f"[{{ op_name }}] Base execution time: {execution_time:.4f} ms")
    
    {% else %}
    # 非triton实现，使用传统循环计时
    # 运行框架实现
    for _ in range({{ total_count }}):
        framework_output = framework_model(*inputs)
        {% if backend == "cuda" %}
        torch.cuda.synchronize()
        {% endif %}
    {% endif %}

if __name__ == "__main__":
    run_base_implementations() 