import os
import json
import numpy as np
from typing import Union, Literal, List, Tuple, Any
{% if framework == "torch" %}
import torch
from {{ op_name }}_torch import Model as FrameworkModel, get_init_inputs
{% if is_dynamic_shape %}
from {{ op_name }}_torch import get_inputs_dyn_list
{% else %}
from {{ op_name }}_torch import get_inputs
{% endif %}
TensorType = torch.Tensor
{% elif framework == "numpy" %}
from {{ op_name }}_numpy import Model as FrameworkModel, get_init_inputs
{% if is_dynamic_shape %}
from {{ op_name }}_numpy import get_inputs_dyn_list
{% else %}
from {{ op_name }}_numpy import get_inputs
{% endif %}
TensorType = np.ndarray
{% elif framework == "mindspore" %}
import mindspore as ms
from mindspore.common import np_dtype
from {{ op_name }}_mindspore import Model as FrameworkModel, get_init_inputs
{% if is_dynamic_shape %}
from {{ op_name }}_mindspore import get_inputs_dyn_list
{% else %}
from {{ op_name }}_mindspore import get_inputs
{% endif %}
TensorType = ms.Tensor
MS_TO_NP_DTYPE_MAP = {
    ms.float32: np.float32,
    ms.float16: np.float16,
    ms.bfloat16: np_dtype.bfloat16,
    ms.int8: np.int8,
    ms.int16: np.int16,
    ms.int32: np.int32,
    ms.int64: np.int64,
    ms.uint8: np.uint8,
    ms.uint16: np.uint16,
    ms.uint32: np.uint32,
    ms.uint64: np.uint64,
    ms.bool_: np.bool_,
}
{% endif %}

{% if "triton_cuda" in dsl or "triton_ascend" in dsl %}
{% if backend == "ascend" %}
try:
    from ai_kernel_generator.utils.triton_autotune_patch import apply_triton_patches
    apply_triton_patches()
except ImportError:
    pass
{% endif %}
from {{ op_name }}_triton import {{ impl_func_name }}
{% elif dsl == "swft" %}
from {{ op_name }}_swft import {{ impl_func_name }}
{% elif dsl == "cuda_c" %}
from {{ op_name }}_cuda_c import {{ impl_func_name }}
{% elif dsl == "cpp" %}
from {{ op_name }}_cpp import {{ impl_func_name }}
{% endif %}

{% if framework == "torch" %}
def save_tensor(tensor: TensorType, bin_path: str):
    """将PyTorch张量保存为二进制文件"""
    tensor_contiguous = tensor.contiguous().cpu()
    uint8_view = tensor_contiguous.view(torch.uint8)
    with open(bin_path, 'wb') as f:
        f.write(uint8_view.numpy().tobytes())

def load_tensor(bin_path: str, expect_tensor: TensorType) -> TensorType:
    """从二进制文件加载PyTorch张量"""
    with open(bin_path, 'rb') as f:
        data = f.read()
        uint8_tensor = torch.frombuffer(data, dtype=torch.uint8)
        return uint8_tensor.view(expect_tensor.dtype).reshape(expect_tensor.shape)
{% elif framework == "numpy" %}
def save_tensor(tensor: TensorType, bin_path: str):
    """将numpy数组保存为二进制文件"""
    uint8_view = tensor.view(np.uint8)
    uint8_view.tofile(bin_path)

def load_tensor(bin_path: str, expect_tensor: TensorType) -> TensorType:
    """从二进制文件加载numpy数组"""
    uint8_array = np.fromfile(bin_path, dtype=np.uint8)
    arr = uint8_array.view(expect_tensor.dtype).reshape(expect_tensor.shape)
    return arr.astype(expect_tensor.dtype)
{% elif framework == "mindspore" %}
def save_tensor(tensor: TensorType, bin_path: str):
    """将MindSpore张量保存为二进制文件"""
    tensor_np = tensor.asnumpy()
    uint8_view = tensor_np.view(np.uint8)
    with open(bin_path, 'wb') as f:
        f.write(uint8_view.tobytes())

def load_tensor(bin_path: str, expect_tensor: TensorType) -> TensorType:
    """从二进制文件加载MindSpore张量"""
    with open(bin_path, 'rb') as f:
        data = f.read()
        uint8_array = np.frombuffer(data, dtype=np.uint8)
        numpy_dtype = MS_TO_NP_DTYPE_MAP.get(expect_tensor.dtype)
        if numpy_dtype is None:
            raise ValueError(f"不支持的数据类型: {expect_tensor.dtype}")
        numpy_tensor = uint8_array.view(numpy_dtype).reshape(expect_tensor.shape)
        return ms.Tensor(numpy_tensor, dtype=expect_tensor.dtype)
{% endif %}

def gen_binary_data(inputs, outputs, data_dir):
    """生成二进制数据文件
    
    Args:
        inputs: 输入张量列表
        outputs: 输出张量列表或单个张量
        data_dir: 数据保存目录
    """
    os.makedirs(data_dir, exist_ok=True)
    
    # 创建输入输出目录
    input_dir = os.path.join(data_dir, "{{ op_name }}", "input")
    output_dir = os.path.join(data_dir, "{{ op_name }}", "output")
    os.makedirs(input_dir, exist_ok=True)
    os.makedirs(output_dir, exist_ok=True)
    
    # 保存输入数据
    for i, input_tensor in enumerate(inputs):
        if isinstance(input_tensor, TensorType):
            bin_path = os.path.join(input_dir, f"input{i}.bin")
            save_tensor(input_tensor, bin_path)
    
    # 处理输出数据
    if not isinstance(outputs, (list, tuple)):
        outputs = [outputs]  # 将单个张量转换为列表
    
    # 保存golden输出
    for i, output_tensor in enumerate(outputs):
        if isinstance(output_tensor, TensorType):
            golden_path = os.path.join(output_dir, f"output{i}_golden.bin")
            save_tensor(output_tensor, golden_path)

def load_binary_data(data_dir, reference_outputs):
    """加载二进制数据文件并转换为张量
    
    Args:
        data_dir: 数据目录
        reference_outputs: 参考输出张量列表或单个张量，用于确定数据类型和形状
    
    Returns:
        加载的张量列表
    """
    if not isinstance(reference_outputs, (list, tuple)):
        reference_outputs = [reference_outputs]
    
    output_dir = os.path.join(data_dir, "{{ op_name }}", "output")
    loaded_outputs = []
    i = 0
    while True:
        output_path = os.path.join(output_dir, f"output{i}_actual.bin")
        if not os.path.exists(output_path):
            break
        if i >= len(reference_outputs):
            raise RuntimeError(f"输出文件数量({i+1})超过参考输出数量({len(reference_outputs)})")
        loaded_outputs.append(load_tensor(output_path, reference_outputs[i]))
        i += 1
    
    if not loaded_outputs:
        raise RuntimeError("未找到任何输出文件, 一般是因为输入数据类型和原任务的输入数据类型不匹配")
    
    return loaded_outputs

def run_generation_implementations():
    """运行生成实现"""
    # 获取运行模式
    backend = "{{ backend }}"  # 计算设备后端
    arch = "{{ arch }}"  # 硬件架构
    dsl = "{{ dsl }}"  # 实现方式
    
    {% if framework == "torch" %}
    torch.manual_seed(0)
    # PyTorch设备设置
    if backend == "cuda":
        os.environ['CUDA_VISIBLE_DEVICES'] = str({{ device_id }})
        device = torch.device("cuda")
        torch.cuda.set_device(0)  # 使用第一个CUDA设备
    elif backend == "ascend":
        if "ascend910" in arch:
            {% if dsl == "triton-russia" %}
            os.environ['TRITON_ASCEND_DEVICE'] = str({{ device_id }})
            device = torch.device("cpu")
            {% else %}
            os.environ['DEVICE_ID'] = str({{ device_id }})
            device = torch.device("npu")
            torch.npu.manual_seed(0)
            torch.npu.set_device({{ device_id }})
            {% endif %}
        elif "ascend310" in arch:
            os.environ['DEVICE_ID'] = str({{ device_id }})
            device = torch.device("cpu")
        else:
            raise ValueError(f"不支持的ascend架构: {arch}")
    elif backend == "cpu":
        device = torch.device("cpu")
    else:
        raise ValueError(f"不支持的后端: {backend}")
    {% elif framework == "numpy" %}
    # numpy 不需要设备设置
    {% elif framework == "mindspore" %}
    # MindSpore设备设置
    os.environ['DEVICE_ID'] = str({{ device_id }})
    ms.set_seed(0)
    if backend == "ascend":
        device = "Ascend"
        # 设置Ascend架构
        # 支持 ascend910b1/b2/b2c/b3/b4 和 ascend310p3
        supported_ascend_archs = ["ascend910b1", "ascend910b2", "ascend910b2c", "ascend910b3", "ascend910b4", "ascend310p3"]
        if arch not in supported_ascend_archs:
            raise ValueError(f"不支持的ascend架构: {arch}，仅支持ascend910b1/b2/b2c/b3/b4和ascend310p3")
    elif backend == "cpu":
        device = "CPU"
    else:
        raise ValueError(f"MindSpore不支持的后端: {backend}")
    {% endif %}

    # 获取初始化参数和输入数据
    init_params = get_init_inputs()
    framework_model = FrameworkModel(*init_params)
    
    {% if framework == "torch" %}
    framework_model = framework_model.to(device)
    {% endif %}
    
    def process_input(x):
        {% if framework == "torch" %}
        if isinstance(x, torch.Tensor):
            return x.to(device)
        elif isinstance(x, np.ndarray):
            return torch.from_numpy(x).to(device)
        elif isinstance(x, (list, tuple)):
            return type(x)(process_input(item) for item in x)
        elif isinstance(x, (int, float, bool, type(None))):
            return x
        else:
            try:
                return x.to(device)
            except (AttributeError, TypeError):
                return x
        {% else %}
        return x
        {% endif %}
    
    def run_benchmark(inputs, case_idx=0):
        """运行基准测试"""
        {% if "triton_cuda" in dsl or "triton_ascend" in dsl %}
        {% if backend == "ascend" %}
        try:
            from ai_kernel_generator.core.verifier.profiler import profiler_npu
            from ai_kernel_generator.utils.triton_autotune_patch import get_collected_config_timings, clear_collected_config_timings
            # 清除之前的配置信息
            clear_collected_config_timings()
            patch_imported = True
        except ImportError:
            get_collected_config_timings = lambda: {}
            clear_collected_config_timings = lambda: None
            patch_imported = False
        
        # 清除缓存确保重新autotune
        if hasattr({{ impl_func_name }}, 'cache'):
            {{ impl_func_name }}.cache.clear()
        
        # 触发autotune
        {{ impl_func_name }}(*inputs)
        
        # 获取收集的配置信息
        config_timings = get_collected_config_timings()
        
        # 保存autotune信息到当前文件夹
        if config_timings:
            autotune_filename = f"autotune_info_case_{case_idx}.json"
            try:
                with open(autotune_filename, 'w') as f:
                    json.dump(config_timings, f, indent=2, ensure_ascii=False)
                print(f"[{{ op_name }}] Autotune info saved to {autotune_filename}")
            except Exception as e:
                print(f"[{{ op_name }}] Warning: Failed to save autotune info: {e}")
        {% endif %}
        
        # 进行最终的性能测试
        def triton_benchmark_fn():
            result = {{ impl_func_name }}(*inputs)
            return result
        
        if backend == "ascend" and patch_imported:
            execution_time_us = profiler_npu(
                triton_benchmark_fn,
                warmup={{ warmup_times }},
                active={{ run_times }},
                prof_dir_name="prof_generation_output",
                keep_res=False,
                suppress_warnings=True
            )
            execution_time_ms = execution_time_us / 1000
            method = "profiler_npu"
        else:
            # GPU环境或补丁导入失败：使用标准do_bench
            import triton.testing
            execution_time_ms = triton.testing.do_bench(
                triton_benchmark_fn,
                warmup={{ warmup_times }},
                rep={{ run_times }},
                return_mode="min"
            )
            method = "triton_do_bench"

        {% elif dsl == "cpp" %}
        # CPU
        import time
        def cpp_benchmark_fn():
            return {{ impl_func_name }}(*inputs)
        # 执行 warmup
        for _ in range({{ warmup_times }}):
            _ = cpp_benchmark_fn()
        # 计时 rep 次
        start_t = time.perf_counter()
        for _ in range({{ run_times }}):
            _ = cpp_benchmark_fn()
        end_t = time.perf_counter()
        execution_time_ms = (end_t - start_t) * 1000.0 / max({{ run_times }}, 1)
        method = "cpu_loop_timer"
        
        {% elif dsl == "swft" %}
        # 运行SWFT实现
        data_dir = os.path.dirname(__file__)
        
        # 生成二进制数据文件
        framework_output = framework_model(*inputs)
        gen_binary_data(inputs, framework_output, data_dir)
        
        # 运行SWFT实现
        import time
        start_time = time.time()
        for _ in range({{ total_count }}):
            {{ impl_func_name }}(device_id=int({{ device_id }}))
        end_time = time.time()
        execution_time_ms = (end_time - start_time) * 1000 / {{ total_count }}  # 转换为毫秒
        method = "traditional_timing"
        {% else %}
        # dsl：cuda_c
        import time
        start_time = time.time()
        for _ in range({{ total_count }}):
            framework_output = framework_model(*inputs)
            {% if backend == "cuda" %}
            torch.cuda.synchronize()
            {% endif %}
        end_time = time.time()
        execution_time_ms = (end_time - start_time) * 1000 / {{ total_count }}  # 转换为毫秒
        method = "traditional_timing"
        {% endif %}

        return execution_time_ms, method
    
    {% if is_dynamic_shape %}
    # 动态shape：获取多组输入数据
    inputs_list = get_inputs_dyn_list()
    
    # 对每组输入进行性能测试
    all_execution_times = []
    for case_idx, inputs in enumerate(inputs_list):
        {% if framework == "torch" %}
        inputs = [process_input(x) for x in inputs]
        {% endif %}
        
        execution_time, method = run_benchmark(inputs, case_idx=case_idx)
        all_execution_times.append(execution_time)
        print(f"[{{ op_name }}] Case {case_idx + 1} execution time: {execution_time * 1000:.4f} us")
    
    # 计算平均执行时间
    avg_execution_time = sum(all_execution_times) / len(all_execution_times)
    
    # 保存时间结果到文件
    result_data = {
        "execution_time_ms": avg_execution_time,
        "execution_time_us": avg_execution_time * 1000,
        "case_count": len(inputs_list),
        "case_times": all_execution_times,
        "warmup_times": {{ warmup_times }},
        "run_times": {{ run_times }},
        "method": method,
        "shape_type": "dynamic"
    }
    
    print(f"[{{ op_name }}] Average execution time: {avg_execution_time * 1000:.4f} us")
    
    {% else %}
    # 静态shape：获取单组输入数据
    {% if framework == "torch" %}
    inputs = get_inputs()
    inputs = [process_input(x) for x in inputs]
    {% elif framework == "numpy" %}
    inputs = get_inputs()
    {% elif framework == "mindspore" %}
    inputs = get_inputs()
    {% endif %}
    
    execution_time, method = run_benchmark(inputs, case_idx=0)
    
    # 保存时间结果到文件
    result_data = {
        "execution_time_ms": execution_time,
        "execution_time_us": execution_time * 1000,
        "warmup_times": {{ warmup_times }},
        "run_times": {{ run_times }},
        "method": method,
        "shape_type": "static"
    }
    
    print(f"[{{ op_name }}] Generation execution time: {execution_time * 1000:.4f} us")
    {% endif %}
    
    with open("generation_profile_result.json", "w") as f:
        json.dump(result_data, f, indent=2)

if __name__ == "__main__":
    run_generation_implementations()