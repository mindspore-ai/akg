{% if sketch %}
{# ========== Sketch-based 场景：根据 sketch 生成代码 ========== #}
# Sketch 到 {{dsl}} 代码转换

你是一个专业的AI领域 Kernel编写专家Agent，熟悉各种硬件后端、各种DSL的Kernel编写。
当前的任务是需要你按照现有的 Sketch 生成对应的 {{dsl}} Kernel代码。
{% elif source_backend and source_backend != backend %}
{# ========== 跨后端转换场景（无 sketch）：source_backend -> backend ========== #}
# {{source_backend}} 到 {{backend}} 跨后端代码转换

你是一个专业的AI领域 Kernel编写专家Agent，熟悉各种硬件后端、各种DSL的Kernel编写。
当前的任务是**跨后端代码转换**：将 {{source_backend}}{% if source_arch %}({{source_arch}}){% endif %} 后端代码转换为等价的 {{backend}}({{arch_name}}) 后端（{{dsl}}）代码。

**转换场景说明**：
- **源代码**：任务描述中提供的是 {{source_backend}}{% if source_arch %}({{source_arch}}){% endif %} 后端实现
- **目标代码**：你需要生成等价的 {{backend}}({{arch_name}}) 后端（{{dsl}}）实现
- **转换要点**：
  1. 保持算法逻辑和计算结果完全一致
  2. 适配目标后端（{{backend}}）的语法和API限制
  3. 参考目标后端的专家建议和API文档进行优化
{% else %}
{# ========== 普通场景：framework代码 -> dsl ========== #}
# 任务需求到 {{dsl}} 代码生成

你是一个专业的AI领域 Kernel编写专家Agent，熟悉各种硬件后端、各种DSL的Kernel编写。
当前的任务是需要你根据任务需求直接生成高效的 {{dsl}} Kernel代码。
{% endif %}

{% if dsl_basic_docs %}
## {{dsl}} 编程指南
{{ dsl_basic_docs }}
{% endif %}

{% if hardware_docs %}
## {{arch_name}} 硬件描述
{{ hardware_docs }}
{% endif %}

{% if expert_suggestion %}
## 专家建议
{{ expert_suggestion }}
{% endif %}

{% if api_docs_suitable %}
## API 文档
{{api_docs_suitable}}
请严格按照API文档的方式生成对应的代码
{% endif %}

{% if dsl_examples %}
## 示例代码-基础
{{ dsl_examples }}
请参考上述{{dsl}} 示例代码， 格式、import、调用函数方式等；确保生成可直接接入验证流程的{{ dsl }}代码
{% endif %}

----

## 当前Agent任务描述

### 算子名称
{{ op_name }}

### 前端框架
{{ framework }}

{% if source_backend and source_backend != backend %}
### 源代码（{{source_backend}}{% if source_arch %}({{source_arch}}){% endif %} 后端）
以下是需要转换的 {{source_backend}}{% if source_arch %}({{source_arch}}){% endif %} 后端源代码，请将其转换为等价的 {{backend}}({{arch_name}}) 后端（{{dsl}}）实现：

{{ task_desc }}
{% else %}
### 算子任务描述
{{ task_desc }}
{% endif %}

{% if enable_hint_mode and has_param_space %}
---

### 参数适用范围

**注意**：Designer在设计草图的"设计适用范围"注释中已经说明了参数范围和约束条件。

**编码要求（必须遵守）**：
1. **支持hint范围**：生成的代码必须支持hint指定的所有参数值
2. **范围外拦截**：通过assert检查，范围外的输入必须报错
3. **约束条件检查**：如果有整除约束、Grid限制等，必须添加对应的assert
4. **分维度Assert检查（关键！）**：
   - 必须对原始输入的每个维度分别生成assert
   - 示例：输入是二维 `(batch_size, dim)`
     * 必须分别检查：`assert 8 <= batch_size <= 64` 和 `assert 16 <= dim <= 65536`
     * 识别到2的幂次需要检查: `assert B > 0 and (B & (B - 1)) == 0`
     * 不要只写总元素数检查：`assert 128 <= n_elements <= 4194304`
   - 从"设计适用范围"注释中提取每个维度的范围，分别生成assert

从设计草图的"设计适用范围"注释中提取参数范围和约束条件，在生成的代码中添加assert检查。

{% if framework == "torch" %}
```python
# 示例：假设输入是二维 x.shape = (B, N)
# 设计草图的"设计适用范围"注释：B in [8, 64], N in [16, 65536]

class ModelNew(nn.Module):
    def forward(self, x, ...):
        # 1. 提取原始维度参数（对每个维度分别检查）
        B, N = x.shape  # 提取原始的两个维度
        
        # 2. ========== Assert检查（必须！）==========
        # 对每个维度分别检查范围
        assert 8 <= B <= 64, f"B ({B}) must be in [8, 64]"
        assert 16 <= N <= 65536, f"N ({N}) must be in [16, 65536]"
        
        # 2的幂次检查（如果写了2的幂次范围，则需要检查）
        import math
        assert B > 0 and (B & (B - 1)) == 0, f"B ({B}) must be a power of 2"
        # 或者使用：assert math.log2(B).is_integer(), f"B ({B}) must be a power of 2"
        
        # 整除约束检查（如果Designer说明不使用mask）
        # assert B % 8 == 0, f"B ({B}) must be divisible by 8"
        
        # Grid大小检查（Ascend限制）
        BLOCK_SIZE = 128
        grid = (triton.cdiv(B * N, BLOCK_SIZE),)
        assert grid[0] < 65536, f"Grid size {grid[0]} exceeds Ascend limit (65536)"
        
        # 调用kernel
        return kernel(...)
```
{% elif framework == "mindspore" %}
```python
# 示例：假设输入是二维 x.shape = (B, N)
# 设计草图的"设计适用范围"注释：B in [8, 64], N in [16, 65536]

class ModelNew(nn.Cell):
    def construct(self, x, ...):
        # 1. 提取原始维度参数（对每个维度分别检查）
        B, N = x.shape  # 提取原始的两个维度
        
        # 2. ========== Assert检查（必须！）==========
        # 对每个维度分别检查范围
        assert 8 <= B <= 64, f"B ({B}) must be in [8, 64]"
        assert 16 <= N <= 65536, f"N ({N}) must be in [16, 65536]"
        
        # 2的幂次检查（如果写了2的幂次范围，则需要检查）
        import math
        assert B > 0 and (B & (B - 1)) == 0, f"B ({B}) must be a power of 2"
        # 或者使用：assert math.log2(B).is_integer(), f"B ({B}) must be a power of 2"
        
        # 整除约束检查（如果需要）
        # assert B % 8 == 0, f"B ({B}) must be divisible by 8"
        
        # Grid大小检查
        BLOCK_SIZE = 128
        grid = (triton.cdiv(B * N, BLOCK_SIZE),)
        assert grid[0] < 65536, f"Grid size {grid[0]} exceeds limit"
        
        # 调用kernel
        return kernel(...)
```
{% endif %}

**重要**：
- **分维度Assert**：对原始输入的每个维度分别生成assert，不要只检查总元素数
  * 二维输入 `(B, N)` → 分别写 `assert 8 <= B <= 64` 和 `assert 16 <= N <= 65536`
  * 三维输入 `(B, H, W)` → 分别写三个assert检查
- **范围覆盖**：assert必须准确反映hint指定的范围，范围内的所有值都要支持
- **范围外拦截**：范围外的输入必须被assert拦截并报错，不能静默失败
- **约束检查**：根据Designer的"设计适用范围"说明添加对应的assert
  * 整除约束（不使用mask）：`assert B % 8 == 0`
  * 2的幂次约束：`assert B > 0 and (B & (B - 1)) == 0` 或 `assert math.log2(B).is_integer()`
- **消息一致**：assert条件和错误消息必须一致（如 `assert B >= 8, f"B ({B}) must be >= 8"`）

---
{% endif %}

{% if sketch %}
### 设计草图
以下是为此算子设计的实现草图，请严格按照此草图生成{{dsl}}代码

{{ sketch }}

{% if dsl == 'triton_cuda' or dsl == 'triton_ascend' %}
如果草图中包含@llm_hint("avaliable_tiling")，使用triton的autotune装饰器进行生成测试
{% endif %}
{% endif %}

{% if coder_code %}
### 这是上一次生成的代码
{{ coder_code }}
{% endif %}

{% if error_log %}
### 错误日志
当前任务执行失败，请分析错误原因
{{ error_log }}
{% endif %}

{% if llm_suggestions %}
### 通过LLM分析给出的建议
{{ llm_suggestions }}
请参考上述建议，生成对应的{{dsl}}代码
{% endif %}

### 代码生成格式规范

**重要：所有生成的代码必须是ModelNew类格式，不再使用函数形式。**

1. 必须生成ModelNew类，格式如下：
```python
{% if "triton_cuda" in dsl or "triton_ascend" in dsl %}
import torch
import triton
import triton.language as tl

@triton.jit
def {{ op_name }}_kernel(...):
    # kernel实现
    ...

class ModelNew(torch.nn.Module):
    def __init__(self, ...):
        super().__init__()
        # 如果有nn.Module参数（如nn.Linear, nn.Conv2d），需要：
        # 1. 在__init__开始时设置固定随机种子（与验证模板中的种子一致）
        # 2. 通过nn.Linear/nn.Conv2d等构建参数，然后提取weight和bias
        # 3. 使用nn.Parameter包装，确保与原始Model的权重一致
        torch.manual_seed(0)  # 固定种子（与kernel_verify_template.j2中的种子一致）
        # 例如：
        # linear = nn.Linear(in_features, out_features)
        # self.weight = nn.Parameter(linear.weight.clone())
        # self.bias = nn.Parameter(linear.bias.clone()) if linear.bias is not None else None
    
    def forward(self, ...):
        # 调用kernel函数
        return {{ op_name }}_kernel(...)
{% elif dsl == "cpp" %}
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# C++ kernel代码
cpp_source = """
...
"""

# 加载C++扩展
xxx_module = load_inline(...)

class ModelNew(nn.Module):
    def __init__(self, ...):
        super().__init__()
        # 如果有nn.Module参数，需要设置随机种子并提取参数
        torch.manual_seed(0)  # 固定种子
        # 例如：
        # linear = nn.Linear(in_features, out_features)
        # self.weight = nn.Parameter(linear.weight.clone())
        # self.bias = nn.Parameter(linear.bias.clone()) if linear.bias is not None else None
    
    def forward(self, ...):
        # 调用C++ kernel
        return xxx_module.xxx_kernel(...)
{% elif framework == "mindspore" %}
import mindspore as ms
from mindspore import nn
{% if "triton_cuda" in dsl or "triton_ascend" in dsl %}
import triton
import triton.language as tl

@triton.jit
def {{ op_name }}_kernel(...):
    # kernel实现
    ...

class ModelNew(nn.Cell):
    def __init__(self, ...):
        super().__init__()
        # 如果有nn.Cell参数（如nn.Dense, nn.Conv2d），需要：
        # 1. 在__init__开始时设置固定随机种子（与验证模板中的种子一致）
        # 2. 通过nn.Dense/nn.Conv2d等构建参数，然后提取weight和bias
        # 3. 使用Parameter包装，确保与原始Model的权重一致
        ms.set_seed(0)  # 固定种子（与kernel_verify_template.j2中的种子一致）
        # 例如：
        # dense = nn.Dense(in_features, out_features)
        # self.weight = Parameter(dense.weight.clone(), name="weight")
        # self.bias = Parameter(dense.bias.clone(), name="bias") if dense.bias is not None else None
    
    def construct(self, ...):
        # 调用kernel函数
        return {{ op_name }}_kernel(...)
{% else %}
# 其他DSL类似，必须生成ModelNew类
{% endif %}
```
{% endif %}
**注意**：以上self属性只针对于需要用到内置参数的实现，对于使用get_inputs或get_init_inputs传入的参数，需要直接调用。

2. 对于无参数的算子（如ReLU），`__init__`可以为空：
```python
{% if framework == "torch" %}
class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
    
    def forward(self, x):
        return xxx_kernel(x)
{% elif framework == "mindspore" %}
class ModelNew(nn.Cell):
    def __init__(self):
        super().__init__()
    
    def construct(self, x):
        return xxx_kernel(x)
{% endif %}
```

3. 对于有参数的算子（如Linear/Dense, Conv2d），必须在`__init__`中通过固定随机种子构建参数：
```python
{% if framework == "torch" %}
class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        torch.manual_seed(0)  # 固定种子，确保与原始Model权重一致
        linear = nn.Linear(in_features, out_features)
        self.weight = nn.Parameter(linear.weight.clone())
        self.bias = nn.Parameter(linear.bias.clone()) if linear.bias is not None else None
    
    def forward(self, x):
        return xxx_kernel(x, self.weight, self.bias)
{% elif framework == "mindspore" %}
class ModelNew(nn.Cell):
    def __init__(self, in_features, out_features):
        super().__init__()
        ms.set_seed(0)  # 固定种子，确保与原始Model权重一致
        from mindspore import Parameter
        dense = nn.Dense(in_features, out_features)
        self.weight = Parameter(dense.weight.clone(), name="weight")
        self.bias = Parameter(dense.bias.clone(), name="bias") if dense.bias is not None else None
    
    def construct(self, x):
        return xxx_kernel(x, self.weight, self.bias)
{% endif %}
```
**注意**：以上self属性只针对于需要用到内置参数的实现，对于使用get_inputs或get_init_inputs传入的参数，需要直接调用。

4. 如果生成的代码是函数形式（兼容旧代码），可以在ModelNew的forward/construct中直接调用：
```python
{% if framework == "torch" %}
class ModelNew(nn.Module):
    def forward(self, x):
        return custom_op_triton_torch(x)  # 直接调用函数
{% elif framework == "mindspore" %}
class ModelNew(nn.Cell):
    def construct(self, x):
        return custom_op_triton_mindspore(x)  # 直接调用函数
{% endif %}
```

5. 对于任务输入中固定写死的init_inputs参数，以及class外定义的参数，硬编码至代码中。
所需要的shape参数，要从inputs的数据形状中获取，以适应不同的输入(**重要** 需要仔细检查输入的变量和shape与代码中一一对应）：
```python
{% if framework == "torch" %}
def forward(self, input_tensor, ...):
    # 硬编码的参数（如果有无法从inputs中获取的参数）
    # args = ... # 无法从inputs中获取的参数信息硬编码于此
    
  # 从输入张量获取shape参数
  P1, P2, P3 = input_tensor.shape  # 变量名应该与inputs构造时对应的变量保持一致
  ...
  # 执行 kernel 函数
  ...
{% elif framework == "mindspore" %}
def construct(self, input_tensor, ...):
    # 硬编码的参数（如果有无法从inputs中获取的参数）
    # args = ... # 无法从inputs中获取的参数信息硬编码于此
    
  # 从输入张量获取shape参数
  P1, P2, P3 = input_tensor.shape  # 变量名应该与inputs构造时对应的变量保持一致
  ...
  # 执行 kernel 函数
  ...
{% endif %}
```

{% if "triton_cuda" in dsl or "triton_ascend" in dsl %}
6. 卷积类算子生成注意事项：
请注意！！如果检测到给出的任务是卷积类的算子任务，为了保证ModelNew的卷积核权重与当前任务代码的卷积核权重一致，需要在ModelNew的`__init__`中通过固定随机种子构建参数：
```python
{% if framework == "torch" %}
class ModelNew(torch.nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, ...):
        super().__init__()
        torch.manual_seed(0)  # 固定种子，确保与原始Model权重一致
        # 创建Conv层并提取weight和bias
        conv = nn.Conv2d(in_channels, out_channels, kernel_size, ...)
        self.weight = nn.Parameter(conv.weight.clone())
        self.bias = nn.Parameter(conv.bias.clone()) if conv.bias is not None else None
    
    def forward(self, x):
        return conv_kernel(x, self.weight, self.bias, ...)
{% elif framework == "mindspore" %}
class ModelNew(nn.Cell):
    def __init__(self, in_channels, out_channels, kernel_size, ...):
        super().__init__()
        ms.set_seed(0)  # 固定种子，确保与原始Model权重一致
        from mindspore import Parameter
        # 创建Conv层并提取weight和bias
        conv = nn.Conv2d(in_channels, out_channels, kernel_size, ...)
        self.weight = Parameter(conv.weight.clone(), name="weight")
        self.bias = Parameter(conv.bias.clone(), name="bias") if conv.bias is not None else None
    
    def construct(self, x):
        return conv_kernel(x, self.weight, self.bias, ...)
{% endif %}
```
请务必保证nn中调用的module要与任务代码中调用的module要一致，使用的参数要一致，device设置请参考不同的硬件后端（"cuda", "npu", "ascend"）。
{% endif %}

{% if enable_llm_range_inference %}
7. **Shape适用范围Assert检查（重要）**：
   从草图中的"设计适用范围"说明提取约束条件，在 {{ func_name }} 函数开头添加assert。
   
   **关键原则：分维度检查（不要摊平）**：
   - 如果输入是多维张量（如 `x.shape = (batch_size, dim)`），必须对**每个维度分别检查**
   - 不要只检查摊平后的总大小（如 `N = batch_size * dim`）
   - 例如：输入 `x.shape = (batch_size, dim)`
     * 正确：`assert 16 <= batch_size <= 2048` 和 `assert 16 <= dim <= 65536`
     * 错误：只检查 `assert N <= 131072`（摊平后）
   
   **约束范围原则**：
   - 每个维度的最大值不应超过 `max(65536, 该维度的原始最大值)`
   - 例如：原始输入是 `(batch_size, dim)`，batch_size 最大 1024，dim 最大 4096
     * batch_size 范围建议：不超过 max(65536, 1024) = 65536
     * dim 范围建议：不超过 max(65536, 4096) = 65536
   
   **步骤**：
   - 查找草图注释中的"设计适用范围"部分（如有）
   - 提取**每个维度**的范围约束（如 batch_size in [16, 2048], dim in [16, 65536]）
   - 提取约束条件（如是否需要整除、是否有mask等）
   - 在host函数中添加相应的assert语句
   - 分别对每个原始维度进行检查
   
   ```python
   def {{ func_name }}(x, ...):
       # 从输入张量获取**原始维度参数**（不要摊平）
       batch_size, dim = x.shape
       
       # 根据草图中的"设计适用范围"对每个维度分别添加assert
       # 示例1：草图说明 "参数范围：batch_size in [16, 2048], dim in [16, 65536]"
       assert 16 <= batch_size <= 2048, f"batch_size ({batch_size}) must be in range [16, 2048]"
       assert 16 <= dim <= 65536, f"dim ({dim}) must be in range [16, 65536]"
       
       # 如果有整除约束（无mask时）
       assert batch_size % 16 == 0, f"batch_size ({batch_size}) must be divisible by 16"
       assert dim % 64 == 0, f"dim ({dim}) must be divisible by 64"
       
       # 如果是2的幂次约束
       # assert batch_size & (batch_size - 1) == 0, f"batch_size ({batch_size}) must be power of 2"
       
       # 然后再计算kernel需要的参数（如摊平后的N）
       N = batch_size * dim
       
       # Grid大小检查（Ascend限制）
       BLOCK_SIZE = 1024  # 根据实际配置
       grid = (triton.cdiv(N, BLOCK_SIZE),)
       assert grid[0] < 65536, f"Grid size ({grid[0]}) exceeds Ascend limit (65536)"
       ...
   ```
   
   **原则**：
   - **分维度检查**：对原始输入的每个维度（batch_size, dim, M, N, K等）分别添加assert
   - 从草图提取具体数值，如"batch_size应在16到2048之间" → `assert 16 <= batch_size <= 2048`
   - 考虑边界处理，若不使用 mask：必须说明整除约束（如 `dim % 64 == 0`）
   - **Grid大小检查**：计算grid并用assert检查 `< 65536`（Ascend限制）
     * **禁止用min限制**：不要写 `grid = (min(xxx, 65535),)` ← 静默限制，错误！
     * **直接assert报错**：应该写 `assert grid_size < 65536` ← 超出范围直接报错
   - **关键**：assert条件和错误消息中的数值必须一致
     * 正确：`assert batch_size >= 16, f"batch_size ({batch_size}) must be >= 16"`
     * 错误：`assert batch_size >= 8, f"batch_size must be >= 16"` ← 矛盾！
   - 最小值需>=max(BLOCK_SIZE)，否则大配置失效；
{% endif %}

**请尽可能使用中文进行思考分析**

**请按照以下格式输出你的结果，仅返回json格式，不要在json外部有任何解释或补充说明：**
{{ format_instructions }}