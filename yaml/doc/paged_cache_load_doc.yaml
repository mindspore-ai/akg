paged_cache_load:
    description: |
        load and concat key, value from kv_cache using block_tables and context_lens.
        Support dtype: fp16, bf16, int8
        Support format: ND, NZ

        Note:
            - The two inputs can not be bool type at the same time,
              [True, Tensor(True), Tensor(np.array([True]))] are all considered bool type.
            - Support broadcast, support implicit type conversion and type promotion.
            - When the input is a tensor, the dimension should be greater than or equal to 1.

        Args:
            key_cache (Tensor): origin key cache tensor. [num_blocks, block_size, num_heads, head_size_k]
            value_cache (Tensor): origin value cache tensor. [num_blocks, block_size, num_heads, head_size_v]
            block_tables (Tensor): block_tables [batch, block_indices]
            seq_lens (Tensor): recording context length of each batch in two form:
             - length of each batch.  e.g. [1, 10, 5, 20]  shape is [batch]
             - accumulated sum of the length of each batch.  e.g. [0, 1, 11, 16, 36] shape is [batch+1]
            seq_starts (Tensor): Optional input, recording where sequence starts. [batch]
            kv_cache_cfg (int): default 0, 0->nd, 1->nz
            is_seq_lens_cumsum_type (bool): default false, when using seq_starts in ND, set it to True. Otherwise, false. 
            has_seq_starts (bool): default false, when using seq_starts in ND, set it to True. Otherwise, false.

        Returns:
            key_out (Tensor): the key after concat [num_tokens, num_heads, head_size_k]
            value_out (Tensor): the value after concat [num_tokens, num_heads, head_size_v]

        Supported Platforms:
            ``Ascend910B``

        Examples:
            import os
            import numpy as np
            from mindspore import Tensor, context
            import mindspore as ms
            import random
            import ms_custom_ops

            class AsdPagedCacheLoadCustom(ms.nn.Cell):
            def __init__(self):
                super().__init__()
        
            def construct(self, key_cache, value_cache, block_table, seq_lens, seq_starts, kv_cache_cfg,
                          is_seq_lens_cumsum_type, has_seq_starts):
                return ms_custom_ops.paged_cache_load(key_cache, value_cache, block_table, seq_lens, seq_starts, kv_cache_cfg,
                                              is_seq_lens_cumsum_type, has_seq_starts)

            ------------------------------------ ND INPUT WITH SEQ_STARTS -------------------------------------------------------------                                              
            # dtype is in [ms.float16, ms.bfloat16, ms.int8]
            if dtype == ms.float16:
                key_cache = np.random.randint(1, 11, 
                                              size=(num_blocks, block_size, num_heads, head_size_k)).astype(np.float16)
                value_cache = np.random.randint(1, 11, 
                                                size=(num_blocks, block_size, num_heads, head_size_v)).astype(np.float16)
            elif dtype == ms.bfloat16:
                key_cache = np.random.randint(1, 11, 
                                              size=(num_blocks, block_size, num_heads, head_size_k)).astype(np.float32)
                value_cache = np.random.randint(1, 11, 
                                                size=(num_blocks, block_size, num_heads, head_size_v)).astype(np.float32)
            else:
                key_cache = np.random.randint(1, 11, 
                                              size=(num_blocks, block_size, num_heads, head_size_k)).astype(np.int8)
                value_cache = np.random.randint(1, 11, 
                                                size=(num_blocks, block_size, num_heads, head_size_v)).astype(np.int8)
            context_lens = [random.randint(1, 1024) for _ in range(num_tokens)]
            max_context_len = max(context_lens)
            max_num_blocks_per_req = (max_context_len + block_size -1) // block_size + 4
            block_tables = []
            for _ in range(num_tokens):
                block_table = [
                    random.randint(0, num_blocks - 1) for _ in range(max_num_blocks_per_req)
                ]
                block_tables.append(block_table)
            cu_context_lens = [0]
            for elem in context_lens:
                cu_context_lens.append(cu_context_lens[-1] + elem)
            seq_starts = [random.randint(0, 4) * block_size for _ in range(num_tokens)]
            context_lens = np.array(cu_context_lens).astype(np.int32)
            block_tables = np.array(block_tables).astype(np.int32)
            seq_starts = np.array(seq_starts).astype(np.int32)
            sum_context_lens = context_lens[-1]

            seq_starts_tensor = None if seq_starts is None else Tensor(seq_starts)
            net = AsdPagedCacheLoadCustom()
            key_out, value_out = net(
                Tensor(key_cache).astype(dtype),
                Tensor(value_cache).astype(dtype),
                Tensor(block_tables),
                Tensor(context_lens),
                seq_starts_tensor,
                format_type, cu_seq_lens, has_seq_starts
            )

            print("key_out is ", key_out)
            print("value_out is ", value_out)


            ------------------------------------ NZ INPUT WITHOUT SEQ_STARTS -------------------------------------------------------------
            # dtype is in [ms.float16, ms.bfloat16, ms.int8]
            if dtype == ms.float16:
                key_cache = np.random.randint(
                    1, 11, size=(num_blocks, num_heads * head_size_k // 16, block_size, 16)).astype(np.float16)
                value_cache = np.random.randint(
                    1, 11, size=(num_blocks, num_heads * head_size_k // 16, block_size, 16)).astype(np.float16)
            elif dtype == ms.bfloat16:
                key_cache = np.random.randint(
                    1, 11, size=(num_blocks, num_heads * head_size_k // 16, block_size, 16)).astype(np.float32)
                value_cache = np.random.randint(
                    1, 11, size=(num_blocks, num_heads * head_size_k // 16, block_size, 16)).astype(np.float32)
            else:
                key_cache = np.random.randint(
                    1, 11, size=(num_blocks, num_heads * head_size_k // 32, block_size, 32)).astype(np.int8)
                value_cache = np.random.randint(
                    1, 11, size=(num_blocks, num_heads * head_size_k // 32, block_size, 32)).astype(np.int8)
            context_lens = [random.randint(1, 1024) for _ in range(num_tokens)]
            max_context_len = max(context_lens)
            max_num_blocks_per_req = (max_context_len + block_size -1) // block_size
            block_tables = []
            for _ in range(num_tokens):
                block_table = [
                    random.randint(0, num_blocks - 1) for _ in range(max_num_blocks_per_req)
                ]
                block_tables.append(block_table)
        
            context_lens = np.array(context_lens).astype(np.int32)
            block_tables = np.array(block_tables).astype(np.int32)
            sum_context_lens = sum(context_lens)
            seq_starts_tensor = None if seq_starts is None else Tensor(seq_starts)
            net = AsdPagedCacheLoadCustom()
            key_out, value_out = net(
                Tensor(key_cache).astype(dtype),
                Tensor(value_cache).astype(dtype),
                Tensor(block_tables),
                Tensor(context_lens),
                seq_starts_tensor,
                format_type, cu_seq_lens, has_seq_starts
            )

            print("key_out is ", key_out)
            print("value_out is ", value_out)