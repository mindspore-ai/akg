mla:
    description: |
        全名Multi-Head Latent Attention，DeepSeek模型中优化技术，使用低秩压缩方法减少kvcache的显存占用。

        Note:
            - The two inputs can not be bool type at the same time,
              [True, Tensor(True), Tensor(np.array([True]))] are all considered bool type.
            - Support broadcast, support implicit type conversion and type promotion.
            - When the input is a tensor, the dimension should be greater than or equal to 1.

        Args:
            q_nope (Tensor): 查询向量中不参与位置编码计算的部分，shape为[num_tokens, num_heads, 512]。数据类型支持：float16/bf16/int8。
            q_rope (Tensor): 查询向量中参与位置编码计算的部分，shape为[num_tokens, num_heads, 64]。数据类型支持：float16/bf16。
            ctkv (Tensor): key/value缓存，不包含位置编码计算。数据类型为int8时，数据排布必须是NZ，shape为[]
            other (Union[Tensor, number.Number, bool]): The second input tensor.

        Returns:
            Tensor

        Supported Platforms:
            ``Ascend``

        Examples:
            >>> import mindspore as ms
            >>> import ms_custom_ops
            >>> # case 1: x and y are both tensor.
            >>> x = ms.tensor([1., 2., 3.])
            >>> y = ms.tensor([4., 5., 6.])
            >>> output = ms_custom_ops.add(x, y)
            >>> print(output)
            [5. 7. 9.]
