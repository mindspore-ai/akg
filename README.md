# ms_custom_ops - MindSpore è‡ªå®šä¹‰ç®—å­æ¡†æ¶

[![License](https://img.shields.io/badge/License-Apache%202.0lue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![MindSpore](https://img.shields.io/badge/MindSpore-2.6-green.svg)](https://www.mindspore.cn/)

## ğŸ“– æ¦‚è¿°

`ms_custom_ops` æ˜¯ä¸€ä¸ªä¸“ä¸º MindSpore è®¾è®¡çš„è‡ªå®šä¹‰ç®—å­å¼€å‘æ¡†æ¶ï¼Œæ”¯æŒåœ¨æ˜‡è…¾ NPU ä¸Šé«˜æ•ˆå®ç°è‡ªå®šä¹‰ç®—å­ã€‚è¯¥æ¡†æ¶æä¾›äº†å®Œæ•´çš„ PyNative å’Œ Graph ä¸¤ç§æ‰§è¡Œæ¨¡å¼æ”¯æŒï¼Œå†…ç½®ç¼“å­˜ä¼˜åŒ–ã€å†…å­˜ç®¡ç†ç­‰é«˜çº§ç‰¹æ€§ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿå¿«é€Ÿæ„å»ºé«˜æ€§èƒ½çš„è‡ªå®šä¹‰ç®—å­ã€‚

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- **åŒæ¨¡å¼æ”¯æŒ**: åŒæ—¶æ”¯æŒ PyNative åŠ¨æ€æ‰§è¡Œå’Œ Graph é™æ€ç¼–è¯‘æ¨¡å¼
- **æ˜‡è…¾ä¼˜åŒ–**: ä¸“ä¸ºæ˜‡è…¾ NPU è®¾è®¡ï¼Œå……åˆ†åˆ©ç”¨ç¡¬ä»¶ç‰¹æ€§
- **ç¼“å­˜æœºåˆ¶**: å†…ç½®ç®—å­ç¼“å­˜å’Œ Tiling ç¼“å­˜ï¼Œæ˜¾è‘—æå‡æ€§èƒ½
- **å†…å­˜ç®¡ç†**: è‡ªåŠ¨ç®¡ç†è®¾å¤‡å†…å­˜å’Œä¸»æœºå†…å­˜ï¼Œç¡®ä¿å†…å­˜å®‰å…¨
- **å¼€å‘å‹å¥½**: æä¾›å®Œæ•´çš„å¼€å‘å·¥å…·é“¾å’Œæµ‹è¯•æ¡†æ¶

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

### æ ¸å¿ƒç»„ä»¶

1. **PyBoost æ¡†æ¶**ï¼šç”¨äº PyNative æ¨¡å¼ä¸‹çš„åŠ¨æ€æ‰§è¡Œ
2. **GraphMode æ¡†æ¶**ï¼šç”¨äºé™æ€å›¾ç¼–è¯‘æ¨¡å¼
3. **å…±äº«ç»„ä»¶**ï¼šåŒ…æ‹¬å†…å­˜ç®¡ç†ã€ç¼“å­˜ä¼˜åŒ–ç­‰é€šç”¨åŠŸèƒ½

### æ ¸å¿ƒæ¨¡å—è¯´æ˜

#### 1. ms_kernels_internal - å†…éƒ¨ç®—å­æ¡†æ¶
- **ccsrc/base/ms_kernels_internal/pyboost**: PyNativeæ¨¡å¼ä¸‹çš„ç®—å­å…¬å…±åŸºç±»å®ç°
- **ccsrc/base/ms_kernels_internal/graphmode**: Graphæ¨¡å¼ä¸‹çš„ç®—å­å…¬å…±åŸºç±»å®ç°
- **ccsrc/ops/ms_kernels_internal/*.cc**: ç®—å­è°ƒç”¨å®ç°

- **å…¬å…±æ–‡ä»¶**:
  - **tiling_mem_mgr.h/cc**: Tilingå†…å­˜ç®¡ç†å™¨ï¼Œè´Ÿè´£è®¾å¤‡å†…å­˜åˆ†é…å’Œé‡Šæ”¾
  - **internal_tiling_cache.h/cc**: å†…éƒ¨Tilingç¼“å­˜ï¼Œæä¾›ç®—å­ç¼“å­˜å’ŒTilingç­–ç•¥ç¼“å­˜
  - **internal_helper.h/cc**: å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼Œæä¾›é€šç”¨å·¥å…·å‡½æ•°
  - **internal_spinlock.h**: è‡ªæ—‹é”å®ç°ï¼Œç”¨äºå¤šçº¿ç¨‹åŒæ­¥

#### 2. ascendc - æ˜‡è…¾Cç®—å­æ¡†æ¶
- **ccsrc/base/ascendc/pyboost**: PyNativeæ¨¡å¼ä¸‹çš„ç®—å­å…¬å…±åŸºç±»å®ç°
- **ccsrc/base/ascendc/graphmode**: Graphæ¨¡å¼ä¸‹çš„ç®—å­å…¬å…±åŸºç±»å®ç°
- **ccsrc/ops/ascendc/**: ç®—å­kernelå’Œè°ƒç”¨å®ç°

### ç›®å½•ç»“æ„

```
ms_custom_ops/
â”œâ”€â”€ ccsrc/                        # C++æ ¸å¿ƒæºç 
â”‚   â”œâ”€â”€ base/                     # åŸºç¡€è®¾æ–½
â”‚   â”‚   â”œâ”€â”€ ms_kernels_internal/  # å†…éƒ¨ç®—å­åŸºç¡€æ¡†æ¶
â”‚   â”‚   â”‚   â”œâ”€â”€ pyboost/          # PyNativeæ¨¡å¼åŸºç±»/å·¥å…·
â”‚   â”‚   â”‚   â”œâ”€â”€ graphmode/        # Graphæ¨¡å¼åŸºç±»/å·¥å…·
â”‚   â”‚   â”‚   â”œâ”€â”€ tiling_mem_mgr.h/cc      # Tilingå†…å­˜ç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ internal_helper.h/cc      # å†…éƒ¨è¾…åŠ©å‡½æ•°
â”‚   â”‚   â”‚   â”œâ”€â”€ internal_spinlock.h       # è‡ªæ—‹é”å®ç°
â”‚   â”‚   â”‚   â””â”€â”€ internal_tiling_cache.h/cc # å†…éƒ¨Tilingç¼“å­˜
â”‚   â”‚   â””â”€â”€ ascendc/              # æ˜‡è…¾ç®—å­åŸºç¡€
â”‚   â”‚       â”œâ”€â”€ pyboost/
â”‚   â”‚       â””â”€â”€ graphmode/
â”‚   â”œâ”€â”€ ops/                      # ç®—å­å®ç°
â”‚   â”‚   â”œâ”€â”€ ms_kernels_internal/
â”‚   â”‚   â”‚   â””â”€â”€ {op_name}.cc
â”‚   â”‚   â”œâ”€â”€ ascendc/
â”‚   â”‚   â”‚   â”œâ”€â”€ {op_name}/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ {op_name}.cc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ op_host/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ op_kernel/
â”‚   â”‚   â”‚   â””â”€â”€ CMakeLists.txt
â”‚   â”‚   â””â”€â”€ CMakeLists.txt
â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”œâ”€â”€ module.h
â”‚   â””â”€â”€ module.cc
â”œâ”€â”€ cmake/                        # CMakeé…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ compile_ascendc_ops.cmake
â”‚   â””â”€â”€ find_ms_internal_kernels_lib.cmake
â”œâ”€â”€ python/
â”‚   â””â”€â”€ ms_custom_ops/
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ yaml/                         # ç®—å­é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ ascendc/
â”‚   |   â””â”€â”€ {op_name}_op.yaml
â”‚   â”œâ”€â”€ doc/
â”‚   |   â”œâ”€â”€ {op_name}.md          # Markdownæºæ–‡ä»¶
â”‚   |   â””â”€â”€ {op_name}_doc.yaml    # ç”Ÿæˆçš„æ–‡æ¡£YAMLæ–‡ä»¶
â”‚   â””â”€â”€ ms_kernels_internal/
â”‚   |   â””â”€â”€ {op_name}_op.yaml
â”œâ”€â”€ tests/                        # æµ‹è¯•æ–‡ä»¶
â”‚   â””â”€â”€ st/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ op_compiler.py
â”œâ”€â”€ build.sh                      # ä¸€é”®ç¼–è¯‘è„šæœ¬
â”œâ”€â”€ setup.py                      # Pythonå®‰è£…é…ç½®
â”œâ”€â”€ requirements.txt               # Pythonä¾èµ–
â”œâ”€â”€ version.txt                   # ç‰ˆæœ¬ä¿¡æ¯
â””â”€â”€ README.md
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²å®‰è£…ï¼š
- **MindSpore**: br_infer_iteråˆ†æ”¯æ—¥æ„å»ºåŒ…
- **æ˜‡è…¾ CANN å·¥å…·åŒ…**: æœ€æ–°ç‰ˆæœ¬
- **CMake**: >= 3.16
- **Python**: >= 3.9
- **Git**: ç”¨äºè·å–æäº¤ä¿¡æ¯

### 2. ç¯å¢ƒé…ç½®

```bash
# è®¾ç½®æ˜‡è…¾ç¯å¢ƒå˜é‡
source ${CANN_INSTALL_PATH}/ascend-toolkit/set_env.sh
```

### 3. ç¼–è¯‘å®‰è£…

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ build.sh è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# æŸ¥çœ‹ç¼–è¯‘é€‰é¡¹
bash build.sh -h

# é»˜è®¤ç¼–è¯‘ï¼ˆReleaseæ¨¡å¼ï¼‰
bash build.sh

# Debugç¼–è¯‘
bash build.sh -d

# ç¼–è¯‘æŒ‡å®šç®—å­
bash build.sh -p ${absolute_op_dir_path}

# ç¼–è¯‘æŒ‡å®šç®—å­
bash build.sh -p ${absolute_op_dir_path}
eg. bash build.sh -p /home/ms_custom_ops/ccsrc/ops/ascendc/add,/home/ms_custom_ops/ccsrc/ops/ascendc/add_rms_norm

# æŒ‡å®šSOC Verisonç¼–è¯‘
eg. bash build.sh -v ascend910b4
```

#### æ–¹æ³•äºŒï¼šä½¿ç”¨ setup.py å®‰è£…

```bash
# å®‰è£…ï¼ˆä¼šè‡ªåŠ¨ç¼–è¯‘è‡ªå®šä¹‰ç®—å­ï¼‰
python setup.py install

# æˆ–è€…æ„å»ºwheelåŒ…
python setup.py bdist_wheel
```

ç¼–è¯‘è¿‡ç¨‹ä¼šè‡ªåŠ¨ï¼š
- æ£€æµ‹æ˜‡è…¾ç¯å¢ƒ
- ä½¿ç”¨ CMake æ„å»ºè‡ªå®šä¹‰ç®—å­
- å°†ç”Ÿæˆçš„ .so æ–‡ä»¶å®‰è£…åˆ°æ­£ç¡®ä½ç½®

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### PyNative æ¨¡å¼

```python
import mindspore as ms
import ms_custom_ops

# ç›´æ¥è°ƒç”¨è‡ªå®šä¹‰ç®—å­
output = ms_custom_ops.reshape_and_cache(
    key, value, key_cache, value_cache, slot_mapping, head_num
)
```

### Graph æ¨¡å¼

```python
import mindspore as ms
import ms_custom_ops

reshape_and_cache = ms.jit(func=ms_custom_ops.reshape_and_cache)
output = reshape_and_cache(key, value, key_cache, value_cache, slot_mapping, head_num)
```

## å¼€å‘è‡ªå®šä¹‰ç®—å­

### å¼€å‘æµç¨‹æ¦‚è§ˆ

å¼€å‘ä¸€ä¸ªæ–°çš„è‡ªå®šä¹‰ç®—å­éœ€è¦ä»¥ä¸‹æ­¥éª¤ï¼š

1. **è®¾è®¡ç®—å­æ¥å£** - ç¡®å®šè¾“å…¥è¾“å‡ºå’Œå‚æ•°ï¼Œç¼–å†™ç®—å­å‰ç«¯æ¥å£å®šä¹‰
2. **å®ç°ç®—å­é€»è¾‘** - ç¼–å†™PyBoostå’ŒGraphModeå®ç°
3. **ç¼–å†™æµ‹è¯•ç”¨ä¾‹** - åˆ›å»ºå•å…ƒæµ‹è¯•
4. **ç¼–è¯‘å’ŒéªŒè¯** - æ„å»ºå¹¶æµ‹è¯•ç®—å­

#### 1. è®¾è®¡ç®—å­æ¥å£

éœ€è¦æ˜ç¡®ç®—å­è¾“å…¥è¾“å‡ºç±»å‹ï¼Œå¹¶æ ¹æ®ç¡®å®šçš„ç®—å­æ¥å£ï¼Œç¼–å†™ç®—å­å‰ç«¯å®šä¹‰çš„YAMLæ–‡ä»¶ï¼Œå¹¶å®ç°{op_name}_op.yamlå’Œ{op_name}_doc.yamlã€‚
{op_name}_doc.yamlæ˜¯ç®—å­å‚æ•°è¯´æ˜æ–‡ä»¶ï¼Œ{op_name}_op.yamlæ˜¯ç®—å­æ¥å£å®šä¹‰æ–‡ä»¶ï¼Œå…¶å†…å®¹æ ¼å¼ä»‹ç»ï¼š
```yaml
# Defining the function name and Primitive name of operators, use the '_' to separate words. For example, op_name is 'word1_word2', then the function name is 'word1_word2', and the Primitive class name is 'Word1Word2'.
<op_name>:
  # The 'args' is a fixed key of yaml file to define input args of operators.
  <args>:
    # Mandatory. For every arg, key is operators' argument name, and the value are some items, items' key name can be 'dtype', 'prim_init', 'default', 'type_cast','arg_handler'.
    <arg1>:
      # Mandatory. The 'dtype' is a fixed key.
      # Value is one of {int, float, bool, number, tensor, tuple, list, tuple[int], tuple[float], tuple[bool], tuple[number], tuple[tensor], list[int], list[float], list[bool], list[number], list[tensor]}.
      # If value is 'number', arg can be 'int', 'float' or 'bool'.
      <dtype>: <value>

      # Optional. The 'default' is a fixed key.
      # This item means input arg can use default value.
      # If arg_handler is not empty, the type of default value should be the first one of 'arg_handler_map'
      <default>: <value>

      # Optional. The 'prim_init' is a fixed key. Value can be 'True' or 'False', arg is arg of '__init__' of Primitive if value is 'True'.
      <prim_init>: <value>

      # Optional. The 'type_cast' is a fixed key. This item means can accept unmatchable input by implicit conversion. Value is one of {int, float, bool, number, tensor, tuple, list, tuple[int], tuple[float], tuple[bool], tuple[number], tuple[tensor], list[int], list[float], list[bool], list[number], list[tensor]}
      # Supported type cast now:
      # 1. int, float, bool, number <-> tensor.
      # 2. int, float, bool, number, tensor <-> list/tuple.
      # 3. list <-> tuple.
      <type_cast>: <value>

      # Optional. The 'arg_handler' is a fixed key. Value is a function name used to convert arg. For example, converting kernel size from 2 to (2, 2).
      <arg_handler>: <value>

    <arg2>:
      ...

    <args_signature>: #Optional
      # Optional. The 'rw_write' is a fixed key, 'arg_name' is the corresponding arg name.
      <rw_write>: <arg_name>

      # Optional. The 'rw_read' is a fixed key, 'arg_name' is the corresponding arg name.
      <rw_read>: <arg_name>

      # Optional. The 'rw_ref' is a fixed key, 'arg_name' is the corresponding arg name.
      <rw_ref>: <arg_name>

      # Optional. arg1 and arg2 should has same dtype. arg3 and arg4 should has same dtype.
      <dtype_group>: (<arg_name1>, <arg_name2>, ...), (<arg_name3>, <arg_name4>, ...), ...

    # The 'returns' is a fixed key of yaml file to define output of operators.
    <returns>:
      # Mandatory. For every output, key is operators' output name, and the value is a item, item's key is 'dtype'.
      <output1>:
        # Mandatory. Just refer to key 'dtype' in args.
        <dtype>: <value>

        # Optional. The 'inplace' is a fixed key. Value is input name of operator if the input is a inplace input.
        <inplace>: <value>

      <output2>:
        ...

    # Optional. The 'view' is a fixed key. Value should be set as 'True' if this is a view operator.
    # Default: False.
    <view>: <value>
```
å…·ä½“å¯å‚è€ƒ`yaml/ms_kernels_internal/reshape_and_cache_op.yaml`å’Œ`yaml/doc/reshape_and_cache_doc.yaml`

#### 2. å®ç°ç®—å­é€»è¾‘

#### ascendcç®—å­

åˆ›å»ºè‡ªå®šä¹‰ç®—å­ç›®å½•ï¼š`ccsrc/ops/ascendc/{op_name}`ï¼Œå…¶ä¸­`ccsrc/ops/ascendc/{op_name}/op_kernel`å’Œ`ccsrc/ops/ascendc/{op_name}/op_host`æ˜¯ascendcç®—å­kernelå®ç°ï¼Œç®—å­ä¼šè¢«ç¼–è¯‘æˆaclnnçš„ä¸¤æ®µå¼æ¥å£ã€‚  
åœ¨`ccsrc/ops/ascendc/{op_name}/{op_name}.cc`æ–‡ä»¶å®ç°ç®—å­kernelçš„è°ƒç”¨ã€‚ç®—å­çš„pyboostå’Œgraphè°ƒç”¨å®ç°å‡åœ¨æ­¤æ–‡ä»¶ã€‚
è¦å®ç°çš„ä¸»è¦ç±»å’Œå‡½æ•°ï¼š  
graph:  
1ï¼‰ç®—å­inferå‡½æ•°ï¼Œç”¨æ¥æ¨å¯¼ç®—å­è¾“å‡ºshapeå’Œdtypeï¼›  
2ï¼‰ç®—å­KernelModï¼Œéœ€è¦ç»§æ‰¿`AscendCKernelMod`å¹¶é‡å†™`Launch`å’Œ`GetWorkSpaceInfo`æ¥å£ï¼›  

pyboost:  
1ï¼‰ç®—å­kernelè°ƒç”¨å‡½æ•°;   
2ï¼‰pybindæ¥å£æ³¨å†Œï¼›  

ä»¥addç®—å­ä¸ºä¾‹ï¼š
```cpp
#include "ascendc_kernel_mod.h"
#include "ms_extension/api.h"
#include <map>
#include <string>
#include <vector>

// =============================================================================
// å›¾æ¨¡å¼è°ƒç”¨å®ç°
// =============================================================================

namespace ms_custom_ops {
// ç®—å­inferå‡½æ•°ï¼Œéœ€è¦å®ç°InferShapeå’ŒInferTypeå‡½æ•°
class OPS_API AddCustomOpFuncImpl : public OpFuncImpl {
public:
  // ç®—å­infershapeï¼Œéœ€è¦è¿”å›ç®—å­æ‰€æœ‰è¾“å‡ºçš„shapeå¤§å°
  ShapeArray InferShape(const PrimitivePtr &primitive,
                        const InferInfoPtrList &input_infos) const override {
    auto out_shape = input_infos[0]->GetShape();
    return {out_shape};
  }

  // ç®—å­infertypeï¼Œéœ€è¦è¿”å›ç®—å­æ‰€æœ‰è¾“å‡ºçš„æ•°æ®ç±»å‹
  std::vector<TypeId> InferType(const PrimitivePtr &primitive,
                                const InferInfoPtrList &input_infos) const override {
    return {input_infos[0]->GetType()};
  }

  bool GeneralInferRegistered() const override { return true; }
};

// ç®—å­graphæ¨¡å¼è°ƒç”¨ï¼Œaclnnä¸¤æ®µå¼æ¥å£è°ƒç”¨ï¼Œéœ€è¦å®ç°Launchå’ŒGetWorkSpaceInfoå‡½æ•°
class AddCustomAscend : public AscendCKernelMod {
public:
  AddCustomAscend() : AscendCKernelMod(std::move("aclnnAddCustom")) {}
  ~AddCustomAscend() = default;

  // ç®—å­æ‰§è¡Œè°ƒç”¨å‡½æ•°ï¼ŒRunOpå‡½æ•°ä¼šè°ƒç”¨aclnnç®—å­ç¬¬äºŒæ®µæ¥å£
  bool Launch(const std::vector<KernelTensor *> &inputs,
              const std::vector<KernelTensor *> &workspace,
              const std::vector<KernelTensor *> &outputs,
              void *stream_ptr) override {
    MS_EXCEPTION_IF_NULL(stream_ptr);
    RunOp(stream_ptr, workspace, inputs[0], inputs[1], outputs[0]);
    return true;
  }

  // ç®—å­workspaceè°ƒç”¨å‡½æ•°ï¼ŒGetWorkspaceForResizeä¼šè°ƒç”¨aclnnç®—å­ç¬¬ä¸€æ®µæ¥å£
  void GetWorkSpaceInfo(const std::vector<KernelTensor *> &inputs,
                        const std::vector<KernelTensor *> &outputs) override {
    GetWorkspaceForResize(inputs[0], inputs[1], outputs[0]);
  }

private:
  DEFINE_GET_WORKSPACE_FOR_RESIZE();
};
} // namespace ms_custom_ops

// æ³¨å†Œç®—å­inferå‡½æ•°
REG_GRAPH_MODE_OP(add, ms_custom_ops::AddCustomOpFuncImpl,
                  ms_custom_ops::AddCustomAscend);

// =============================================================================
// PYBOOSTè°ƒç”¨å®ç°
// =============================================================================

#include "ascendc_pyboost_runner.h"

namespace ms_custom_ops {
using namespace mindspore;
using namespace mindspore::device::ascend;
// ç®—å­kernelè°ƒç”¨å‡½æ•°ï¼Œéœ€è¦æ‰‹åŠ¨åˆ›å»ºè¾“å‡ºtensor
ms::Tensor custom_add(const ms::Tensor &x, const ms::Tensor &y) {
  // åˆ›å»ºè¾“å‡ºç©ºtensor
  auto out = ms::Tensor(x.data_type(), x.shape());
  // åˆå§‹åŒ–runnerè¿è¡Œå™¨
  auto runner = std::make_shared<ms::pynative::AscendCOpRunner>("AddCustom");
  // è®¾ç½®runneréœ€è¦å…·ä½“æ‰§è¡Œçš„å‡½æ•°ï¼Œç”±LAUNCH_ASCENDC_FUNCå°è£…äº†aclnnæ¥å£è°ƒç”¨
  runner->SetLaunchFunc(LAUNCH_ASCENDC_FUNC(aclnnAddCustom, x, y, out));
  // æ‰§è¡Œrunner
  runner->Run({x, y}, {out});
  return out;
}

// pybindè°ƒç”¨å‡½æ•°
auto pyboost_add(const ms::Tensor &x, const ms::Tensor &y) {
  // Call<è¾“å‡ºä¸ªæ•°>
  return ms::pynative::PyboostRunner::Call<1>(custom_add, x, y);
}
} // namespace ms_custom_ops

// ç®—å­æ¥å£æ³¨å†Œï¼Œå¯¹æ¥C++å’Œpythonæ¥å£
MS_CUSTOM_OPS_EXTENSION_MODULE(m) {
  m.def("add", &ms_custom_ops::pyboost_add, "add", pybind11::arg("x"),
        pybind11::arg("y"));
}
```

#### internalç®—å­

åœ¨`ccsrc/ops/ms_kernels_internal/{op_name}.cc`æ–‡ä»¶å®ç°ç®—å­kernelçš„è°ƒç”¨ã€‚ç®—å­çš„pyboostå’Œgraphè°ƒç”¨å®ç°å‡åœ¨æ­¤æ–‡ä»¶ã€‚
è¦å®ç°çš„ä¸»è¦ç±»å’Œå‡½æ•°ï¼š  
graphï¼š  
1ï¼‰ç®—å­inferå‡½æ•°ï¼Œç”¨æ¥æ¨å¯¼ç®—å­è¾“å‡ºshapeå’Œdtypeï¼›  
2ï¼‰ç®—å­KernelModï¼Œéœ€è¦ç»§æ‰¿`InternalKernelMod`å¹¶é‡å†™`InitKernelInputsOutputsIndex`å’Œ`CreateKernel`æ¥å£ï¼› 

pyboost:  
1ï¼‰ç®—å­kernelè°ƒç”¨å‡½æ•°;  
2ï¼‰pybindæ¥å£æ³¨å†Œï¼›  

ä»¥reshape_and_cacheç®—å­ä¸ºä¾‹ï¼š
```cpp
#include "ccsrc/base/ms_kernels_internal/graphmode/internal_kernel_mod.h"
#include "ms_extension/api.h"
#include "ccsrc/utils/utils.h"

namespace ms_custom_ops {

// =============================================================================
// å›¾æ¨¡å¼è°ƒç”¨å®ç°
// =============================================================================

// 1. ç®—å­inferå‡½æ•°
class OPS_API CustomReshapeAndCacheOpFuncImpl : public OpFuncImpl {
public:
  ShapeArray InferShape(const PrimitivePtr &primitive,
                        const InferInfoPtrList &input_infos) const override {
    return {input_infos[0]->GetShape()}; // è¾“å‡ºshapeä¸ç¬¬ä¸€ä¸ªè¾“å…¥ç›¸åŒ
  }

  std::vector<TypeId> InferType(const PrimitivePtr &primitive,
                                const InferInfoPtrList &input_infos) const override {
    return {input_infos[0]->GetType()}; // è¾“å‡ºç±»å‹ä¸ç¬¬ä¸€ä¸ªè¾“å…¥ç›¸åŒ
  }
  
  bool GeneralInferRegistered() const override { return true; }
};

// 2. ç®—å­KernelMod
class CustomReshapeAndCache : public InternalKernelMod {
public:
  CustomReshapeAndCache() : InternalKernelMod(), skip_execution_(false) {}
  ~CustomReshapeAndCache() = default;

  void InitKernelInputsOutputsIndex() override {
    // æŒ‡å®šå‚ä¸è®¡ç®—çš„è¾“å…¥è¾“å‡ºç´¢å¼•
    kernel_inputs_index_ = {0, 1, 2, 3, 4}; // key, value, key_cache, value_cache, slot_mapping
    kernel_outputs_index_ = {0};
  }

  // é‡å†™Resizeå¤„ç†é›¶ç»´åº¦è¾“å…¥
  int Resize(const std::vector<KernelTensor *> &inputs, 
             const std::vector<KernelTensor *> &outputs) override {
    // æ£€æŸ¥è¾“å…¥æ˜¯å¦åŒ…å«0ç»´åº¦ï¼Œå¦‚æœæœ‰åˆ™è·³è¿‡æ‰§è¡Œ
    for (const auto &input : inputs) {
      if (input == nullptr) continue;
      auto shape = input->GetShapeVector();
      for (const auto &dim : shape) {
        if (dim == 0) {
          skip_execution_ = true;
          return KernelMod::Resize(inputs, outputs);
        }
      }
    }
    skip_execution_ = false;
    return InternalKernelMod::Resize(inputs, outputs);
  }

  // é‡å†™Launchå¤„ç†è·³è¿‡æ‰§è¡Œæ ‡å¿—
  bool Launch(const std::vector<KernelTensor *> &inputs,
              const std::vector<KernelTensor *> &workspace,
              const std::vector<KernelTensor *> &outputs, 
              void *stream_ptr) override {
    if (skip_execution_) {
      return true; // è·³è¿‡æ‰§è¡Œï¼Œç›´æ¥è¿”å›æˆåŠŸ
    }
    return InternalKernelMod::Launch(inputs, workspace, outputs, stream_ptr);
  }

protected:
  internal::InternalOpPtr CreateKernel(
      const internal::InputsImmutableInfoList &inputs,
      const internal::OutputsImmutableInfoList &outputs,
      const std::vector<KernelTensor *> &ms_inputs,
      const std::vector<KernelTensor *> &ms_outputs) override {
    // ä»è¾“å…¥å¼ é‡ä¸­æå–å‚æ•°
    internal::ReshapeAndCacheParam param;
    auto head_num = ms_inputs.at(6); // head_numåœ¨ç¬¬6ä¸ªä½ç½®
    param.head_num = static_cast<int32_t>(head_num->GetValue<int64_t>().value());
    
    auto cache_mode = ms_inputs.at(5); // cache_modeåœ¨ç¬¬5ä¸ªä½ç½®
    int32_t cache_mode_val = static_cast<int32_t>(cache_mode->GetValue<int64_t>().value());

    // æ ¹æ®cache_modeè®¾ç½®æ ¼å¼ï¼šNZæ ¼å¼éœ€è¦ç‰¹æ®Šå¤„ç†
    if (cache_mode_val == 1) { // NZæ ¼å¼
      auto inputs_clone = inputs;
      inputs_clone[2].SetFormat(internal::kFormatFRACTAL_NZ); // key_cache
      inputs_clone[3].SetFormat(internal::kFormatFRACTAL_NZ); // value_cache
      return internal::CreateAsdReshapeAndCacheOp(inputs_clone, outputs, param,
                                                  internal::kInternalAsdReshapeAndCacheOpName);
    }
    return internal::CreateAsdReshapeAndCacheOp(inputs, outputs, param, 
                                                internal::kInternalAsdReshapeAndCacheOpName);
  }

private:
  bool skip_execution_; // è·³è¿‡æ‰§è¡Œæ ‡å¿—
};
} // namespace ms_custom_ops

// æ³¨å†Œç®—å­
REG_GRAPH_MODE_OP(reshape_and_cache, ms_custom_ops::CustomReshapeAndCacheOpFuncImpl,
                  ms_custom_ops::CustomReshapeAndCache);

// =============================================================================
// PYBOOSTè°ƒç”¨å®ç°
// =============================================================================

#include "internal_pyboost_runner.h"

namespace ms_custom_ops {
// 1. åˆ›å»ºç®—å­Pyboostæ‰§è¡Œå™¨
class ReshapeAndCacheRunner : public InternalPyboostRunner {
public:
  using InternalPyboostRunner::InternalPyboostRunner;

  void SetHeadNum(const int32_t &head_num) { this->head_num_ = head_num; }
  void SetCacheMode(const int32_t &cache_mode) { this->cache_mode_ = cache_mode; }

protected:
  internal::InternalOpPtr CreateKernel(
      const internal::InputsImmutableInfoList &inputs,
      const internal::OutputsImmutableInfoList &outputs) override {
    internal::ReshapeAndCacheParam param;
    param.head_num = this->head_num_;
    
    // æ ¹æ®cache_modeè®¾ç½®æ ¼å¼
    if (this->cache_mode_ == 1) { // NZæ ¼å¼
      auto inputs_clone = inputs;
      inputs_clone[2].SetFormat(internal::kFormatFRACTAL_NZ);
      inputs_clone[3].SetFormat(internal::kFormatFRACTAL_NZ);
      return internal::CreateAsdReshapeAndCacheOp(inputs_clone, outputs, param,
                                                  internal::kInternalAsdReshapeAndCacheOpName);
    }
    return internal::CreateAsdReshapeAndCacheOp(inputs, outputs, param, 
                                                internal::kInternalAsdReshapeAndCacheOpName);
  }

private:
  int32_t head_num_{0};
  int32_t cache_mode_{0};
};

// 2. ç®—å­kernelè°ƒç”¨å‡½æ•°
void npu_reshape_and_cache(const ms::Tensor &key,
                           const std::optional<ms::Tensor> &value,
                           const std::optional<ms::Tensor> &key_cache,
                           const std::optional<ms::Tensor> &value_cache,
                           const std::optional<ms::Tensor> &slot_mapping,
                           std::optional<int64_t> cache_mode,
                           std::optional<int64_t> head_num) {
  auto op_name = "ReshapeAndCache";
  auto runner = std::make_shared<ms_custom_ops::ReshapeAndCacheRunner>(op_name);
  MS_EXCEPTION_IF_NULL(runner);

  // è®¾ç½®å‚æ•°
  if (cache_mode.has_value()) {
    runner->SetCacheMode(static_cast<int32_t>(cache_mode.value()));
  }
  if (head_num.has_value()) {
    runner->SetHeadNum(static_cast<int32_t>(head_num.value()));
  }

  // æ‰§è¡Œç®—å­
  runner->Setup(op_name, key, value, key_cache, value_cache, slot_mapping, 
                cache_mode, head_num);
  std::vector<ms::Tensor> inputs = {
      key, GetTensorOrEmpty(value), GetTensorOrEmpty(key_cache),
      GetTensorOrEmpty(value_cache), GetTensorOrEmpty(slot_mapping)};
  std::vector<ms::Tensor> outputs = {};
  runner->GetOrCreateKernel(inputs, outputs);
  runner->Run(inputs, outputs);
}

// 3. pybindæ¥å£æ³¨å†Œ
auto pyboost_reshape_and_cache(const ms::Tensor &key,
                               const std::optional<ms::Tensor> &value,
                               const std::optional<ms::Tensor> &key_cache,
                               const std::optional<ms::Tensor> &value_cache,
                               const std::optional<ms::Tensor> &slot_mapping,
                               std::optional<int64_t> cache_mode,
                               std::optional<int64_t> head_num) {
  // Call<è¾“å‡ºTensorçš„ä¸ªæ•°>(ç®—å­kernelè°ƒç”¨å‡½æ•°, è¾“å…¥Tensor...)
  return ms::pynative::PyboostRunner::Call<0>(ms_custom_ops::npu_reshape_and_cache, 
                                             key, value, key_cache, value_cache,
                                             slot_mapping, cache_mode, head_num);
}
} // namespace ms_custom_ops

// æ³¨å†ŒPythonæ¥å£
MS_CUSTOM_OPS_EXTENSION_MODULE(m) {
  m.def("reshape_and_cache", &pyboost_reshape_and_cache, "Reshape And Cache",
        pybind11::arg("key"),
        pybind11::arg("value") = std::nullopt,
        pybind11::arg("key_cache") = std::nullopt,
        pybind11::arg("value_cache") = std::nullopt,
        pybind11::arg("slot_mapping") = std::nullopt,
        pybind11::arg("cache_mode") = std::nullopt,
        pybind11::arg("head_num") = std::nullopt);
}
```

#### 3. ç‰¹æ®Šformatçš„æ”¯æŒ

**èƒŒæ™¯è¯´æ˜**ï¼š
æŸäº›ç®—å­éœ€è¦æ”¯æŒç‰¹æ®Šçš„æ•°æ®æ ¼å¼ï¼ˆå¦‚FRACTAL_NZï¼‰ï¼Œä½†MindSporeæ¡†æ¶ä¸æä¾›è‡ªåŠ¨formatæ¨å¯¼èƒ½åŠ›ã€‚å› æ­¤éœ€è¦é€šè¿‡ç”¨æˆ·å‚æ•°æ¥æŒ‡å®šæ ¼å¼ç±»å‹ï¼Œå¹¶é…åˆ`trans_data`ç®—å­è¿›è¡Œæ ¼å¼è½¬æ¢ã€‚

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š

1. **æ ¼å¼è½¬æ¢ç®—å­**ï¼š`trans_data`
   - `transdata_type=0`: FRACTAL_NZ_TO_ND (NZâ†’ND)
   - `transdata_type=1`: ND_TO_FRACTAL_NZ (NDâ†’NZ)
   - ç”¨äºåœ¨ä¸åŒæ•°æ®æ ¼å¼é—´è¿›è¡Œæ— æŸè½¬æ¢

2. **ç®—å­æ ¼å¼é€‚é…**ï¼šé€šè¿‡å‚æ•°æ§åˆ¶å†…éƒ¨æ ¼å¼å¤„ç†
   - `cache_mode=0`: NDæ ¼å¼æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
   - `cache_mode=1`: FRACTAL_NZæ ¼å¼æ¨¡å¼

**å…¸å‹ä½¿ç”¨æ¨¡å¼**ï¼š

**æ¨¡å¼1ï¼šæ”¯æŒå¤šæ ¼å¼çš„ç®—å­**
```python
# NDæ ¼å¼æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
ms_custom_ops.reshape_and_cache(key, value, key_cache, value_cache, 
                                slot_mapping, cache_mode=0)

# FRACTAL_NZæ ¼å¼æ¨¡å¼
# 1. å°†NDæ ¼å¼ç¼“å­˜è½¬æ¢ä¸ºNZæ ¼å¼
key_cache_nz = ms_custom_ops.trans_data(key_cache, transdata_type=1)  # NDâ†’NZ
value_cache_nz = ms_custom_ops.trans_data(value_cache, transdata_type=1)  # NDâ†’NZ

# 2. ä½¿ç”¨NZæ ¼å¼æ¨¡å¼æ‰§è¡Œç®—å­
ms_custom_ops.reshape_and_cache(key, value, key_cache_nz, value_cache_nz, 
                                slot_mapping, cache_mode=1)

# 3. å¦‚éœ€è¦ï¼Œå°†ç»“æœè½¬æ¢å›NDæ ¼å¼è¿›è¡ŒéªŒè¯
key_cache_result = ms_custom_ops.trans_data(key_cache_nz, transdata_type=0)  # NZâ†’ND
value_cache_result = ms_custom_ops.trans_data(value_cache_nz, transdata_type=0)  # NZâ†’ND
```

**æ¨¡å¼2ï¼šä¸“ç”¨æ ¼å¼è½¬æ¢ç®—å­**
```python
# å•çº¯çš„æ ¼å¼è½¬æ¢
nz_tensor = ms_custom_ops.trans_data(nd_tensor, transdata_type=1)  # NDâ†’NZ
nd_tensor = ms_custom_ops.trans_data(nz_tensor, transdata_type=0)   # NZâ†’ND
```

**å®ç°æ­¥éª¤**ï¼š

1. **æ·»åŠ æ ¼å¼é€‰æ‹©å‚æ•°**
   - ä¸ºç®—å­æ·»åŠ formaté€‰æ‹©å‚æ•°ï¼ˆå¦‚`cache_mode`ï¼‰
   - å®šä¹‰æ ¼å¼æ˜ å°„å…³ç³»ï¼š`0`=NDæ ¼å¼ï¼Œ`1`=FRACTAL_NZæ ¼å¼

2. **å®ç°æ ¼å¼è½¬æ¢é€»è¾‘**
   - åœ¨`CreateKernel`å‡½æ•°ä¸­æ ¹æ®å‚æ•°å€¼åˆ¤æ–­æ˜¯å¦éœ€è¦æ ¼å¼è½¬æ¢
   - å¯¹éœ€è¦ç‰¹æ®Šæ ¼å¼çš„è¾“å…¥å¼ é‡è°ƒç”¨`SetFormat()`æ–¹æ³•

**ä»£ç ç¤ºä¾‹**ï¼ˆä»¥reshape_and_cacheä¸ºä¾‹ï¼‰ï¼š
```cpp
// åœ¨CreateKernelå‡½æ•°ä¸­å®ç°æ ¼å¼é€‚é…
internal::InternalOpPtr CreateKernel(
    const internal::InputsImmutableInfoList &inputs,
    const internal::OutputsImmutableInfoList &outputs,
    const std::vector<KernelTensor *> &ms_inputs,
    const std::vector<KernelTensor *> &ms_outputs) override {
  
  // è·å–æ ¼å¼å‚æ•°
  auto cache_mode = ms_inputs.at(5); // cache_modeå‚æ•°ä½ç½®
  int32_t cache_mode_val = static_cast<int32_t>(cache_mode->GetValue<int64_t>().value());
  
  // æ ¹æ®å‚æ•°è®¾ç½®ç‰¹æ®Šæ ¼å¼
  if (cache_mode_val == 1) { // FRACTAL_NZæ ¼å¼
    auto inputs_clone = inputs;
    inputs_clone[2].SetFormat(internal::kFormatFRACTAL_NZ); // key_cache
    inputs_clone[3].SetFormat(internal::kFormatFRACTAL_NZ); // value_cache
    return internal::CreateAsdReshapeAndCacheOp(inputs_clone, outputs, param, op_name);
  }
  
  // é»˜è®¤NDæ ¼å¼ï¼Œæ— éœ€è½¬æ¢
  return internal::CreateAsdReshapeAndCacheOp(inputs, outputs, param, op_name);
}
```

**æµ‹è¯•ä¸­çš„ä½¿ç”¨æ¨¡å¼**ï¼ˆä»¥NZæ ¼å¼æµ‹è¯•ä¸ºä¾‹ï¼‰ï¼š
```cpp
// NZ Format Test Flow:
// 1. Create initial ND format cache tensors
np_k, np_v, np_k_cache, np_v_cache, np_slot_map = create_nd_inputs(...)

// 2. Convert cache tensors to FRACTAL_NZ format
ms_k_cache = ms_custom_ops.trans_data(ms_k_cache, transdata_type=1)  # NDâ†’NZ
ms_v_cache = ms_custom_ops.trans_data(ms_v_cache, transdata_type=1)  # NDâ†’NZ

// 3. Run ReshapeAndCache with cache_mode=1 (NZ format mode)
net(key, value, ms_k_cache, ms_v_cache, slot_mapping, cache_mode=1)

// 4. Convert results back to ND format for verification
ms_k_cache_nd = ms_custom_ops.trans_data(ms_k_cache, transdata_type=0)  # NZâ†’ND
ms_v_cache_nd = ms_custom_ops.trans_data(ms_v_cache, transdata_type=0)  # NZâ†’ND

// 5. Compare with golden ND results
verify_results(ms_k_cache_nd, golden_k_output, dtype)
```

**å…³é”®æ³¨æ„äº‹é¡¹**ï¼š
- âœ… **æ•°æ®ä¸€è‡´æ€§**ï¼šæ ¼å¼è½¬æ¢åº”ä¿æŒæ•°æ®å®Œå…¨ä¸€è‡´ï¼Œä»»ä½•ç²¾åº¦æŸå¤±éƒ½å¯èƒ½è¡¨æ˜å®ç°é”™è¯¯
- âœ… **Internalç®—å­**ï¼šåº•å±‚ç®—å­åº“ä¼šè‡ªåŠ¨å¤„ç†shapeè½¬æ¢ï¼Œç”¨æˆ·åªéœ€è®¾ç½®formatå³å¯
- âš ï¸ **AscendCç®—å­**ï¼šéœ€è¦ç”¨æˆ·æ‰‹åŠ¨å®ç°formatè½¬æ¢å’Œshapeè®¡ç®—é€»è¾‘
- ğŸ“ **å‚æ•°è®¾è®¡**ï¼šå»ºè®®ä½¿ç”¨æšä¸¾å€¼ï¼ˆ0,1,2...ï¼‰è€Œéå­—ç¬¦ä¸²ï¼Œæé«˜æ€§èƒ½
- ğŸ” **æµ‹è¯•éªŒè¯**ï¼šç¡®ä¿ä¸åŒformatä¸‹çš„è¾“å…¥è¾“å‡ºshapeå’Œæ•°æ®æ­£ç¡®æ€§
- ğŸ’¡ **æ€§èƒ½ä¼˜åŒ–**ï¼šé¿å…ä¸å¿…è¦çš„æ ¼å¼è½¬æ¢ï¼Œå°½é‡åœ¨åŒä¸€æ ¼å¼ä¸‹å®Œæˆæ•´ä¸ªè®¡ç®—æµç¨‹

**æ ¼å¼è½¬æ¢æ•°æ®ç±»å‹æ”¯æŒ**ï¼š
- âœ… **FRACTAL_NZ_TO_ND**: æ”¯æŒ float16, bfloat16ï¼ˆint8ä¸æ”¯æŒï¼‰
- âœ… **ND_TO_FRACTAL_NZ**: æ”¯æŒ float16, bfloat16, int8
- âš ï¸ **å¯¹é½è¦æ±‚**: float16/bfloat16éœ€è¦16å­—èŠ‚å¯¹é½ï¼Œint8éœ€è¦32å­—èŠ‚å¯¹é½

**é€‚é…æ£€æŸ¥æ¸…å•**ï¼š
- [ ] æ˜¯å¦æ·»åŠ äº†formaté€‰æ‹©å‚æ•°ï¼Ÿ
- [ ] æ˜¯å¦æ­£ç¡®ä½¿ç”¨äº†trans_dataè¿›è¡Œæ ¼å¼è½¬æ¢ï¼Ÿ
- [ ] æ˜¯å¦åœ¨ä¸¤ç§æ¨¡å¼ï¼ˆgraph/pyboostï¼‰ä¸­éƒ½å®ç°äº†æ ¼å¼è½¬æ¢ï¼Ÿ
- [ ] æ˜¯å¦éªŒè¯äº†ä¸åŒæ ¼å¼ä¸‹çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Ÿ
- [ ] æ˜¯å¦æµ‹è¯•äº†æ ¼å¼è½¬æ¢çš„å¾€è¿”ä¸€è‡´æ€§ï¼Ÿ
- [ ] æ˜¯å¦åœ¨æ–‡æ¡£ä¸­è¯´æ˜äº†å‚æ•°å«ä¹‰å’Œä½¿ç”¨æ–¹å¼ï¼Ÿ