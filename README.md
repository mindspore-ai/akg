# ms_custom_ops - MindSpore è‡ªå®šä¹‰ç®—å­æ¡†æ¶

[![License](https://img.shields.io/badge/License-Apache%202.0lue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![MindSpore](https://img.shields.io/badge/MindSpore-2.6-green.svg)](https://www.mindspore.cn/)

## ğŸ“– æ¦‚è¿°

`ms_custom_ops` æ˜¯ä¸€ä¸ªä¸“ä¸º MindSpore è®¾è®¡çš„è‡ªå®šä¹‰ç®—å­å¼€å‘æ¡†æ¶ï¼Œæ”¯æŒåœ¨æ˜‡è…¾ NPU ä¸Šé«˜æ•ˆå®ç°è‡ªå®šä¹‰ç®—å­ã€‚è¯¥æ¡†æ¶æä¾›äº†å®Œæ•´çš„ PyNative å’Œ Graph ä¸¤ç§æ‰§è¡Œæ¨¡å¼æ”¯æŒï¼Œå†…ç½®ç¼“å­˜ä¼˜åŒ–ã€å†…å­˜ç®¡ç†ç­‰é«˜çº§ç‰¹æ€§ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿå¿«é€Ÿæ„å»ºé«˜æ€§èƒ½çš„è‡ªå®šä¹‰ç®—å­ã€‚

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- **åŒæ¨¡å¼æ”¯æŒ**: åŒæ—¶æ”¯æŒ PyNative åŠ¨æ€æ‰§è¡Œå’Œ Graph é™æ€ç¼–è¯‘æ¨¡å¼
- **æ˜‡è…¾ä¼˜åŒ–**: ä¸“ä¸ºæ˜‡è…¾ NPU è®¾è®¡ï¼Œå……åˆ†åˆ©ç”¨ç¡¬ä»¶ç‰¹æ€§
- **ç¼“å­˜æœºåˆ¶**: å†…ç½®ç®—å­ç¼“å­˜å’Œ Tiling ç¼“å­˜ï¼Œæ˜¾è‘—æå‡æ€§èƒ½
- **å†…å­˜ç®¡ç†**: è‡ªåŠ¨ç®¡ç†è®¾å¤‡å†…å­˜å’Œä¸»æœºå†…å­˜ï¼Œç¡®ä¿å†…å­˜å®‰å…¨
- **å¼€å‘å‹å¥½**: æä¾›å®Œæ•´çš„å¼€å‘å·¥å…·é“¾å’Œæµ‹è¯•æ¡†æ¶

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

### æ ¸å¿ƒç»„ä»¶

1. **PyBoost æ¡†æ¶**ï¼šç”¨äº PyNative æ¨¡å¼ä¸‹çš„åŠ¨æ€æ‰§è¡Œ
2. **GraphMode æ¡†æ¶**ï¼šç”¨äºé™æ€å›¾ç¼–è¯‘æ¨¡å¼
3. **å…±äº«ç»„ä»¶**ï¼šåŒ…æ‹¬å†…å­˜ç®¡ç†ã€ç¼“å­˜ä¼˜åŒ–ç­‰é€šç”¨åŠŸèƒ½

### æ ¸å¿ƒæ¨¡å—è¯´æ˜

#### 1. ms_kernels_internal - å†…éƒ¨ç®—å­æ¡†æ¶
- **ccsrc/base/ms_kernels_internal/pyboost**: PyNativeæ¨¡å¼ä¸‹çš„ç®—å­å…¬å…±åŸºç±»å®ç°
- **ccsrc/base/ms_kernels_internal/graphmode**: Graphæ¨¡å¼ä¸‹çš„ç®—å­å…¬å…±åŸºç±»å®ç°
- **ccsrc/ops/ms_kernels_internal/*.cc**: ç®—å­è°ƒç”¨å®ç°

- **å…¬å…±æ–‡ä»¶**:
  - **tiling_mem_mgr.h/cc**: Tilingå†…å­˜ç®¡ç†å™¨ï¼Œè´Ÿè´£è®¾å¤‡å†…å­˜åˆ†é…å’Œé‡Šæ”¾
  - **internal_tiling_cache.h/cc**: å†…éƒ¨Tilingç¼“å­˜ï¼Œæä¾›ç®—å­ç¼“å­˜å’ŒTilingç­–ç•¥ç¼“å­˜
  - **internal_helper.h/cc**: å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼Œæä¾›é€šç”¨å·¥å…·å‡½æ•°
  - **internal_spinlock.h**: è‡ªæ—‹é”å®ç°ï¼Œç”¨äºå¤šçº¿ç¨‹åŒæ­¥

#### 2. ascendc - æ˜‡è…¾Cç®—å­æ¡†æ¶
- **ccsrc/base/ascendc/pyboost**: PyNativeæ¨¡å¼ä¸‹çš„ç®—å­å…¬å…±åŸºç±»å®ç°
- **ccsrc/base/ascendc/graphmode**: Graphæ¨¡å¼ä¸‹çš„ç®—å­å…¬å…±åŸºç±»å®ç°
- **ccsrc/ops/ascendc/**: ç®—å­kernelå’Œè°ƒç”¨å®ç°

### ç›®å½•ç»“æ„

```
ms_custom_ops/
â”œâ”€â”€ ccsrc/                        # C++æ ¸å¿ƒæºç 
â”‚   â”œâ”€â”€ base/                     # åŸºç¡€è®¾æ–½
â”‚   â”‚   â”œâ”€â”€ ms_kernels_internal/  # å†…éƒ¨ç®—å­åŸºç¡€æ¡†æ¶
â”‚   â”‚   â”‚   â”œâ”€â”€ pyboost/          # PyNativeæ¨¡å¼åŸºç±»/å·¥å…·
â”‚   â”‚   â”‚   â”œâ”€â”€ graphmode/        # Graphæ¨¡å¼åŸºç±»/å·¥å…·
â”‚   â”‚   â”‚   â”œâ”€â”€ tiling_mem_mgr.h/cc      # Tilingå†…å­˜ç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ internal_helper.h/cc      # å†…éƒ¨è¾…åŠ©å‡½æ•°
â”‚   â”‚   â”‚   â”œâ”€â”€ internal_spinlock.h       # è‡ªæ—‹é”å®ç°
â”‚   â”‚   â”‚   â””â”€â”€ internal_tiling_cache.h/cc # å†…éƒ¨Tilingç¼“å­˜
â”‚   â”‚   â””â”€â”€ ascendc/              # æ˜‡è…¾ç®—å­åŸºç¡€
â”‚   â”‚       â”œâ”€â”€ pyboost/
â”‚   â”‚       â””â”€â”€ graphmode/
â”‚   â”œâ”€â”€ ops/                      # ç®—å­å®ç°
â”‚   â”‚   â”œâ”€â”€ ms_kernels_internal/
â”‚   â”‚   â”‚   â””â”€â”€ {op_name}.cc
â”‚   â”‚   â”œâ”€â”€ ascendc/
â”‚   â”‚   â”‚   â”œâ”€â”€ {op_name}/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ {op_name}.cc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ op_host/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ op_kernel/
â”‚   â”‚   â”‚   â””â”€â”€ CMakeLists.txt
â”‚   â”‚   â””â”€â”€ CMakeLists.txt
â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”œâ”€â”€ module.h
â”‚   â””â”€â”€ module.cc
â”œâ”€â”€ cmake/                        # CMakeé…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ compile_ascendc_ops.cmake
â”‚   â””â”€â”€ find_ms_internal_kernels_lib.cmake
â”œâ”€â”€ python/
â”‚   â””â”€â”€ ms_custom_ops/
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ yaml/                         # ç®—å­é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ ascendc/
â”‚   |   â””â”€â”€ {op_name}_op.yaml
â”‚   â”œâ”€â”€ doc/
â”‚   |   â””â”€â”€ {op_name}_doc.yaml
â”‚   â””â”€â”€ ms_kernels_internal/
â”‚   |   â””â”€â”€ {op_name}_op.yaml
â”œâ”€â”€ tests/                        # æµ‹è¯•æ–‡ä»¶
â”‚   â””â”€â”€ st/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ op_compiler.py
â”œâ”€â”€ build.sh                      # ä¸€é”®ç¼–è¯‘è„šæœ¬
â”œâ”€â”€ setup.py                      # Pythonå®‰è£…é…ç½®
â”œâ”€â”€ requirements.txt               # Pythonä¾èµ–
â”œâ”€â”€ version.txt                   # ç‰ˆæœ¬ä¿¡æ¯
â””â”€â”€ README.md
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²å®‰è£…ï¼š
- **MindSpore**: br_infer_iteråˆ†æ”¯æ—¥æ„å»ºåŒ…
- **æ˜‡è…¾ CANN å·¥å…·åŒ…**: æœ€æ–°ç‰ˆæœ¬
- **CMake**: >= 3.16
- **Python**: >= 3.9
- **Git**: ç”¨äºè·å–æäº¤ä¿¡æ¯

### 2. ç¯å¢ƒé…ç½®

```bash
# è®¾ç½®æ˜‡è…¾ç¯å¢ƒå˜é‡
source ${CANN_INSTALL_PATH}/ascend-toolkit/set_env.sh
```

### 3. ç¼–è¯‘å®‰è£…

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ build.sh è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# æŸ¥çœ‹ç¼–è¯‘é€‰é¡¹
bash build.sh -h

# é»˜è®¤ç¼–è¯‘ï¼ˆReleaseæ¨¡å¼ï¼‰
bash build.sh

# Debugç¼–è¯‘
bash build.sh -d

# ç¼–è¯‘æŒ‡å®šç®—å­
bash build.sh -p ${absolute_op_dir_path}

# ç¼–è¯‘æŒ‡å®šç®—å­
bash build.sh -p ${absolute_op_dir_path}
eg. bash build.sh -p /home/ms_custom_ops/ccsrc/ops/ascendc/add,/home/ms_custom_ops/ccsrc/ops/ascendc/add_rms_norm

# æŒ‡å®šSOC Verisonç¼–è¯‘
eg. bash build.sh -v ascend910b4
```

#### æ–¹æ³•äºŒï¼šä½¿ç”¨ setup.py å®‰è£…

```bash
# å®‰è£…ï¼ˆä¼šè‡ªåŠ¨ç¼–è¯‘è‡ªå®šä¹‰ç®—å­ï¼‰
python setup.py install

# æˆ–è€…æ„å»ºwheelåŒ…
python setup.py bdist_wheel
```

ç¼–è¯‘è¿‡ç¨‹ä¼šè‡ªåŠ¨ï¼š
- æ£€æµ‹æ˜‡è…¾ç¯å¢ƒ
- ä½¿ç”¨ CMake æ„å»ºè‡ªå®šä¹‰ç®—å­
- å°†ç”Ÿæˆçš„ .so æ–‡ä»¶å®‰è£…åˆ°æ­£ç¡®ä½ç½®

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### PyNative æ¨¡å¼

```python
import mindspore as ms
import ms_custom_ops

# ç›´æ¥è°ƒç”¨è‡ªå®šä¹‰ç®—å­
output = ms_custom_ops.reshape_and_cache(
    key, value, key_cache, value_cache, slot_mapping, head_num
)
```

### Graph æ¨¡å¼

```python
import mindspore as ms
import ms_custom_ops

reshape_and_cache = ms.jit(func=ms_custom_ops.reshape_and_cache)
output = reshape_and_cache(key, value, key_cache, value_cache, slot_mapping, head_num)
```

## å¼€å‘è‡ªå®šä¹‰ç®—å­

### å¼€å‘æµç¨‹æ¦‚è§ˆ

å¼€å‘ä¸€ä¸ªæ–°çš„è‡ªå®šä¹‰ç®—å­éœ€è¦ä»¥ä¸‹æ­¥éª¤ï¼š

1. **è®¾è®¡ç®—å­æ¥å£** - ç¡®å®šè¾“å…¥è¾“å‡ºå’Œå‚æ•°ï¼Œç¼–å†™ç®—å­å‰ç«¯æ¥å£å®šä¹‰
2. **å®ç°ç®—å­é€»è¾‘** - ç¼–å†™PyBoostå’ŒGraphModeå®ç°
3. **ç¼–å†™æµ‹è¯•ç”¨ä¾‹** - åˆ›å»ºå•å…ƒæµ‹è¯•
4. **ç¼–è¯‘å’ŒéªŒè¯** - æ„å»ºå¹¶æµ‹è¯•ç®—å­

#### 1. è®¾è®¡ç®—å­æ¥å£

éœ€è¦æ˜ç¡®ç®—å­è¾“å…¥è¾“å‡ºç±»å‹ï¼Œå¹¶æ ¹æ®ç¡®å®šçš„ç®—å­æ¥å£ï¼Œç¼–å†™ç®—å­å‰ç«¯å®šä¹‰çš„YAMLæ–‡ä»¶ï¼Œå¹¶å®ç°{op_name}_op.yamlå’Œ{op_name}_doc.yamlã€‚
{op_name}_doc.yamlæ˜¯ç®—å­å‚æ•°è¯´æ˜æ–‡ä»¶ï¼Œ{op_name}_op.yamlæ˜¯ç®—å­æ¥å£å®šä¹‰æ–‡ä»¶ï¼Œå…¶å†…å®¹æ ¼å¼ä»‹ç»ï¼š
```yaml
# Defining the function name and Primitive name of operators, use the '_' to separate words. For example, op_name is 'word1_word2', then the function name is 'word1_word2', and the Primitive class name is 'Word1Word2'.
<op_name>:
  # The 'args' is a fixed key of yaml file to define input args of operators.
  <args>:
    # Mandatory. For every arg, key is operators' argument name, and the value are some items, items' key name can be 'dtype', 'prim_init', 'default', 'type_cast','arg_handler'.
    <arg1>:
      # Mandatory. The 'dtype' is a fixed key.
      # Value is one of {int, float, bool, number, tensor, tuple, list, tuple[int], tuple[float], tuple[bool], tuple[number], tuple[tensor], list[int], list[float], list[bool], list[number], list[tensor]}.
      # If value is 'number', arg can be 'int', 'float' or 'bool'.
      <dtype>: <value>

      # Optional. The 'default' is a fixed key.
      # This item means input arg can use default value.
      # If arg_handler is not empty, the type of default value should be the first one of 'arg_handler_map'
      <default>: <value>

      # Optional. The 'prim_init' is a fixed key. Value can be 'True' or 'False', arg is arg of '__init__' of Primitive if value is 'True'.
      <prim_init>: <value>

      # Optional. The 'type_cast' is a fixed key. This item means can accept unmatchable input by implicit conversion. Value is one of {int, float, bool, number, tensor, tuple, list, tuple[int], tuple[float], tuple[bool], tuple[number], tuple[tensor], list[int], list[float], list[bool], list[number], list[tensor]}
      # Supported type cast now:
      # 1. int, float, bool, number <-> tensor.
      # 2. int, float, bool, number, tensor <-> list/tuple.
      # 3. list <-> tuple.
      <type_cast>: <value>

      # Optional. The 'arg_handler' is a fixed key. Value is a function name used to convert arg. For example, converting kernel size from 2 to (2, 2).
      <arg_handler>: <value>

    <arg2>:
      ...

    <args_signature>: #Optional
      # Optional. The 'rw_write' is a fixed key, 'arg_name' is the corresponding arg name.
      <rw_write>: <arg_name>

      # Optional. The 'rw_read' is a fixed key, 'arg_name' is the corresponding arg name.
      <rw_read>: <arg_name>

      # Optional. The 'rw_ref' is a fixed key, 'arg_name' is the corresponding arg name.
      <rw_ref>: <arg_name>

      # Optional. arg1 and arg2 should has same dtype. arg3 and arg4 should has same dtype.
      <dtype_group>: (<arg_name1>, <arg_name2>, ...), (<arg_name3>, <arg_name4>, ...), ...

    # The 'returns' is a fixed key of yaml file to define output of operators.
    <returns>:
      # Mandatory. For every output, key is operators' output name, and the value is a item, item's key is 'dtype'.
      <output1>:
        # Mandatory. Just refer to key 'dtype' in args.
        <dtype>: <value>

        # Optional. The 'inplace' is a fixed key. Value is input name of operator if the input is a inplace input.
        <inplace>: <value>

      <output2>:
        ...

    # Optional. The 'view' is a fixed key. Value should be set as 'True' if this is a view operator.
    # Default: False.
    <view>: <value>
```
å…·ä½“å¯å‚è€ƒ`yaml/ms_kernels_internal/reshape_and_cache_op.yaml`å’Œ`yaml/doc/reshape_and_cache_doc.yaml`

#### 2. å®ç°ç®—å­é€»è¾‘

#### ascendcç®—å­

åˆ›å»ºè‡ªå®šä¹‰ç®—å­ç›®å½•ï¼š`ccsrc/ops/ascendc/{op_name}`ï¼Œå…¶ä¸­`ccsrc/ops/ascendc/{op_name}/op_kernel`å’Œ`ccsrc/ops/ascendc/{op_name}/op_host`æ˜¯ascendcç®—å­kernelå®ç°ï¼Œç®—å­ä¼šè¢«ç¼–è¯‘æˆaclnnçš„ä¸¤æ®µå¼æ¥å£ã€‚  
åœ¨`ccsrc/ops/ascendc/{op_name}/{op_name}.cc`æ–‡ä»¶å®ç°ç®—å­kernelçš„è°ƒç”¨ã€‚ç®—å­çš„pyboostå’Œgraphè°ƒç”¨å®ç°å‡åœ¨æ­¤æ–‡ä»¶ã€‚
è¦å®ç°çš„ä¸»è¦ç±»å’Œå‡½æ•°ï¼š  
graph:  
1ï¼‰ç®—å­inferå‡½æ•°ï¼Œç”¨æ¥æ¨å¯¼ç®—å­è¾“å‡ºshapeå’Œdtypeï¼›  
2ï¼‰ç®—å­KernelModï¼Œéœ€è¦ç»§æ‰¿`AscendCKernelMod`å¹¶é‡å†™`Launch`å’Œ`GetWorkSpaceInfo`æ¥å£ï¼›  

pyboost:  
1ï¼‰ç®—å­kernelè°ƒç”¨å‡½æ•°;   
2ï¼‰pybindæ¥å£æ³¨å†Œï¼›  

ä»¥addç®—å­ä¸ºä¾‹ï¼š
```cpp
#include "ascendc_kernel_mod.h"
#include "ms_extension/api.h"
#include <map>
#include <string>
#include <vector>

// =============================================================================
// å›¾æ¨¡å¼è°ƒç”¨å®ç°
// =============================================================================

namespace ms_custom_ops {
// ç®—å­inferå‡½æ•°ï¼Œéœ€è¦å®ç°InferShapeå’ŒInferTypeå‡½æ•°
class OPS_API AddCustomOpFuncImpl : public OpFuncImpl {
public:
  // ç®—å­infershapeï¼Œéœ€è¦è¿”å›ç®—å­æ‰€æœ‰è¾“å‡ºçš„shapeå¤§å°
  ShapeArray InferShape(const PrimitivePtr &primitive,
                        const InferInfoPtrList &input_infos) const override {
    auto out_shape = input_infos[0]->GetShape();
    return {out_shape};
  }

  // ç®—å­infertypeï¼Œéœ€è¦è¿”å›ç®—å­æ‰€æœ‰è¾“å‡ºçš„æ•°æ®ç±»å‹
  std::vector<TypeId> InferType(const PrimitivePtr &primitive,
                                const InferInfoPtrList &input_infos) const override {
    return {input_infos[0]->GetType()};
  }

  bool GeneralInferRegistered() const override { return true; }
};

// ç®—å­graphæ¨¡å¼è°ƒç”¨ï¼Œaclnnä¸¤æ®µå¼æ¥å£è°ƒç”¨ï¼Œéœ€è¦å®ç°Launchå’ŒGetWorkSpaceInfoå‡½æ•°
class AddCustomAscend : public AscendCKernelMod {
public:
  AddCustomAscend() : AscendCKernelMod(std::move("aclnnAddCustom")) {}
  ~AddCustomAscend() = default;

  // ç®—å­æ‰§è¡Œè°ƒç”¨å‡½æ•°ï¼ŒRunOpå‡½æ•°ä¼šè°ƒç”¨aclnnç®—å­ç¬¬äºŒæ®µæ¥å£
  bool Launch(const std::vector<KernelTensor *> &inputs,
              const std::vector<KernelTensor *> &workspace,
              const std::vector<KernelTensor *> &outputs,
              void *stream_ptr) override {
    MS_EXCEPTION_IF_NULL(stream_ptr);
    RunOp(stream_ptr, workspace, inputs[0], inputs[1], outputs[0]);
    return true;
  }

  // ç®—å­workspaceè°ƒç”¨å‡½æ•°ï¼ŒGetWorkspaceForResizeä¼šè°ƒç”¨aclnnç®—å­ç¬¬ä¸€æ®µæ¥å£
  void GetWorkSpaceInfo(const std::vector<KernelTensor *> &inputs,
                        const std::vector<KernelTensor *> &outputs) override {
    GetWorkspaceForResize(inputs[0], inputs[1], outputs[0]);
  }

private:
  DEFINE_GET_WORKSPACE_FOR_RESIZE();
};
} // namespace ms_custom_ops

// æ³¨å†Œç®—å­inferå‡½æ•°ï¼Œç”¨äºåœ¨è®¡ç®—è¿‡ç¨‹ä¸­æ¨å¯¼ç®—å­è¾“å‡ºshapeå’Œdtypeï¼Œä»¥ä¾¿åˆ†é…ç®—å­è¾“å‡ºå†…å­˜
REG_GRAPH_MODE_OP(add, ms_custom_ops::AddCustomOpFuncImpl,
                  ms_custom_ops::AddCustomAscend);

// =============================================================================
// PYBOOSTè°ƒç”¨å®ç°
// =============================================================================

#include "ascendc_pyboost_runner.h"

namespace ms_custom_ops {
using namespace mindspore;
using namespace mindspore::device::ascend;
// ç®—å­kernelè°ƒç”¨å‡½æ•°ï¼Œéœ€è¦æ‰‹åŠ¨åˆ›å»ºè¾“å‡ºtensor
ms::Tensor custom_add(const ms::Tensor &x, const ms::Tensor &y) {
  // åˆ›å»ºè¾“å‡ºç©ºtensor
  auto out = ms::Tensor(x.data_type(), x.shape());
  // åˆå§‹åŒ–runnerè¿è¡Œå™¨
  auto runner = std::make_shared<ms::pynative::AscendCOpRunner>("AddCustom");
  // è®¾ç½®runneréœ€è¦å…·ä½“æ‰§è¡Œçš„å‡½æ•°ï¼Œç”±LAUNCH_ASCENDC_FUNCå°è£…äº†aclnnæ¥å£è°ƒç”¨
  runner->SetLaunchFunc(LAUNCH_ASCENDC_FUNC(aclnnAddCustom, x, y, out));
  // æ‰§è¡Œrunner
  runner->Run({x, y}, {out});
  return out;
}

// pybindè°ƒç”¨å‡½æ•°
auto pyboost_add(const ms::Tensor &x, const ms::Tensor &y) {
  return ms::pynative::PyboostRunner::Call<1>(custom_add, x, y);
}
} // namespace ms_custom_ops

// ç®—å­æ¥å£æ³¨å†Œï¼Œå¯¹æ¥C++å’Œpythonæ¥å£
MS_CUSTOM_OPS_EXTENSION_MODULE(m) {
  m.def("add", &ms_custom_ops::pyboost_add, "add", pybind11::arg("x"),
        pybind11::arg("y"));
}
```

#### internalç®—å­

åœ¨`ccsrc/ops/ms_kernels_internal/{op_name}.cc`æ–‡ä»¶å®ç°ç®—å­kernelçš„è°ƒç”¨ã€‚ç®—å­çš„pyboostå’Œgraphè°ƒç”¨å®ç°å‡åœ¨æ­¤æ–‡ä»¶ã€‚
è¦å®ç°çš„ä¸»è¦ç±»å’Œå‡½æ•°ï¼š  
graphï¼š  
1ï¼‰ç®—å­inferå‡½æ•°ï¼Œç”¨æ¥æ¨å¯¼ç®—å­è¾“å‡ºshapeå’Œdtypeï¼›  
2ï¼‰ç®—å­KernelModï¼Œéœ€è¦ç»§æ‰¿`InternalKernelMod`å¹¶é‡å†™`InitKernelInputsOutputsIndex`å’Œ`CreateKernel`æ¥å£ï¼› 

pyboost:  
1ï¼‰ç®—å­kernelè°ƒç”¨å‡½æ•°;  
2ï¼‰pybindæ¥å£æ³¨å†Œï¼›  

ä»¥reshape_and_cacheç®—å­ä¸ºä¾‹ï¼š
```cpp
#include "internal_kernel_mod.h"
#include "ir/tensor.h"
#include "kernel/ascend/acl_ir/acl_convert.h"
#include "mindspore/ops/ops_utils/op_utils.h"
#include "ms_extension/api.h"
#include "ops/base_operator.h"
#include "ops/ops_func_impl/op_func_impl.h"
#include "ops/ops_func_impl/simple_infer.h"
#include "runtime/device/kernel_runtime.h"
#include "utils/check_convert_utils.h"
#include <map>
#include <string>
#include <utility>
#include <vector>

// =============================================================================
// å›¾æ¨¡å¼è°ƒç”¨å®ç°
// =============================================================================

namespace ms_custom_ops {
// ç®—å­inferå‡½æ•°ï¼Œéœ€è¦å®ç°InferShapeå’ŒInferTypeå‡½æ•°
class OPS_API CustomReshapeAndCacheOpFuncImpl : public OpFuncImpl {
public:
  // ç®—å­infershapeï¼Œéœ€è¦è¿”å›ç®—å­æ‰€æœ‰è¾“å‡ºçš„shapeå¤§å°
  ShapeArray InferShape(const PrimitivePtr &primitive,
                        const InferInfoPtrList &input_infos) const override {
    return {input_infos[0]->GetShape()};
  }

  // ç®—å­infertypeï¼Œéœ€è¦è¿”å›ç®—å­æ‰€æœ‰è¾“å‡ºçš„æ•°æ®ç±»å‹
  std::vector<TypeId> InferType(const PrimitivePtr &primitive,
                                const InferInfoPtrList &input_infos) const override {
    return {input_infos[0]->GetType()};
  }

  bool GeneralInferRegistered() const override { return true; }
};

constexpr size_t kInputKeyIndex = 0;
constexpr size_t kInputValueIndex = 1;
constexpr size_t kInputKeyCacheIndex = 2;
constexpr size_t kInputValueCacheIndex = 3;
constexpr size_t kInputSlotMappingIndex = 4;
constexpr size_t kInputHeadNumIndex = 5;
constexpr size_t kOutputIndex = 0;
// ç®—å­graphæ¨¡å¼è°ƒç”¨ï¼Œéœ€è¦ç»§æ‰¿InternalKernelModåŸºç±»ï¼Œå¹¶å®ç°InitKernelInputsOutputsIndexå’ŒCreateKernelå‡½æ•°
class CustomReshapeAndCache : public InternalKernelMod {
public:
  CustomReshapeAndCache() : InternalKernelMod() {}
  ~CustomReshapeAndCache() = default;

  // æ˜¯ç®—å­å‰ç«¯å®šä¹‰çš„è¾“å…¥è¾“å‡ºå’Œç®—å­kernelè¾“å…¥è¾“å‡ºä½ç½®ç´¢å¼•çš„æ˜ å°„å…³ç³»ã€‚
  void InitKernelInputsOutputsIndex() override {
    kernel_inputs_index_ = {kInputKeyIndex, kInputValueIndex, kInputKeyCacheIndex,
                            kInputValueCacheIndex, kInputSlotMappingIndex};
    kernel_outputs_index_ = {kOutputIndex};
  }

protected:
  // åˆ›å»ºå…·ä½“ç®—å­çš„opå®ä¾‹
  internal::InternalOpPtr CreateKernel(const internal::InputsImmutableInfoList &inputs,
                                       const internal::OutputsImmutableInfoList &outputs,
                                       const std::vector<KernelTensor *> &ms_inputs,
                                       const std::vector<KernelTensor *> &ms_outputs) override {
    return internal::CreateReshapeAndCacheOp(
        inputs, outputs, internal::kInternalReshapeAndCacheOpName);
  }
};
} // namespace ms_custom_ops

// æ³¨å†Œç®—å­inferå‡½æ•°ï¼Œç”¨äºåœ¨è®¡ç®—è¿‡ç¨‹ä¸­æ¨å¯¼ç®—å­è¾“å‡ºshapeå’Œdtypeï¼Œä»¥ä¾¿åˆ†é…ç®—å­è¾“å‡ºå†…å­˜
REG_GRAPH_MODE_OP(reshape_and_cache, ms_custom_ops::CustomReshapeAndCacheOpFuncImpl,
                  ms_custom_ops::CustomReshapeAndCache);

// =============================================================================
// PYBOOSTè°ƒç”¨å®ç°
// =============================================================================

#include "internal_pyboost_runner.h"

using namespace ms_custom_ops;
namespace ms::pynative {

// åˆ›å»ºç®—å­pyboostæ‰§è¡Œå™¨ï¼Œéœ€è¦ç»§æ‰¿InternalPyboostRunner
class ReshapeAndCacheRunner : public InternalPyboostRunner {
public:
  using InternalPyboostRunner::InternalPyboostRunner;

  void SetHeadNum(const int32_t &head_num) { this->head_num_ = head_num; }

protected:
   // åˆ›å»ºå…·ä½“ç®—å­çš„opå®ä¾‹
  internal::InternalOpPtr CreateKernel(const internal::InputsImmutableInfoList &inputs,
                                       const internal::OutputsImmutableInfoList &outputs) override {
    return internal::CreateReshapeAndCacheOp(
        inputs, outputs, internal::kInternalReshapeAndCacheOpName);
  }

private:
  int32_t head_num_{0};
};

// ç®—å­æ³¨å†Œ
MS_KERNELS_INTERNAL_NAME_REG(ReshapeAndCache,
                             internal::kInternalReshapeAndCacheOpName);
} // namespace ms::pynative

namespace ms_custom_ops {
// è·å–tensoræˆ–åˆ›å»ºç©ºtensor
ms::Tensor GetTensorOrEmpty(const std::optional<ms::Tensor> &opt_tensor) {
  return opt_tensor.has_value() ? opt_tensor.value() : ms::Tensor();
}

// ç®—å­kernelè°ƒç”¨å‡½æ•°ï¼Œéœ€è¦æ‰‹åŠ¨åˆ›å»ºè¾“å‡ºtensor
void npu_reshape_and_cache(const ms::Tensor &key,
                           const std::optional<ms::Tensor> &value,
                           const std::optional<ms::Tensor> &key_cache,
                           const std::optional<ms::Tensor> &value_cache,
                           const std::optional<ms::Tensor> &slot_mapping,
                           std::optional<int64_t> head_num) {
  auto op_name = "ReshapeAndCache";
  auto runner = std::make_shared<ms::pynative::ReshapeAndCacheRunner>(op_name);
  MS_EXCEPTION_IF_NULL(runner);

  // è®¾ç½®head_numå±æ€§
  if (head_num.has_value()) {
    runner->SetHeadNum(static_cast<int32_t>(head_num.value()));
  }

  // ç´¢å¼•å…¥å‚è®¾ç½®åˆ°runner
  runner->Setup(op_name, key, value, key_cache, value_cache, slot_mapping,
                head_num);

  // è·å–è¾“å…¥è¾“å‡ºtensor;
  std::vector<ms::Tensor> inputs = {
      key, GetTensorOrEmpty(value), GetTensorOrEmpty(key_cache),
      GetTensorOrEmpty(value_cache), GetTensorOrEmpty(slot_mapping)};
  std::vector<ms::Tensor> outputs = {};
  runner->GetOrCreateKernel(inputs, outputs);
  runner->Run(inputs, outputs);
  return;
}
} // namespace ms_custom_ops

// pybindè°ƒç”¨å‡½æ•°
auto pyboost_reshape_and_cache(const ms::Tensor &key,
                               const std::optional<ms::Tensor> &value,
                               const std::optional<ms::Tensor> &key_cache,
                               const std::optional<ms::Tensor> &value_cache,
                               const std::optional<ms::Tensor> &slot_mapping,
                               std::optional<int64_t> head_num) {
  return ms::pynative::PyboostRunner::Call<0>(
      ms_custom_ops::npu_reshape_and_cache, key, value, key_cache, value_cache,
      slot_mapping, head_num);
}

// ç®—å­æ¥å£æ³¨å†Œï¼Œå¯¹æ¥C++å’Œpythonæ¥å£
MS_CUSTOM_OPS_EXTENSION_MODULE(m) {
  m.def("reshape_and_cache", &pyboost_reshape_and_cache, "Reshape And Cache",
        pybind11::arg("key"), pybind11::arg("value") = std::nullopt,
        pybind11::arg("key_cache") = std::nullopt,
        pybind11::arg("value_cache") = std::nullopt,
        pybind11::arg("slot_mapping") = std::nullopt,
        pybind11::arg("head_num") = std::nullopt);
}
```

#### 3. ç¼–å†™æµ‹è¯•

åˆ›å»ºæµ‹è¯•æ–‡ä»¶ `tests/st/test_my_op.py`ï¼š

```python
import pytest
import numpy as np
import mindspore as ms
import ms_custom_ops

@pytest.mark.parametrize('exec_mode', [ms.context.GRAPH_MODE, ms.context.PYNATIVE_MODE])
def test_my_op(exec_mode):
    ms.set_context(mode=exec_mode)
    ms.set_device("Ascend")
    
    # å‡†å¤‡è¾“å…¥æ•°æ®
    input_data = np.random.rand(10, 20).astype(np.float16)
    
    # æ‰§è¡Œç®—å­
    output = ms_custom_ops.my_op(ms.Tensor(input_data))
    
    # éªŒè¯ç»“æœ
    expected = # è®¡ç®—æœŸæœ›ç»“æœ
    assert np.allclose(output.asnumpy(), expected, rtol=1e-3, atol=1e-3)
```


## ğŸ› è°ƒè¯•æŠ€å·§

### 1. æ—¥å¿—è¾“å‡º

è®¾ç½®ç¯å¢ƒå˜é‡å¼€å¯è¯¦ç»†æ—¥å¿—ï¼š
```bash
export GLOG_v=3
export ASCEND_GLOBAL_LOG_LEVEL=3
```

### 2. æ€§èƒ½åˆ†æ

ä½¿ç”¨ MindSpore Profiler åˆ†æç®—å­æ€§èƒ½ï¼š
```python
from mindspore.profiler import Profiler

profiler = Profiler()
# æ‰§è¡Œç®—å­
profiler.analyse()
```

### 3. å¸¸è§é—®é¢˜

**Q: Resize æ¥å£è¿”å› KRET_RESIZE_FAILED**  
A: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. ç¡®ä¿ `CreateKernel` æ–¹æ³•æ­£ç¡®å®ç°å¹¶è¿”å›æœ‰æ•ˆçš„å†…éƒ¨ç®—å­
2. éªŒè¯ `UpdateParam` æ–¹æ³•æ˜¯å¦æ­£ç¡®å¤„ç†å‚æ•°
3. æ£€æŸ¥è¾“å…¥è¾“å‡ºç´¢å¼•æ˜ å°„æ˜¯å¦æ­£ç¡®æ³¨å†Œ
4. æŸ¥çœ‹æ—¥å¿—ç¡®è®¤å…·ä½“çš„å¤±è´¥åŸå› 

**Q: ç¼–è¯‘å¤±è´¥æç¤ºæ‰¾ä¸åˆ° CANN ç¯å¢ƒ**  
A: ç¡®ä¿æ­£ç¡®å®‰è£…æ˜‡è…¾ CANN å·¥å…·åŒ…ï¼Œå¹¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
```bash
source /usr/local/Ascend/ascend-toolkit/set_env.sh
```

**Q: æ€§èƒ½ä¸å¦‚é¢„æœŸ**  
A: 1) æ£€æŸ¥æ˜¯å¦æ­£ç¡®ä½¿ç”¨äº†ç¼“å­˜æœºåˆ¶ï¼›2) ç¡®è®¤å†…å­˜è®¿é—®æ¨¡å¼æ˜¯å¦é«˜æ•ˆï¼›3) ä½¿ç”¨ Profiler å®šä½ç“¶é¢ˆã€‚

**Q: PyBoost æ¨¡å¼ä¸‹ç®—å­æ‰§è¡Œå¤±è´¥**  
A: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. ç¡®ä¿ `CreateKernel` æ–¹æ³•æ­£ç¡®å®ç°å¹¶è¿”å›æœ‰æ•ˆçš„å†…éƒ¨ç®—å­
2. éªŒè¯ `LaunchKernel` æ–¹æ³•ä¸­çš„å¼ é‡å¤„ç†é€»è¾‘
3. æ£€æŸ¥ `Setup` æ–¹æ³•ä¸­çš„å‚æ•°è®¾ç½®å’Œ hash è®¡ç®—
4. ç¡®è®¤ Python æ¨¡å—æ³¨å†Œæ˜¯å¦æ­£ç¡®

## ç¤ºä¾‹ï¼šreshape_and_cache ç®—å­

reshape_and_cache æ˜¯ä¸€ä¸ªå…¸å‹çš„è‡ªå®šä¹‰ç®—å­ç¤ºä¾‹ï¼Œç”¨äº KV Cache çš„æ›´æ–°æ“ä½œï¼š

### åŠŸèƒ½æè¿°
- å°†è¾“å…¥çš„ key å’Œ value å¼ é‡ reshape åå†™å…¥åˆ°æŒ‡å®šçš„ç¼“å­˜ä½ç½®
- æ”¯æŒçµæ´»çš„ slot æ˜ å°„æœºåˆ¶
- é«˜æ•ˆçš„å†…å­˜æ›´æ–°æ“ä½œ

### ä½¿ç”¨æ–¹æ³•
```python
# å‚æ•°è¯´æ˜
# key: è¾“å…¥çš„ key å¼ é‡ï¼Œshape ä¸º (batch, seq_len, hidden_dim) æˆ– (batch*seq_len, hidden_dim)
# value: è¾“å…¥çš„ value å¼ é‡ï¼Œshape åŒ key
# key_cache: key ç¼“å­˜å¼ é‡ï¼Œshape ä¸º (num_slots, slot_size, num_heads, head_dim)
# value_cache: value ç¼“å­˜å¼ é‡ï¼Œshape åŒ key_cache
# slot_mapping: æŒ‡å®šæ¯ä¸ª token å†™å…¥çš„ slot ä½ç½®
# head_num: attention head æ•°é‡

output = ms_custom_ops.reshape_and_cache(
    key, value, key_cache, value_cache, slot_mapping, head_num
)
```

## ğŸ“‹ æ–‡ä»¶å‘½åè§„èŒƒ

ä¸ºäº†ä¿æŒé¡¹ç›®ç»“æ„çš„ä¸€è‡´æ€§ï¼Œè¯·éµå¾ªä»¥ä¸‹å‘½åè§„èŒƒï¼š

### ç®—å­å®ç°æ–‡ä»¶
- **ç®—å­**: `{op_name}.cc` (å¦‚: `reshape_and_cache.cc`)
- **AscendCç®—å­kernel**ï¼šæŒ‰ç…§AscendCå®˜æ–¹è¦æ±‚å®ç°`op_host`å’Œ`op_kernel`ç›®å½•ä¸‹ç®—å­æ–‡ä»¶ã€‚

### é…ç½®æ–‡ä»¶
- **YAMLé…ç½®**: `{op_name}_op.yaml` (å¦‚: `reshape_and_cache_op.yaml`)
- **ç®—å­æ–‡æ¡£**: `{op_name}_doc.yaml` (å¦‚: `reshape_and_cache_doc.yaml`)

### æµ‹è¯•æ–‡ä»¶
- **æµ‹è¯•æ–‡ä»¶**: `test_{op_name}.py` (å¦‚: `test_reshape_and_cache.py`)

### å¤´æ–‡ä»¶
- **åŸºç±»å¤´æ–‡ä»¶**: ä½¿ç”¨æè¿°æ€§åç§° (å¦‚: `internal_pyboost_runner.h`)
- **å·¥å…·å¤´æ–‡ä»¶**: ä½¿ç”¨åŠŸèƒ½æè¿° (å¦‚: `internal_helper.h`)

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®æ–°çš„è‡ªå®šä¹‰ç®—å­ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. **Fork** ä»£ç ä»“åº“
2. **åˆ›å»ºç‰¹æ€§åˆ†æ”¯**: `git checkout -b feature/your-new-op`
3. **å®ç°ç®—å­**å¹¶æ·»åŠ æµ‹è¯•
4. **æäº¤æ›´æ”¹**: `git commit -m "Add new operator: your-new-op"`
5. **æ¨é€åˆ†æ”¯**: `git push origin feature/your-new-op`
6. **åˆ›å»º Pull Request**

ç¡®ä¿ï¼š
- ä»£ç ç¬¦åˆé¡¹ç›®ç¼–ç è§„èŒƒ
- æ·»åŠ å……åˆ†çš„å•å…ƒæµ‹è¯•
- æ›´æ–°ç›¸å…³æ–‡æ¡£
- éµå¾ªæ–‡ä»¶å‘½åè§„èŒƒ
- é€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ Apache License 2.0 è®¸å¯è¯ã€‚