/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

/*
 * 2021.12.8 - Add new file.
 */

/*
 * \file sgemm_x86_64.cc
 */

#include <string>

namespace air {
namespace codegen {

extern const std::string SGEMM_KERNEL_AVX_N12 =
    "vbroadcastss $7,%ymm12; movq $9,%r12; salq $$2,%r12; movq $1,%r14; movq $8,%r11;"
    "cmpq $$16,%r11; jb 1f; movq %r12,$6; sarq $$2,$6; movq %r14,$1;"
    "vpxor %ymm0,%ymm0,%ymm0; vpxor %ymm1,%ymm1,%ymm1; vpxor %ymm2,%ymm2,%ymm2;"
    "vpxor %ymm3,%ymm3,%ymm3; vpxor %ymm4,%ymm4,%ymm4;"
    "vpxor %ymm5,%ymm5,%ymm5; vpxor %ymm6,%ymm6,%ymm6; vpxor %ymm7,%ymm7,%ymm7;"
    "vpxor %ymm8,%ymm8,%ymm8; vpxor %ymm9,%ymm9,%ymm9;"
    "vpxor %ymm10,%ymm10,%ymm10; vpxor %ymm11,%ymm11,%ymm11;"
    "cmpq $$8,$6; jb 2f; movq $2,$4; xorq %r15,%r15;"
    "3:\n\tvmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14; prefetcht0 512($0);"
    "vbroadcastsd ($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastsd 8($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastsd ($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14; addq $$64,$0;"
    "vbroadcastsd 16($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastsd 24($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastsd 16($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11;"
    "addq $$32,$1; vmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14;"
    "prefetcht0 512($0); vbroadcastsd ($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastsd 8($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd ($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14;"
    "addq $$64,$0; vbroadcastsd 16($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastsd 24($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastsd 16($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1; cmpq $$62,%r15; movq $$62,%r15;"
    "cmoveq $5,%r15; vmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14;"
    "prefetcht0 512($0); vbroadcastsd ($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastsd 8($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd ($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14;"
    "addq $$64,$0; vbroadcastsd 16($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastsd 24($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastsd 16($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1; vmovsldup ($0),%ymm13;"
    "vmovshdup ($0),%ymm14; prefetcht0 512($0); vbroadcastsd ($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastsd 8($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd ($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vmovsldup 32($0),%ymm13;"
    "vmovshdup 32($0),%ymm14; addq $$64,$0; vbroadcastsd 16($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastsd 24($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastsd 16($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1;"
    "prefetcht2 ($4); leaq -31($4,%r15,1),$4; subq $$8,$6; cmpq $$8,$6; jnb 3b;"
    "2:\n\ttestq $6,$6; jz 4f;"
    "5:\n\tvmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14;"
    "prefetcht0 512($0); addq $$32,$0; vbroadcastsd ($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastsd 8($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd ($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; addq $$16,$1; decq $6;"
    "jnz 5b; 4:\n\tmovq $2,$4; vaddps %ymm0,%ymm6,%ymm0; vaddps %ymm1,%ymm7,%ymm1;"
    "vaddps %ymm2,%ymm8,%ymm2; vaddps %ymm3,%ymm9,%ymm3;"
    "vaddps %ymm4,%ymm10,%ymm4; vaddps %ymm5,%ymm11,%ymm5; vunpcklps %ymm1,%ymm0,%ymm14;"
    "vunpckhps %ymm1,%ymm0,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm0;"
    "vunpckhpd %ymm15,%ymm14,%ymm1; vfmadd213ps ($4),%ymm12,%ymm0;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm1; vmovups %ymm0,($4);"
    "vmovups %ymm1,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %ymm3,%ymm2,%ymm14;"
    "vunpckhps %ymm3,%ymm2,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm2; vunpckhpd %ymm15,%ymm14,%ymm3;"
    "vfmadd213ps ($4),%ymm12,%ymm2; vfmadd213ps ($4,$5,1),%ymm12,%ymm3;"
    "vmovups %ymm2,($4); vmovups %ymm3,($4,$5,1); leaq ($4,$5,2),$4;"
    "vunpcklps %ymm5,%ymm4,%ymm14; vunpckhps %ymm5,%ymm4,%ymm15;"
    "vunpcklpd %ymm15,%ymm14,%ymm4; vunpckhpd %ymm15,%ymm14,%ymm5; vfmadd213ps ($4),%ymm12,%ymm4;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm5; vmovups %ymm4,($4);"
    "vmovups %ymm5,($4,$5,1); leaq ($4,$5,2),$4; negq %r12; leaq ($0,%r12,8),$0;"
    "negq %r12; 7:\n\tmovq %r12,%r13; sarq $$2,%r13; movq %r14,$1;"
    "vpxor %ymm0,%ymm0,%ymm0; vpxor %ymm1,%ymm1,%ymm1; vpxor %ymm2,%ymm2,%ymm2;"
    "vpxor %ymm3,%ymm3,%ymm3; vpxor %ymm4,%ymm4,%ymm4;"
    "vpxor %ymm5,%ymm5,%ymm5; vpxor %ymm6,%ymm6,%ymm6; vpxor %ymm7,%ymm7,%ymm7;"
    "vpxor %ymm8,%ymm8,%ymm8; vpxor %ymm9,%ymm9,%ymm9;"
    "vpxor %ymm10,%ymm10,%ymm10; vpxor %ymm11,%ymm11,%ymm11;"
    "movq %r13,$6; leaq ($2,$5,4),$4; leaq ($4,$5,2),$4;"
    "movq $4,%r10; cmpq $$16,%r13; jb 8f; movq $$14,$6; 6:\n\tvmovups ($0),%ymm13;"
    "vmovups ($0,%r12,8),%ymm14; prefetcht0 512($0,%r12,8);"
    "vbroadcastss 8($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastss 12($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastss ($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vbroadcastss 4($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss 8($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 12($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11; vmovups 32($0),%ymm13;"
    "vmovups 32($0,%r12,8),%ymm14; addq $$64,$0; vbroadcastss 24($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastss 28($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastss 16($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastss 20($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastss 24($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastss 28($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1;"
    "prefetcht0 128($1,%r12,8); testq $$24,$6; movq $$84,%r15; cmovz $5,%r15;"
    "vmovups ($0),%ymm13; vmovups ($0,%r12,8),%ymm14; prefetcht0 512($0,%r12,8);"
    "vbroadcastss 8($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastss 12($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastss ($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vbroadcastss 4($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss 8($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 12($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11; vmovups 32($0),%ymm13;"
    "vmovups 32($0,%r12,8),%ymm14; addq $$64,$0; vbroadcastss 24($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastss 28($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastss 16($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastss 20($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastss 24($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastss 28($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1;"
    "prefetcht1 ($4); subq $$63,$4; addq %r15,$4; vmovups ($0),%ymm13; vmovups ($0,%r12,8),%ymm14;"
    "prefetcht0 512($0,%r12,8); vbroadcastss 8($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastss 12($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastss ($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastss 4($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastss 8($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastss 12($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; vmovups 32($0),%ymm13;"
    "vmovups 32($0,%r12,8),%ymm14; addq $$64,$0; vbroadcastss 24($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastss 28($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastss 16($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastss 20($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastss 24($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastss 28($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1;"
    "prefetcht0 128($1,%r12,8); prefetcht1 ($3); cmpq $$198,$6; cmoveq %r10,$4;"
    "vmovups ($0),%ymm13; vmovups ($0,%r12,8),%ymm14; prefetcht0 512($0,%r12,8);"
    "vbroadcastss 8($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastss 12($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastss ($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vbroadcastss 4($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss 8($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 12($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11;"
    "vmovups 32($0),%ymm13; vmovups 32($0,%r12,8),%ymm14;"
    "addq $$64,$0; vbroadcastss 24($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastss 28($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastss 16($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vbroadcastss 20($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss 24($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 28($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11;"
    "addq $$32,$1; addq $$16,$3; addq $$8,$6; cmpq $6,%r13;"
    "jnb 6b; movq %r10,$4; negq $6; leaq 14(%r13,$6,1),$6; 8:\n\txorq %r15,%r15;"
    "testq $6,$6; jz 34f; 33:\n\tprefetcht0 ($4); prefetcht0 63($4);"
    "addq $5,$4; incq %r15; vmovups ($0),%ymm13; vmovups ($0,%r12,8),%ymm14;"
    "prefetcht0 512($0,%r12,8); addq $$32,$0; vbroadcastss 8($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastss 12($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastss ($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastss 4($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastss 8($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastss 12($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$16,$1; cmpq $$6,%r15;"
    "cmoveq %r10,$4; decq $6; jnz 33b; 34:\n\tprefetcht0 (%r14);"
    "prefetcht0 64(%r14); leaq ($2,$5,4),$4; leaq ($4,$5,2),$4; vfmadd213ps ($4),%ymm12,%ymm0;"
    "vfmadd213ps 32($4),%ymm12,%ymm1; vmovups %ymm0,($4);"
    "vmovups %ymm1,32($4); vfmadd213ps ($4,$5,1),%ymm12,%ymm2; vfmadd213ps 32($4,$5,1),%ymm12,%ymm3;"
    "vmovups %ymm2,($4,$5,1); vmovups %ymm3,32($4,$5,1);"
    "leaq ($4,$5,2),$4; vfmadd213ps ($4),%ymm12,%ymm4;"
    "vfmadd213ps 32($4),%ymm12,%ymm5; vmovups %ymm4,($4); vmovups %ymm5,32($4);"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm6; vfmadd213ps 32($4,$5,1),%ymm12,%ymm7;"
    "vmovups %ymm6,($4,$5,1); vmovups %ymm7,32($4,$5,1); leaq ($4,$5,2),$4;"
    "vfmadd213ps ($4),%ymm12,%ymm8; vfmadd213ps 32($4),%ymm12,%ymm9; vmovups %ymm8,($4);"
    "vmovups %ymm9,32($4); vfmadd213ps ($4,$5,1),%ymm12,%ymm10;"
    "vfmadd213ps 32($4,$5,1),%ymm12,%ymm11; vmovups %ymm10,($4,$5,1); vmovups %ymm11,32($4,$5,1);"
    "leaq ($4,$5,2),$4; addq $$32,$2; subq $$8,%r11; cmpq $$16,%r11;"
    "jb 16f; movq %r12,%r13; sarq $$2,%r13; movq %r14,$1; vpxor %ymm0,%ymm0,%ymm0;"
    "vpxor %ymm1,%ymm1,%ymm1; vpxor %ymm2,%ymm2,%ymm2;"
    "vpxor %ymm3,%ymm3,%ymm3; vpxor %ymm4,%ymm4,%ymm4; vpxor %ymm5,%ymm5,%ymm5;"
    "vpxor %ymm6,%ymm6,%ymm6; vpxor %ymm7,%ymm7,%ymm7;"
    "vpxor %ymm8,%ymm8,%ymm8; vpxor %ymm9,%ymm9,%ymm9;"
    "vpxor %ymm10,%ymm10,%ymm10; vpxor %ymm11,%ymm11,%ymm11;"
    "movq %r13,$6; movq $2,$4; cmpq $$16,%r13; jb 10f;"
    "movq $$14,$6; 35:\n\tvmovups ($0),%ymm13; vmovups ($0,%r12,8),%ymm14;"
    "prefetcht0 512($0,%r12,8); vbroadcastss ($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastss 4($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastss 8($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastss 12($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastss ($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastss 4($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; vmovups 32($0),%ymm13;"
    "vmovups 32($0,%r12,8),%ymm14; addq $$64,$0; vbroadcastss 16($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastss 20($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastss 24($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vbroadcastss 28($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss 16($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 20($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1;"
    "prefetcht0 128($1); testq $$24,$6; movq $$84,%r15; cmovz $5,%r15;"
    "vmovups ($0),%ymm13; vmovups ($0,%r12,8),%ymm14; prefetcht0 512($0,%r12,8);"
    "vbroadcastss ($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastss 4($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastss 8($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastss 12($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss ($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 4($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11;"
    "vmovups 32($0),%ymm13; vmovups 32($0,%r12,8),%ymm14;"
    "addq $$64,$0; vbroadcastss 16($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastss 20($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastss 24($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastss 28($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss 16($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 20($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11;"
    "addq $$32,$1; prefetcht1 ($4); subq $$63,$4; addq %r15,$4;"
    "vmovups ($0),%ymm13; vmovups ($0,%r12,8),%ymm14; prefetcht0 512($0,%r12,8);"
    "vbroadcastss ($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastss 4($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastss 8($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastss 12($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss ($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 4($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11;"
    "vmovups 32($0),%ymm13; vmovups 32($0,%r12,8),%ymm14;"
    "addq $$64,$0; vbroadcastss 16($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastss 20($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastss 24($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastss 28($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss 16($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 20($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11;"
    "addq $$32,$1; prefetcht0 128($1); prefetcht1 ($3);"
    "cmpq $$198,$6; cmoveq $2,$4; vmovups ($0),%ymm13; vmovups ($0,%r12,8),%ymm14;"
    "prefetcht0 512($0,%r12,8); vbroadcastss ($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastss 4($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastss 8($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vbroadcastss 12($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss ($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 4($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11; vmovups 32($0),%ymm13;"
    "vmovups 32($0,%r12,8),%ymm14; addq $$64,$0; vbroadcastss 16($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastss 20($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastss 24($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastss 28($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss 16($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 20($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11;"
    "addq $$32,$1; addq $$16,$3; addq $$8,$6; cmpq $6,%r13;"
    "jnb 35b; movq $2,$4; negq $6; leaq 14(%r13,$6,1),$6; 10:\n\txorq %r15,%r15;"
    "testq $6,$6; jz 11f; 9:\n\tprefetcht0 ($4); prefetcht0 63($4);"
    "addq $5,$4; incq %r15; vmovups ($0),%ymm13; vmovups ($0,%r12,8),%ymm14;"
    "prefetcht0 512($0,%r12,8); addq $$32,$0; vbroadcastss ($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastss 4($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastss 8($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vbroadcastss 12($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastss ($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastss 4($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$16,$1;"
    "cmpq $$6,%r15; cmoveq $2,$4; decq $6; jnz 9b; 11:\n\tmovq $2,$4;"
    "vfmadd213ps ($4),%ymm12,%ymm0; vfmadd213ps 32($4),%ymm12,%ymm1; vmovups %ymm0,($4);"
    "vmovups %ymm1,32($4); vfmadd213ps ($4,$5,1),%ymm12,%ymm2;"
    "vfmadd213ps 32($4,$5,1),%ymm12,%ymm3; vmovups %ymm2,($4,$5,1); vmovups %ymm3,32($4,$5,1);"
    "leaq ($4,$5,2),$4; vfmadd213ps ($4),%ymm12,%ymm4;"
    "vfmadd213ps 32($4),%ymm12,%ymm5; vmovups %ymm4,($4); vmovups %ymm5,32($4);"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm6; vfmadd213ps 32($4,$5,1),%ymm12,%ymm7;"
    "vmovups %ymm6,($4,$5,1); vmovups %ymm7,32($4,$5,1); leaq ($4,$5,2),$4;"
    "vfmadd213ps ($4),%ymm12,%ymm8; vfmadd213ps 32($4),%ymm12,%ymm9; vmovups %ymm8,($4);"
    "vmovups %ymm9,32($4); vfmadd213ps ($4,$5,1),%ymm12,%ymm10;"
    "vfmadd213ps 32($4,$5,1),%ymm12,%ymm11; vmovups %ymm10,($4,$5,1); vmovups %ymm11,32($4,$5,1);"
    "leaq ($4,$5,2),$4; addq $$32,$2; subq $$8,%r11; cmpq $$16,%r11;"
    "jnb 7b; movq %r12,$6; sarq $$2,$6; movq %r14,$1; vpxor %ymm0,%ymm0,%ymm0;"
    "vpxor %ymm1,%ymm1,%ymm1; vpxor %ymm2,%ymm2,%ymm2;"
    "vpxor %ymm3,%ymm3,%ymm3; vpxor %ymm4,%ymm4,%ymm4; vpxor %ymm5,%ymm5,%ymm5;"
    "vpxor %ymm6,%ymm6,%ymm6; vpxor %ymm7,%ymm7,%ymm7;"
    "vpxor %ymm8,%ymm8,%ymm8; vpxor %ymm9,%ymm9,%ymm9;"
    "vpxor %ymm10,%ymm10,%ymm10; vpxor %ymm11,%ymm11,%ymm11;"
    "cmpq $$8,$6; jb 13f; 12:\n\tvmovsldup ($0),%ymm13;"
    "vmovshdup ($0),%ymm14; prefetcht0 512($0); vbroadcastsd 8($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastsd ($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastsd 8($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14; addq $$64,$0;"
    "vbroadcastsd 24($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastsd 16($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastsd 24($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1;"
    "vmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14; prefetcht0 512($0);"
    "vbroadcastsd 8($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastsd ($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd 8($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14;"
    "addq $$64,$0; vbroadcastsd 24($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastsd 16($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastsd 24($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1;"
    "vmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14; prefetcht0 512($0);"
    "vbroadcastsd 8($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastsd ($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd 8($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14;"
    "addq $$64,$0; vbroadcastsd 24($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastsd 16($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastsd 24($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1;"
    "vmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14; prefetcht0 512($0);"
    "vbroadcastsd 8($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastsd ($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd 8($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14;"
    "addq $$64,$0; vbroadcastsd 24($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastsd 16($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastsd 24($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1; subq $$8,$6;"
    "cmpq $$8,$6; jnb 12b; 13:\n\ttestq $6,$6; jz 15f;"
    "14:\n\tvmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14; prefetcht0 512($0);"
    "addq $$32,$0; vbroadcastsd 8($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastsd ($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd 8($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; addq $$16,$1; decq $6;"
    "jnz 14b; 15:\n\tleaq ($2,$5,4),$4; leaq ($4,$5,2),$4; vaddps %ymm0,%ymm6,%ymm0;"
    "vaddps %ymm1,%ymm7,%ymm1; vaddps %ymm2,%ymm8,%ymm2;"
    "vaddps %ymm3,%ymm9,%ymm3; vaddps %ymm4,%ymm10,%ymm4; vaddps %ymm5,%ymm11,%ymm5;"
    "vunpcklps %ymm1,%ymm0,%ymm14; vunpckhps %ymm1,%ymm0,%ymm15;"
    "vunpcklpd %ymm15,%ymm14,%ymm0; vunpckhpd %ymm15,%ymm14,%ymm1;"
    "vfmadd213ps ($4),%ymm12,%ymm0; vfmadd213ps ($4,$5,1),%ymm12,%ymm1;"
    "vmovups %ymm0,($4); vmovups %ymm1,($4,$5,1); leaq ($4,$5,2),$4;"
    "vunpcklps %ymm3,%ymm2,%ymm14; vunpckhps %ymm3,%ymm2,%ymm15;"
    "vunpcklpd %ymm15,%ymm14,%ymm2; vunpckhpd %ymm15,%ymm14,%ymm3; vfmadd213ps ($4),%ymm12,%ymm2;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm3; vmovups %ymm2,($4);"
    "vmovups %ymm3,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %ymm5,%ymm4,%ymm14;"
    "vunpckhps %ymm5,%ymm4,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm4; vunpckhpd %ymm15,%ymm14,%ymm5;"
    "vfmadd213ps ($4),%ymm12,%ymm4; vfmadd213ps ($4,$5,1),%ymm12,%ymm5;"
    "vmovups %ymm4,($4); vmovups %ymm5,($4,$5,1); leaq ($4,$5,2),$4;"
    "addq $$32,$2; subq $$8,%r11; jmp 27f; 16:\n\tmovq %r12,$6;"
    "sarq $$2,$6; movq %r14,$1; vpxor %ymm0,%ymm0,%ymm0; vpxor %ymm1,%ymm1,%ymm1;"
    "vpxor %ymm2,%ymm2,%ymm2; vpxor %ymm3,%ymm3,%ymm3;"
    "vpxor %ymm4,%ymm4,%ymm4; vpxor %ymm5,%ymm5,%ymm5; vpxor %ymm6,%ymm6,%ymm6;"
    "vpxor %ymm7,%ymm7,%ymm7; vpxor %ymm8,%ymm8,%ymm8;"
    "vpxor %ymm9,%ymm9,%ymm9; vpxor %ymm10,%ymm10,%ymm10;"
    "vpxor %ymm11,%ymm11,%ymm11; cmpq $$8,$6; jb 18f;"
    "17:\n\tvmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14; prefetcht0 512($0);"
    "vbroadcastsd ($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastsd 8($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastsd ($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14; addq $$64,$0;"
    "vbroadcastsd 16($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastsd 24($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastsd 16($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11;"
    "addq $$32,$1; vmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14;"
    "prefetcht0 512($0); vbroadcastsd ($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastsd 8($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd ($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14;"
    "addq $$64,$0; vbroadcastsd 16($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastsd 24($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastsd 16($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1; vmovsldup ($0),%ymm13;"
    "vmovshdup ($0),%ymm14; prefetcht0 512($0); vbroadcastsd ($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastsd 8($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd ($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vmovsldup 32($0),%ymm13;"
    "vmovshdup 32($0),%ymm14; addq $$64,$0; vbroadcastsd 16($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastsd 24($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8; vfmadd231ps %ymm14,%ymm15,%ymm9;"
    "vbroadcastsd 16($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm10;"
    "vfmadd231ps %ymm14,%ymm15,%ymm11; addq $$32,$1;"
    "vmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14; prefetcht0 512($0);"
    "vbroadcastsd ($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastsd 8($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vbroadcastsd ($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14; addq $$64,$0;"
    "vbroadcastsd 16($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; vbroadcastsd 24($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastsd 16($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11;"
    "addq $$32,$1; subq $$8,$6; cmpq $$8,$6; jnb 17b;"
    "18:\n\ttestq $6,$6; jz 20f; 19:\n\tvmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14;"
    "prefetcht0 512($0); addq $$32,$0; vbroadcastsd ($1) ,%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastsd 8($1) ,%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd ($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; addq $$16,$1; decq $6;"
    "jnz 19b; 20:\n\tmovq $2,$4; vaddps %ymm0,%ymm6,%ymm0; vaddps %ymm1,%ymm7,%ymm1;"
    "vaddps %ymm2,%ymm8,%ymm2; vaddps %ymm3,%ymm9,%ymm3;"
    "vaddps %ymm4,%ymm10,%ymm4; vaddps %ymm5,%ymm11,%ymm5; vunpcklps %ymm1,%ymm0,%ymm14;"
    "vunpckhps %ymm1,%ymm0,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm0;"
    "vunpckhpd %ymm15,%ymm14,%ymm1; vfmadd213ps ($4),%ymm12,%ymm0;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm1; vmovups %ymm0,($4);"
    "vmovups %ymm1,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %ymm3,%ymm2,%ymm14;"
    "vunpckhps %ymm3,%ymm2,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm2; vunpckhpd %ymm15,%ymm14,%ymm3;"
    "vfmadd213ps ($4),%ymm12,%ymm2; vfmadd213ps ($4,$5,1),%ymm12,%ymm3;"
    "vmovups %ymm2,($4); vmovups %ymm3,($4,$5,1); leaq ($4,$5,2),$4;"
    "vunpcklps %ymm5,%ymm4,%ymm14; vunpckhps %ymm5,%ymm4,%ymm15;"
    "vunpcklpd %ymm15,%ymm14,%ymm4; vunpckhpd %ymm15,%ymm14,%ymm5; vfmadd213ps ($4),%ymm12,%ymm4;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm5; vmovups %ymm4,($4);"
    "vmovups %ymm5,($4,$5,1); leaq ($4,$5,2),$4; addq $$32,$2; subq $$8,%r11;"
    "jmp 27f; 1:\n\tcmpq $$8,%r11; jb 27f; movq %r12,$6; sarq $$2,$6; movq %r14,$1;"
    "vpxor %ymm0,%ymm0,%ymm0; vpxor %ymm1,%ymm1,%ymm1; vpxor %ymm2,%ymm2,%ymm2;"
    "vpxor %ymm3,%ymm3,%ymm3; vpxor %ymm4,%ymm4,%ymm4;"
    "vpxor %ymm5,%ymm5,%ymm5; vpxor %ymm6,%ymm6,%ymm6; vpxor %ymm7,%ymm7,%ymm7;"
    "vpxor %ymm8,%ymm8,%ymm8; vpxor %ymm9,%ymm9,%ymm9;"
    "vpxor %ymm10,%ymm10,%ymm10; vpxor %ymm11,%ymm11,%ymm11;"
    "testq $6,$6; jz 22f; 21:\n\tvmovsldup ($0),%ymm13;"
    "vmovshdup ($0),%ymm14; addq $$32,$0; vbroadcastsd ($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastsd 8($1),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd 0($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vbroadcastsd 0+8($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "vbroadcastsd 0($1,%r12,8),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm8;"
    "vfmadd231ps %ymm14,%ymm15,%ymm9; vbroadcastsd 0+8($1,%r12,8),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm10; vfmadd231ps %ymm14,%ymm15,%ymm11;"
    "addq $$16,$1; decq $6; jnz 21b; 22:\n\tmovq $2,$4;"
    "vunpcklps %ymm1,%ymm0,%ymm14; vunpckhps %ymm1,%ymm0,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm0;"
    "vunpckhpd %ymm15,%ymm14,%ymm1; vfmadd213ps ($4),%ymm12,%ymm0;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm1; vmovups %ymm0,($4);"
    "vmovups %ymm1,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %ymm3,%ymm2,%ymm14;"
    "vunpckhps %ymm3,%ymm2,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm2; vunpckhpd %ymm15,%ymm14,%ymm3;"
    "vfmadd213ps ($4),%ymm12,%ymm2; vfmadd213ps ($4,$5,1),%ymm12,%ymm3;"
    "vmovups %ymm2,($4); vmovups %ymm3,($4,$5,1); leaq ($4,$5,2),$4;"
    "vunpcklps %ymm5,%ymm4,%ymm14; vunpckhps %ymm5,%ymm4,%ymm15;"
    "vunpcklpd %ymm15,%ymm14,%ymm4; vunpckhpd %ymm15,%ymm14,%ymm5; vfmadd213ps ($4),%ymm12,%ymm4;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm5; vmovups %ymm4,($4);"
    "vmovups %ymm5,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %ymm7,%ymm6,%ymm14;"
    "vunpckhps %ymm7,%ymm6,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm6;"
    "vunpckhpd %ymm15,%ymm14,%ymm7; vfmadd213ps ($4),%ymm12,%ymm6;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm7; vmovups %ymm6,($4);"
    "vmovups %ymm7,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %ymm9,%ymm8,%ymm14;"
    "vunpckhps %ymm9,%ymm8,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm8;"
    "vunpckhpd %ymm15,%ymm14,%ymm9; vfmadd213ps ($4),%ymm12,%ymm8;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm9; vmovups %ymm8,($4);"
    "vmovups %ymm9,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %ymm11,%ymm10,%ymm14;"
    "vunpckhps %ymm11,%ymm10,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm10;"
    "vunpckhpd %ymm15,%ymm14,%ymm11; vfmadd213ps ($4),%ymm12,%ymm10;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm11; vmovups %ymm10,($4);"
    "vmovups %ymm11,($4,$5,1); leaq ($4,$5,2),$4; addq $$8*4,$2;"
    "subq $$8,%r11; 27:\n\tcmpq $$4,%r11; jb 28f; movq %r12,$6; sarq $$2,$6;"
    "movq %r14,$1; vpxor %xmm0,%xmm0,%xmm0; vpxor %xmm1,%xmm1,%xmm1; vpxor %xmm2,%xmm2,%xmm2;"
    "vpxor %xmm3,%xmm3,%xmm3; vpxor %xmm4,%xmm4,%xmm4;"
    "vpxor %xmm5,%xmm5,%xmm5; vpxor %xmm6,%xmm6,%xmm6; vpxor %xmm7,%xmm7,%xmm7;"
    "vpxor %xmm8,%xmm8,%xmm8; vpxor %xmm9,%xmm9,%xmm9;"
    "vpxor %xmm10,%xmm10,%xmm10; vpxor %xmm11,%xmm11,%xmm11;"
    "testq $6,$6; jz 23f; 26:\n\tvmovsldup ($0),%xmm13;"
    "vmovshdup ($0),%xmm14; addq $$16,$0; vmovddup ($1),%xmm15; vfmadd231ps %xmm13,%xmm15,%xmm0;"
    "vfmadd231ps %xmm14,%xmm15,%xmm1; vmovddup 8($1),%xmm15;"
    "vfmadd231ps %xmm13,%xmm15,%xmm2; vfmadd231ps %xmm14,%xmm15,%xmm3; vmovddup ($1,%r12,4),%xmm15;"
    "vfmadd231ps %xmm13,%xmm15,%xmm4; vfmadd231ps %xmm14,%xmm15,%xmm5;"
    "vmovddup 8($1,%r12,4),%xmm15; vfmadd231ps %xmm13,%xmm15,%xmm6;"
    "vfmadd231ps %xmm14,%xmm15,%xmm7; vmovddup ($1,%r12,8),%xmm15;"
    "vfmadd231ps %xmm13,%xmm15,%xmm8; vfmadd231ps %xmm14,%xmm15,%xmm9;"
    "vmovddup 8($1,%r12,8),%xmm15; vfmadd231ps %xmm13,%xmm15,%xmm10;"
    "vfmadd231ps %xmm14,%xmm15,%xmm11; addq $$16,$1; decq $6;"
    "jnz 26b; 23:\n\tmovq $2,$4; vunpcklps %xmm1,%xmm0,%xmm14; vunpckhps %xmm1,%xmm0,%xmm15;"
    "vunpcklpd %xmm15,%xmm14,%xmm0; vunpckhpd %xmm15,%xmm14,%xmm1;"
    "vfmadd213ps ($4),%xmm12,%xmm0; vmovups %xmm0,($4);"
    "vfmadd213ps ($4,$5,1),%xmm12,%xmm1; vmovups %xmm1,($4,$5,1); leaq ($4,$5,2),$4;"
    "vunpcklps %xmm3,%xmm2,%xmm14; vunpckhps %xmm3,%xmm2,%xmm15;"
    "vunpcklpd %xmm15,%xmm14,%xmm2; vunpckhpd %xmm15,%xmm14,%xmm3; vfmadd213ps ($4),%xmm12,%xmm2;"
    "vmovups %xmm2,($4); vfmadd213ps ($4,$5,1),%xmm12,%xmm3;"
    "vmovups %xmm3,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %xmm5,%xmm4,%xmm14;"
    "vunpckhps %xmm5,%xmm4,%xmm15; vunpcklpd %xmm15,%xmm14,%xmm4; vunpckhpd %xmm15,%xmm14,%xmm5;"
    "vfmadd213ps ($4),%xmm12,%xmm4; vmovups %xmm4,($4);"
    "vfmadd213ps ($4,$5,1),%xmm12,%xmm5; vmovups %xmm5,($4,$5,1); leaq ($4,$5,2),$4;"
    "vunpcklps %xmm7,%xmm6,%xmm14; vunpckhps %xmm7,%xmm6,%xmm15;"
    "vunpcklpd %xmm15,%xmm14,%xmm6; vunpckhpd %xmm15,%xmm14,%xmm7;"
    "vfmadd213ps ($4),%xmm12,%xmm6; vmovups %xmm6,($4);"
    "vfmadd213ps ($4,$5,1),%xmm12,%xmm7; vmovups %xmm7,($4,$5,1); leaq ($4,$5,2),$4;"
    "vunpcklps %xmm9,%xmm8,%xmm14; vunpckhps %xmm9,%xmm8,%xmm15;"
    "vunpcklpd %xmm15,%xmm14,%xmm8; vunpckhpd %xmm15,%xmm14,%xmm9;"
    "vfmadd213ps ($4),%xmm12,%xmm8; vmovups %xmm8,($4);"
    "vfmadd213ps ($4,$5,1),%xmm12,%xmm9; vmovups %xmm9,($4,$5,1); leaq ($4,$5,2),$4;"
    "vunpcklps %xmm11,%xmm10,%xmm14; vunpckhps %xmm11,%xmm10,%xmm15;"
    "vunpcklpd %xmm15,%xmm14,%xmm10; vunpckhpd %xmm15,%xmm14,%xmm11;"
    "vfmadd213ps ($4),%xmm12,%xmm10; vmovups %xmm10,($4);"
    "vfmadd213ps ($4,$5,1),%xmm12,%xmm11; vmovups %xmm11,($4,$5,1); leaq ($4,$5,2),$4;"
    "addq $$4*4,$2; subq $$4,%r11; 28:\n\tcmpq $$2,%r11; jb 29f;"
    "movq %r12,$6; sarq $$2,$6; movq %r14,$1; vpxor %xmm0,%xmm0,%xmm0; vpxor %xmm1,%xmm1,%xmm1;"
    "vpxor %xmm2,%xmm2,%xmm2; vpxor %xmm3,%xmm3,%xmm3;"
    "vpxor %xmm4,%xmm4,%xmm4; vpxor %xmm5,%xmm5,%xmm5; testq $6,$6; jz 25f;"
    "24:\n\tvmovups ($1),%xmm15; vmovups ($1,%r12,4),%xmm14; vmovups ($1,%r12,8),%xmm13;"
    "addq $$16,$1; vbroadcastss ($0),%xmm6; vfmadd231ps %xmm15,%xmm6,%xmm0;"
    "vfmadd231ps %xmm14,%xmm6,%xmm2; vfmadd231ps %xmm13,%xmm6,%xmm4;"
    "vbroadcastss 4($0),%xmm6; vfmadd231ps %xmm15,%xmm6,%xmm1;"
    "vfmadd231ps %xmm14,%xmm6,%xmm3; vfmadd231ps %xmm13,%xmm6,%xmm5;"
    "addq $$8,$0; decq $6; jnz 24b; 25:\n\tmovq $2,$4;"
    "vunpcklps %xmm1,%xmm0,%xmm13; vunpckhps %xmm1,%xmm0,%xmm14; vmovsd ($4),%xmm15;"
    "vmovhpd ($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm13;"
    "vmovsd %xmm13,($4); vmovhpd %xmm13,($4,$5,1); leaq ($4,$5,2),$4; vmovsd ($4),%xmm15;"
    "vmovhpd ($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm14;"
    "vmovsd %xmm14,($4); vmovhpd %xmm14,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %xmm3,%xmm2,%xmm13;"
    "vunpckhps %xmm3,%xmm2,%xmm14; vmovsd ($4),%xmm15;"
    "vmovhpd ($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm13; vmovsd %xmm13,($4);"
    "vmovhpd %xmm13,($4,$5,1); leaq ($4,$5,2),$4; vmovsd ($4),%xmm15;"
    "vmovhpd ($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm14;"
    "vmovsd %xmm14,($4); vmovhpd %xmm14,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %xmm5,%xmm4,%xmm13;"
    "vunpckhps %xmm5,%xmm4,%xmm14; vmovsd ($4),%xmm15;"
    "vmovhpd ($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm13; vmovsd %xmm13,($4);"
    "vmovhpd %xmm13,($4,$5,1); leaq ($4,$5,2),$4; vmovsd ($4),%xmm15;"
    "vmovhpd ($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm14;"
    "vmovsd %xmm14,($4); vmovhpd %xmm14,($4,$5,1); leaq ($4,$5,2),$4; addq $$2*4,$2;"
    "subq $$2,%r11; 29:\n\ttestq %r11,%r11; jz 32f; movq %r12,$6;"
    "sarq $$2,$6; movq %r14,$1; vpxor %xmm0,%xmm0,%xmm0; vpxor %xmm1,%xmm1,%xmm1;"
    "vpxor %xmm2,%xmm2,%xmm2; testq $6,$6; jz 31f;"
    "30:\n\tvmovups ($1),%xmm15; vmovups ($1,%r12,4),%xmm14; vmovups ($1,%r12,8),%xmm13;"
    "addq $$16,$1; vbroadcastss ($0),%xmm6; vfmadd231ps %xmm15,%xmm6,%xmm0;"
    "vfmadd231ps %xmm14,%xmm6,%xmm1; vfmadd231ps %xmm13,%xmm6,%xmm2;"
    "addq $$4,$0; decq $6; jnz 30b; 31:\n\tmovq $2,$4;"
    "vpxor %xmm6,%xmm6,%xmm6; vmovsd %xmm0,%xmm6,%xmm14; vmovhlps %xmm0,%xmm6,%xmm13;"
    "vmovss ($4),%xmm15; vinsertps $$16,($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm14; vmovss %xmm14,($4);"
    "vextractps $$1,%xmm14,($4,$5,1); leaq ($4,$5,2),$4; vmovss ($4),%xmm15;"
    "vinsertps $$16,($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm13;"
    "vmovss %xmm13,($4); vextractps $$1,%xmm13,($4,$5,1);"
    "leaq ($4,$5,2),$4; vpxor %xmm6,%xmm6,%xmm6; vmovsd %xmm1,%xmm6,%xmm14;"
    "vmovhlps %xmm1,%xmm6,%xmm13; vmovss ($4),%xmm15; vinsertps $$16,($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm14; vmovss %xmm14,($4);"
    "vextractps $$1,%xmm14,($4,$5,1); leaq ($4,$5,2),$4; vmovss ($4),%xmm15;"
    "vinsertps $$16,($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm13;"
    "vmovss %xmm13,($4); vextractps $$1,%xmm13,($4,$5,1);"
    "leaq ($4,$5,2),$4; vpxor %xmm6,%xmm6,%xmm6; vmovsd %xmm2,%xmm6,%xmm14;"
    "vmovhlps %xmm2,%xmm6,%xmm13; vmovss ($4),%xmm15; vinsertps $$16,($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm14; vmovss %xmm14,($4);"
    "vextractps $$1,%xmm14,($4,$5,1); leaq ($4,$5,2),$4; vmovss ($4),%xmm15;"
    "vinsertps $$16,($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm13;"
    "vmovss %xmm13,($4); vextractps $$1,%xmm13,($4,$5,1);"
    "leaq ($4,$5,2),$4; addq $$1*4,$2; 32:\n\tmovq %r14,$1; vzeroupper;";

extern const std::string SGEMM_KERNEL_AVX_N8 =
    "vbroadcastss $7,%ymm12; movq $9,%r12; salq $$2,%r12; movq $1,%r14; movq $8,%r11;"
    "cmpq $$8,%r11; jb 39f; 36:\n\tmovq %r12,$6; sarq $$2,$6; movq %r14,$1;"
    "vpxor %ymm0,%ymm0,%ymm0; vpxor %ymm1,%ymm1,%ymm1; vpxor %ymm2,%ymm2,%ymm2;"
    "vpxor %ymm3,%ymm3,%ymm3; vpxor %ymm4,%ymm4,%ymm4;"
    "vpxor %ymm5,%ymm5,%ymm5; vpxor %ymm6,%ymm6,%ymm6; vpxor %ymm7,%ymm7,%ymm7;"
    "testq $6,$6; jz 38f; 37:\n\tvmovsldup ($0),%ymm13;"
    "vmovshdup ($0),%ymm14; addq $$32,$0; vbroadcastsd ($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastsd 8($1),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vbroadcastsd 0($1,%r12,4),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vbroadcastsd 0+8($1,%r12,4),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "addq $$16,$1; decq $6; jnz 37b; 38:\n\tmovq $2,$4;"
    "vunpcklps %ymm1,%ymm0,%ymm14; vunpckhps %ymm1,%ymm0,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm0;"
    "vunpckhpd %ymm15,%ymm14,%ymm1; vfmadd213ps ($4),%ymm12,%ymm0;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm1; vmovups %ymm0,($4);"
    "vmovups %ymm1,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %ymm3,%ymm2,%ymm14;"
    "vunpckhps %ymm3,%ymm2,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm2; vunpckhpd %ymm15,%ymm14,%ymm3;"
    "vfmadd213ps ($4),%ymm12,%ymm2; vfmadd213ps ($4,$5,1),%ymm12,%ymm3;"
    "vmovups %ymm2,($4); vmovups %ymm3,($4,$5,1); leaq ($4,$5,2),$4;"
    "vunpcklps %ymm5,%ymm4,%ymm14; vunpckhps %ymm5,%ymm4,%ymm15;"
    "vunpcklpd %ymm15,%ymm14,%ymm4; vunpckhpd %ymm15,%ymm14,%ymm5; vfmadd213ps ($4),%ymm12,%ymm4;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm5; vmovups %ymm4,($4);"
    "vmovups %ymm5,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %ymm7,%ymm6,%ymm14;"
    "vunpckhps %ymm7,%ymm6,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm6;"
    "vunpckhpd %ymm15,%ymm14,%ymm7; vfmadd213ps ($4),%ymm12,%ymm6;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm7; vmovups %ymm6,($4);"
    "vmovups %ymm7,($4,$5,1); leaq ($4,$5,2),$4; addq $$8*4,$2;"
    "subq $$8,%r11; cmpq $$8,%r11; jnb 36b; 39:\n\tcmpq $$4,%r11; jb 44f; movq %r12,$6;"
    "sarq $$2,$6; movq %r14,$1; vpxor %xmm0,%xmm0,%xmm0; vpxor %xmm1,%xmm1,%xmm1;"
    "vpxor %xmm2,%xmm2,%xmm2; vpxor %xmm3,%xmm3,%xmm3;"
    "vpxor %xmm4,%xmm4,%xmm4; vpxor %xmm5,%xmm5,%xmm5; vpxor %xmm6,%xmm6,%xmm6;"
    "vpxor %xmm7,%xmm7,%xmm7; testq $6,$6; jz 42f;"
    "40:\n\tvmovsldup ($0),%xmm13; vmovshdup ($0),%xmm14; addq $$16,$0; vmovddup ($1),%xmm15;"
    "vfmadd231ps %xmm13,%xmm15,%xmm0; vfmadd231ps %xmm14,%xmm15,%xmm1;"
    "vmovddup 8($1),%xmm15; vfmadd231ps %xmm13,%xmm15,%xmm2; vfmadd231ps %xmm14,%xmm15,%xmm3;"
    "vmovddup ($1,%r12,4),%xmm15; vfmadd231ps %xmm13,%xmm15,%xmm4;"
    "vfmadd231ps %xmm14,%xmm15,%xmm5; vmovddup 8($1,%r12,4),%xmm15;"
    "vfmadd231ps %xmm13,%xmm15,%xmm6; vfmadd231ps %xmm14,%xmm15,%xmm7;"
    "addq $$16,$1; decq $6; jnz 40b; 42:\n\tmovq $2,$4;"
    "vunpcklps %xmm1,%xmm0,%xmm14; vunpckhps %xmm1,%xmm0,%xmm15; vunpcklpd %xmm15,%xmm14,%xmm0;"
    "vunpckhpd %xmm15,%xmm14,%xmm1; vfmadd213ps ($4),%xmm12,%xmm0;"
    "vmovups %xmm0,($4); vfmadd213ps ($4,$5,1),%xmm12,%xmm1;"
    "vmovups %xmm1,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %xmm3,%xmm2,%xmm14;"
    "vunpckhps %xmm3,%xmm2,%xmm15; vunpcklpd %xmm15,%xmm14,%xmm2; vunpckhpd %xmm15,%xmm14,%xmm3;"
    "vfmadd213ps ($4),%xmm12,%xmm2; vmovups %xmm2,($4);"
    "vfmadd213ps ($4,$5,1),%xmm12,%xmm3; vmovups %xmm3,($4,$5,1); leaq ($4,$5,2),$4;"
    "vunpcklps %xmm5,%xmm4,%xmm14; vunpckhps %xmm5,%xmm4,%xmm15;"
    "vunpcklpd %xmm15,%xmm14,%xmm4; vunpckhpd %xmm15,%xmm14,%xmm5; vfmadd213ps ($4),%xmm12,%xmm4;"
    "vmovups %xmm4,($4); vfmadd213ps ($4,$5,1),%xmm12,%xmm5;"
    "vmovups %xmm5,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %xmm7,%xmm6,%xmm14;"
    "vunpckhps %xmm7,%xmm6,%xmm15; vunpcklpd %xmm15,%xmm14,%xmm6;"
    "vunpckhpd %xmm15,%xmm14,%xmm7; vfmadd213ps ($4),%xmm12,%xmm6;"
    "vmovups %xmm6,($4); vfmadd213ps ($4,$5,1),%xmm12,%xmm7;"
    "vmovups %xmm7,($4,$5,1); leaq ($4,$5,2),$4; addq $$4*4,$2;"
    "subq $$4,%r11; 44:\n\tcmpq $$2,%r11; jb 48f; movq %r12,$6; sarq $$2,$6;"
    "movq %r14,$1; vpxor %xmm0,%xmm0,%xmm0; vpxor %xmm1,%xmm1,%xmm1;"
    "vpxor %xmm2,%xmm2,%xmm2; vpxor %xmm3,%xmm3,%xmm3; testq $6,$6; jz 46f;"
    "45:\n\tvmovups ($1),%xmm15; vmovups ($1,%r12,4),%xmm14; addq $$16,$1;"
    "vbroadcastss ($0),%xmm13; vfmadd231ps %xmm15,%xmm13,%xmm0; vfmadd231ps %xmm14,%xmm13,%xmm2;"
    "vbroadcastss 4($0),%xmm13; vfmadd231ps %xmm15,%xmm13,%xmm1;"
    "vfmadd231ps %xmm14,%xmm13,%xmm3; addq $$8,$0; decq $6;"
    "jnz 45b; 46:\n\tmovq $2,$4; vunpcklps %xmm1,%xmm0,%xmm13; vunpckhps %xmm1,%xmm0,%xmm14;"
    "vmovsd ($4),%xmm15; vmovhpd ($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm13; vmovsd %xmm13,($4); vmovhpd %xmm13,($4,$5,1);"
    "leaq ($4,$5,2),$4; vmovsd ($4),%xmm15; vmovhpd ($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm14; vmovsd %xmm14,($4);"
    "vmovhpd %xmm14,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %xmm3,%xmm2,%xmm13;"
    "vunpckhps %xmm3,%xmm2,%xmm14; vmovsd ($4),%xmm15; vmovhpd ($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm13; vmovsd %xmm13,($4);"
    "vmovhpd %xmm13,($4,$5,1); leaq ($4,$5,2),$4; vmovsd ($4),%xmm15; vmovhpd ($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm14; vmovsd %xmm14,($4);"
    "vmovhpd %xmm14,($4,$5,1); leaq ($4,$5,2),$4; addq $$2*4,$2; subq $$2,%r11;"
    "48:\n\ttestq %r11,%r11; jz 50f; movq %r12,$6; sarq $$2,$6; movq %r14,$1;"
    "vpxor %xmm0,%xmm0,%xmm0; vpxor %xmm1,%xmm1,%xmm1; testq $6,$6; jz 49f;"
    "47:\n\tvmovups ($1),%xmm15; vmovups ($1,%r12,4),%xmm14; addq $$16,$1;"
    "vbroadcastss ($0),%xmm13; vfmadd231ps %xmm15,%xmm13,%xmm0; vfmadd231ps %xmm14,%xmm13,%xmm1;"
    "addq $$4,$0; decq $6; jnz 47b; 49:\n\tmovq $2,$4;"
    "vpxor %xmm6,%xmm6,%xmm6; vmovsd %xmm0,%xmm6,%xmm14; vmovhlps %xmm0,%xmm6,%xmm13;"
    "vmovss ($4),%xmm15; vinsertps $$16,($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm14; vmovss %xmm14,($4);"
    "vextractps $$1,%xmm14,($4,$5,1); leaq ($4,$5,2),$4; vmovss ($4),%xmm15;"
    "vinsertps $$16,($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm13;"
    "vmovss %xmm13,($4); vextractps $$1,%xmm13,($4,$5,1);"
    "leaq ($4,$5,2),$4; vpxor %xmm6,%xmm6,%xmm6; vmovsd %xmm1,%xmm6,%xmm14;"
    "vmovhlps %xmm1,%xmm6,%xmm13; vmovss ($4),%xmm15; vinsertps $$16,($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm14; vmovss %xmm14,($4);"
    "vextractps $$1,%xmm14,($4,$5,1); leaq ($4,$5,2),$4; vmovss ($4),%xmm15;"
    "vinsertps $$16,($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm13;"
    "vmovss %xmm13,($4); vextractps $$1,%xmm13,($4,$5,1);"
    "leaq ($4,$5,2),$4; addq $$1*4,$2; 50:\n\tmovq %r14,$1; vzeroupper;";

extern const std::string SGEMM_KERNEL_AVX_N4 =
    "vbroadcastss $7,%ymm12; movq $9,%r12; salq $$2,%r12; movq $1,%r14; movq $8,%r11;"
    "cmpq $$8,%r11; jb 55f; 90:\n\tmovq %r12,$6; sarq $$2,$6; movq %r14,$1;"
    "vpxor %ymm0,%ymm0,%ymm0; vpxor %ymm1,%ymm1,%ymm1; vpxor %ymm2,%ymm2,%ymm2;"
    "vpxor %ymm3,%ymm3,%ymm3; vpxor %ymm4,%ymm4,%ymm4;"
    "vpxor %ymm5,%ymm5,%ymm5; vpxor %ymm6,%ymm6,%ymm6; vpxor %ymm7,%ymm7,%ymm7;"
    "cmpq $$8,$6; jb 52f; 51:\n\tvmovsldup ($0),%ymm13;"
    "vmovshdup ($0),%ymm14; prefetcht0 512($0); vbroadcastsd 0($1),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastsd 0+8($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14; addq $$64,$0;"
    "vbroadcastsd 16($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vbroadcastsd 16+8($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; addq $$32,$1;"
    "vmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14; prefetcht0 512($0);"
    "vbroadcastsd 0($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastsd 0+8($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vmovsldup 32($0),%ymm13;"
    "vmovshdup 32($0),%ymm14; addq $$64,$0; vbroadcastsd 16($1),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastsd 16+8($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "addq $$32,$1; vmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14;"
    "prefetcht0 512($0); vbroadcastsd 0($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastsd 0+8($1),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "vmovsldup 32($0),%ymm13; vmovshdup 32($0),%ymm14; addq $$64,$0;"
    "vbroadcastsd 16($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm4;"
    "vfmadd231ps %ymm14,%ymm15,%ymm5; vbroadcastsd 16+8($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6;"
    "vfmadd231ps %ymm14,%ymm15,%ymm7; addq $$32,$1;"
    "vmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14; prefetcht0 512($0);"
    "vbroadcastsd 0($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "vbroadcastsd 0+8($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm2;"
    "vfmadd231ps %ymm14,%ymm15,%ymm3; vmovsldup 32($0),%ymm13;"
    "vmovshdup 32($0),%ymm14; addq $$64,$0; vbroadcastsd 16($1),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm4; vfmadd231ps %ymm14,%ymm15,%ymm5;"
    "vbroadcastsd 16+8($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm6; vfmadd231ps %ymm14,%ymm15,%ymm7;"
    "addq $$32,$1; subq $$8,$6; cmpq $$8,$6; jnb 51b;"
    "52:\n\ttestq $6,$6; jz 54f; 53:\n\tvmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14;"
    "addq $$32,$0; vbroadcastsd ($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0;"
    "vfmadd231ps %ymm14,%ymm15,%ymm1; vbroadcastsd 8($1),%ymm15;"
    "vfmadd231ps %ymm13,%ymm15,%ymm2; vfmadd231ps %ymm14,%ymm15,%ymm3;"
    "addq $$16,$1; decq $6; jnz 53b; 54:\n\tmovq $2,$4;"
    "vaddps %ymm0,%ymm4,%ymm0; vaddps %ymm1,%ymm5,%ymm1; vaddps %ymm2,%ymm6,%ymm2;"
    "vaddps %ymm3,%ymm7,%ymm3; vunpcklps %ymm1,%ymm0,%ymm14;"
    "vunpckhps %ymm1,%ymm0,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm0;"
    "vunpckhpd %ymm15,%ymm14,%ymm1; vfmadd213ps ($4),%ymm12,%ymm0;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm1; vmovups %ymm0,($4);"
    "vmovups %ymm1,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %ymm3,%ymm2,%ymm14;"
    "vunpckhps %ymm3,%ymm2,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm2; vunpckhpd %ymm15,%ymm14,%ymm3;"
    "vfmadd213ps ($4),%ymm12,%ymm2; vfmadd213ps ($4,$5,1),%ymm12,%ymm3;"
    "vmovups %ymm2,($4); vmovups %ymm3,($4,$5,1); leaq ($4,$5,2),$4;"
    "addq $$32,$2; subq $$8,%r11; cmpq $$8,%r11; jnb 90b;"
    "55:\n\tcmpq $$4,%r11; jb 58f; movq %r12,$6; sarq $$2,$6; movq %r14,$1;"
    "vpxor %xmm0,%xmm0,%xmm0; vpxor %xmm1,%xmm1,%xmm1; vpxor %xmm2,%xmm2,%xmm2;"
    "vpxor %xmm3,%xmm3,%xmm3; testq $6,$6; jz 57f;"
    "56:\n\tvmovsldup ($0),%xmm13; vmovshdup ($0),%xmm14; addq $$16,$0; vmovddup ($1),%xmm15;"
    "vfmadd231ps %xmm13,%xmm15,%xmm0; vfmadd231ps %xmm14,%xmm15,%xmm1;"
    "vmovddup 8($1),%xmm15; vfmadd231ps %xmm13,%xmm15,%xmm2; vfmadd231ps %xmm14,%xmm15,%xmm3;"
    "addq $$16,$1; decq $6; jnz 56b; 57:\n\tmovq $2,$4;"
    "vunpcklps %xmm1,%xmm0,%xmm14; vunpckhps %xmm1,%xmm0,%xmm15; vunpcklpd %xmm15,%xmm14,%xmm0;"
    "vunpckhpd %xmm15,%xmm14,%xmm1; vfmadd213ps ($4),%xmm12,%xmm0;"
    "vmovups %xmm0,($4); vfmadd213ps ($4,$5,1),%xmm12,%xmm1;"
    "vmovups %xmm1,($4,$5,1); leaq ($4,$5,2),$4; vunpcklps %xmm3,%xmm2,%xmm14;"
    "vunpckhps %xmm3,%xmm2,%xmm15; vunpcklpd %xmm15,%xmm14,%xmm2; vunpckhpd %xmm15,%xmm14,%xmm3;"
    "vfmadd213ps ($4),%xmm12,%xmm2; vmovups %xmm2,($4);"
    "vfmadd213ps ($4,$5,1),%xmm12,%xmm3; vmovups %xmm3,($4,$5,1); leaq ($4,$5,2),$4;"
    "addq $$4*4,$2; subq $$4,%r11; 58:\n\tcmpq $$2,%r11; jb 64f;"
    "movq %r12,$6; sarq $$2,$6; movq %r14,$1; vpxor %xmm0,%xmm0,%xmm0; vpxor %xmm1,%xmm1,%xmm1;"
    "testq $6,$6; jz 60f; 59:\n\tvmovups ($1),%xmm15; addq $$16,$1;"
    "vbroadcastss ($0),%xmm13; vfmadd231ps %xmm15,%xmm13,%xmm0;"
    "vbroadcastss 4($0),%xmm14; vfmadd231ps %xmm15,%xmm14,%xmm1; addq $$8,$0;"
    "decq $6; jnz 59b; 60:\n\tmovq $2,$4; vunpcklps %xmm1,%xmm0,%xmm13;"
    "vunpckhps %xmm1,%xmm0,%xmm14; vmovsd ($4),%xmm15; vmovhpd ($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm13; vmovsd %xmm13,($4);"
    "vmovhpd %xmm13,($4,$5,1); leaq ($4,$5,2),$4; vmovsd ($4),%xmm15; vmovhpd ($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm14; vmovsd %xmm14,($4);"
    "vmovhpd %xmm14,($4,$5,1); leaq ($4,$5,2),$4; addq $$2*4,$2; subq $$2,%r11;"
    "64:\n\ttestq %r11,%r11; jz 63f; movq %r12,$6; sarq $$2,$6; movq %r14,$1;"
    "vpxor %xmm0,%xmm0,%xmm0; testq $6,$6; jz 62f; 61:\n\tvmovups ($1),%xmm15;"
    "addq $$16,$1; vbroadcastss ($0),%xmm13; vfmadd231ps %xmm15,%xmm13,%xmm0;"
    "addq $$4,$0; decq $6; jnz 61b; 62:\n\tmovq $2,$4; vpxor %xmm6,%xmm6,%xmm6;"
    "vmovsd %xmm0,%xmm6,%xmm14; vmovhlps %xmm0,%xmm6,%xmm13;"
    "vmovss ($4),%xmm15; vinsertps $$16,($4,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm14; vmovss %xmm14,($4);"
    "vextractps $$1,%xmm14,($4,$5,1); leaq ($4,$5,2),$4; vmovss ($4),%xmm15;"
    "vinsertps $$16,($4,$5,1),%xmm15,%xmm15; vfmadd213ps %xmm15,%xmm12,%xmm13;"
    "vmovss %xmm13,($4); vextractps $$1,%xmm13,($4,$5,1);"
    "leaq ($4,$5,2),$4; addq $$1*4,$2; 63:\n\tmovq %r14,$1; vzeroupper;";

extern const std::string SGEMM_KERNEL_AVX_N2 =
    "vbroadcastss $7,%ymm12; movq $9,%r12; salq $$2,%r12; movq $1,%r14; movq $8,%r11;"
    "cmpq $$8,%r11; jb 68f; 65:\n\tmovq %r12,$6; sarq $$2,$6; movq %r14,$1;"
    "vpxor %ymm0,%ymm0,%ymm0; vpxor %ymm1,%ymm1,%ymm1; testq $6,$6;"
    "jz 67f; 66:\n\tvmovsldup ($0),%ymm13; vmovshdup ($0),%ymm14; addq $$32,$0;"
    "vbroadcastsd ($1),%ymm15; vfmadd231ps %ymm13,%ymm15,%ymm0; vfmadd231ps %ymm14,%ymm15,%ymm1;"
    "addq $$8,$1; decq $6; jnz 66b; 67:\n\tmovq $2,$4;"
    "vunpcklps %ymm1,%ymm0,%ymm14; vunpckhps %ymm1,%ymm0,%ymm15; vunpcklpd %ymm15,%ymm14,%ymm0;"
    "vunpckhpd %ymm15,%ymm14,%ymm1; vfmadd213ps ($4),%ymm12,%ymm0;"
    "vfmadd213ps ($4,$5,1),%ymm12,%ymm1; vmovups %ymm0,($4);"
    "vmovups %ymm1,($4,$5,1); leaq ($4,$5,2),$4; addq $$8*4,$2; subq $$8,%r11;"
    "cmpq $$8,%r11; jnb 65b; 68:\n\tcmpq $$4,%r11; jb 71f; movq %r12,$6; sarq $$2,$6;"
    "movq %r14,$1; vpxor %xmm0,%xmm0,%xmm0; vpxor %xmm1,%xmm1,%xmm1;"
    "testq $6,$6; jz 69f; 70:\n\tvmovsldup ($0),%xmm13; vmovshdup ($0),%xmm14;"
    "addq $$16,$0; vmovddup ($1),%xmm15; vfmadd231ps %xmm13,%xmm15,%xmm0;"
    "vfmadd231ps %xmm14,%xmm15,%xmm1; addq $$8,$1; decq $6; jnz 70b; 69:\n\tmovq $2,$4;"
    "vunpcklps %xmm1,%xmm0,%xmm14; vunpckhps %xmm1,%xmm0,%xmm15;"
    "vunpcklpd %xmm15,%xmm14,%xmm0; vunpckhpd %xmm15,%xmm14,%xmm1; vfmadd213ps ($4),%xmm12,%xmm0;"
    "vmovups %xmm0,($4); vfmadd213ps ($4,$5,1),%xmm12,%xmm1;"
    "vmovups %xmm1,($4,$5,1); leaq ($4,$5,2),$4; addq $$4*4,$2; subq $$4,%r11;"
    "71:\n\tcmpq $$2,%r11; jb 92f; movq %r12,$6; sarq $$2,$6; movq %r14,$1;"
    "vpxor %xmm0,%xmm0,%xmm0; vpxor %xmm1,%xmm1,%xmm1; testq $6,$6; jz 74f;"
    "72:\n\tvmovsd ($0),%xmm13; addq $$8,$0; vbroadcastss ($1),%xmm14;"
    "vfmadd231ps %xmm13,%xmm14,%xmm0; vbroadcastss 4($1),%xmm15; vfmadd231ps %xmm13,%xmm15,%xmm1;"
    "addq $$8,$1; decq $6; jnz 72b; 74:\n\tvmovsd ($2),%xmm13;"
    "vfmadd213ps %xmm13,%xmm12,%xmm0; vmovsd %xmm0,($2); vmovsd ($2,$5,1),%xmm13;"
    "vfmadd213ps %xmm13,%xmm12,%xmm1; vmovsd %xmm1,($2,$5,1);"
    "addq $$2*4,$2; subq $$2,%r11; 92:\n\ttestq %r11,%r11; jz 93f; movq %r12,$6;"
    "sarq $$2,$6; movq %r14,$1; vpxor %xmm0,%xmm0,%xmm0; testq $6,$6;"
    "jz 75f; 76:\n\tvmovsd ($1),%xmm15; addq $$8,$1; vbroadcastss ($0),%xmm13;"
    "vfmadd231ps %xmm15,%xmm13,%xmm0; addq $$4,$0; decq $6; jnz 76b;"
    "75:\n\tvmovss ($2),%xmm15; vinsertps $$16,($2,$5,1),%xmm15,%xmm15;"
    "vfmadd213ps %xmm15,%xmm12,%xmm0; vmovss %xmm0,($2); vextractps $$1,%xmm0,($2,$5,1);"
    "addq $$1*4,$2; 93:\n\tmovq %r14,$1; vzeroupper;";

extern const std::string SGEMM_KERNEL_AVX_N1 =
    "vbroadcastss $7,%ymm12; movq $9,%r12; salq $$2,%r12; movq $1,%r14; movq $8,%r11;"
    "cmpq $$8,%r11; jb 80f; 77:\n\tmovq %r12,$6; sarq $$2,$6; movq %r14,$1;"
    "vpxor %ymm0,%ymm0,%ymm0; testq $6,$6; jz 79f; 78:\n\tvmovups ($0),%ymm13;"
    "addq $$32,$0; vbroadcastss ($1),%ymm14; vfmadd231ps %ymm13,%ymm14,%ymm0;"
    "addq $$4,$1; decq $6; jnz 78b; 79:\n\tvfmadd213ps ($2),%ymm12,%ymm0;"
    "vmovups %ymm0,($2); addq $$8*4,$2; subq $$8,%r11; cmpq $$8,%r11;"
    "jnb 77b; 80:\n\tcmpq $$4,%r11; jb 83f; movq %r12,$6; sarq $$2,$6;"
    "movq %r14,$1; vpxor %xmm0,%xmm0,%xmm0; testq $6,$6; jz 82f;"
    "81:\n\tvmovups ($0),%xmm13; addq $$16,$0; vbroadcastss ($1),%xmm14;"
    "vfmadd231ps %xmm13,%xmm14,%xmm0; addq $$4,$1; decq $6; jnz 81b;"
    "82:\n\tvfmadd213ps ($2),%xmm12,%xmm0; vmovups %xmm0,($2); addq $$4*4,$2;"
    "subq $$4,%r11; 83:\n\tcmpq $$2,%r11; jb 86f; movq %r12,$6; sarq $$2,$6;"
    "movq %r14,$1; vpxor %xmm0,%xmm0,%xmm0; testq $6,$6; jz 85f; 84:\n\tvmovsd ($0),%xmm13;"
    "addq $$8,$0; vbroadcastss ($1),%xmm14; vfmadd231ps %xmm13,%xmm14,%xmm0;"
    "addq $$4,$1; decq $6; jnz 84b; 85:\n\tvmovsd ($2),%xmm13;"
    "vfmadd213ps %xmm13,%xmm12,%xmm0; vmovsd %xmm0,($2); addq $$2*4,$2;"
    "subq $$2,%r11; 86:\n\ttestq %r11,%r11; jz 89f; movq %r12,$6;"
    "sarq $$2,$6; movq %r14,$1; vpxor %xmm0,%xmm0,%xmm0; testq $6,$6; jz 88f;"
    "87:\n\tvmovss ($1),%xmm15; addq $$4,$1; vmovss ($0),%xmm13; vfmadd231ss %xmm15,%xmm13,%xmm0;"
    "addq $$4,$0; decq $6; jnz 87b; 88:\n\tvfmadd213ss ($2),%xmm12,%xmm0;"
    "vmovss %xmm0,($2); addq $$1*4,$2; 89:\n\tmovq %r14,$1;"
    "vzeroupper;";

}  // namespace codegen
}  // namespace air
